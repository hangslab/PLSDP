{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "wicked-daisy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vipuser\\.conda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\vipuser\\.conda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\vipuser\\.conda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\vipuser\\.conda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\vipuser\\.conda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\vipuser\\.conda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import multilabel_confusion_matrix, confusion_matrix\n",
    "import math\n",
    "from itertools import product\n",
    "import argparse\n",
    "import sys\n",
    "from utils_res_3CS import *\n",
    "import calendar\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "damaged-planner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_eachseq(seq,pssmfile,mask_seq,new_pssms):\n",
    "    if os.path.exists(pssmfile):  #如果pssm文件存在\n",
    "        print(\"found \" + pssmfile + \"\\n\")  #输出找到pssm文件+换行\n",
    "        pssm = readPSSM(pssmfile)  #读取pssm文件\n",
    "    else:  #否则\n",
    "        print(\"using Blosum62\\n\")  #输出使用Blosum62+换行\n",
    "        #pssm = convertSampleToBlosum62(seq)  #把Blosum62矩阵当作pssm用\n",
    "        pssm = convertSampleToCBOW(seq)\n",
    "    pssm = pssm.astype(float)  #对pssm的数据类型转换为浮点型\n",
    "    PhyChem = convertSampleToPhysicsVector_pca(seq)  #将样本转化为物理向量\n",
    "    pssm = np.concatenate((PhyChem, pssm), axis=1)  #物化指标和pssm对应行进行数组拼接\n",
    "    seql = len(seq)   #序列长度  \n",
    "    if seql <= 1000:  #如果序列长度小于等于1000\n",
    "        padnum = 1000 - seql  #pad大小为1000-序列长度\n",
    "        padmatrix = np.zeros([padnum, 25])  #pad矩阵为行数为padnum，列数为25的全0矩阵，即用0填充不足的地方\n",
    "        pssm = np.concatenate((pssm, padmatrix), axis=0)  #物化指标和pssm进行数组拼接 \n",
    "        new_pssms.append(pssm)  #新的pssm空列表中添加pssm矩阵\n",
    "        mask_seq.append(gen_mask_mat(seql, padnum))  #mask序列空列表添加gen_mask矩阵，序列长度为行数，padnum为列数？？？\n",
    "    else:  #如果序列长度大于1000\n",
    "        pssm = np.concatenate((pssm[0:500, :], pssm[seql - 500:seql, :]), axis=0)  #pssm矩阵为前500行和后500行矩阵的拼接\n",
    "        new_pssms.append(pssm)  #新的pssm空列表中添加pssm矩阵\n",
    "        mask_seq.append(gen_mask_mat(1000, 0))  #mask序列空列表添加1000行0列的？？？gen_mask矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "metric-seven",
   "metadata": {},
   "outputs": [],
   "source": [
    "def endpad(seqfile, labelfile, pssmdir=\"\", npzfile = \"\"): #定义endpad(序列文件，标签文件，pssm路径，npz文件)\n",
    "    if not os.path.exists(npzfile):  #如果npz文件不存在，建立新的pssm空列表，标签空列表，mask序列空列表，id空列表\n",
    "        new_pssms = []\n",
    "        labels = []\n",
    "        mask_seq = []\n",
    "        ids=[]\n",
    "        seqs=[]\n",
    "        f = open(seqfile, \"r\")  #f为打开序列文件\n",
    "        f2 = open(labelfile, \"r\")  #f2为打开标签文件\n",
    "        line = f.readline()  #读取序列文件的第一行\n",
    "        while line != '':\n",
    "            pssmfile = pssmdir + line[1:].strip() + \"_pssm.txt\"  #pssm文件名=pssm地址+id名+_pssm.txt\n",
    "            if line[0] == '>':  #如果该行第一个字符为>\n",
    "                id = line.strip()[1:]  #id为去掉>的字符\n",
    "                ids.append(id)   #在id空列表中添加id\n",
    "            label = f2.readline().strip()  #标签为f2（标签文件）中去掉首尾空格的内容\n",
    "            labels.append(label)  #在标签空列表中添加标签\n",
    "            seq = f.readline().strip()  #第一次seq为第2行的内容，实际seq为>行的下一行\n",
    "            #seql = len(seq)   #序列长度  \n",
    "            process_eachseq(seq,pssmfile,mask_seq,new_pssms)\n",
    "            line = f.readline()  #继续读取下一行，即>行\n",
    "        x = np.array(new_pssms)  #把new_pssms列表变为数组，赋给x\n",
    "        y = [convertlabels_to_categorical(i) for i in labels]  #把标签列表转化为类别(i)\n",
    "        y = np.array(y)  #再把类别转化为数组\n",
    "        mask = np.array(mask_seq)  #把mask_seq（标注的序列？）转化为数组\n",
    "        np.savez(npzfile, x=x, y=y, mask=mask, ids=ids)  #保存多个数组到同一个文件中,保存格式是.npz\n",
    "        return [x, y, mask,ids]  #返回pssm矩阵，类别，标注序列，名字id\n",
    "    else:  #如果上述都存在，直接转化为数组\n",
    "        mask = np.load(npzfile)['mask']\n",
    "        x = np.load(npzfile)['x']\n",
    "        y = np.load(npzfile)['y']\n",
    "        ids=np.load(npzfile)['ids']\n",
    "        return [x, y, mask,ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "warming-recommendation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_MULocDeep(lv1_dir,lv2_dir,pssm_dir,output_dir,foldnum):\n",
    "    # get small data\n",
    "    [train_x, train_y, train_mask, train_ids] = endpad(\n",
    "        lv2_dir+\"lv2_train_fold\" + str(foldnum) + \"_seq\",\n",
    "        lv2_dir+\"lv2_train_fold\" + str(foldnum) + \"_lab\",\n",
    "        pssm_dir,\n",
    "        \"D:/mulocdeep/mul_data/lv2_train_fold\"+str(foldnum)+\"_seq.npz\")\n",
    "    [val_x, val_y, val_mask,val_ids] = endpad(\n",
    "        lv2_dir+\"lv2_val_fold\" + str(foldnum) + \"_seq\",\n",
    "        lv2_dir+\"lv2_val_fold\" + str(foldnum) + \"_lab\",\n",
    "        pssm_dir,\n",
    "        \"D:/mulocdeep/mul_data/lv2_val_fold\"+str(foldnum)+\"_seq.npz\")\n",
    "\n",
    "    # get big data 训练10分类的多分类\n",
    "    [train_x_big, train_y_big, train_mask_big, train_ids_big] = endpad(\n",
    "        lv1_dir + \"lv1_train_fold\" + str(foldnum) + \"_seq\",\n",
    "        lv1_dir + \"lv1_train_fold\" + str(foldnum) + \"_lab\",\n",
    "        pssm_dir,\n",
    "        \"D:/mulocdeep/mul_data/lv1_train_fold\" + str(foldnum) + \"_seq.npz\")\n",
    "\n",
    "    [val_x_big, val_y_big, val_mask_big, val_ids_big] = endpad(\n",
    "        lv1_dir + \"lv1_val_fold\" + str(foldnum) + \"_seq\",\n",
    "        lv1_dir + \"lv1_val_fold\" + str(foldnum) + \"_lab\",\n",
    "        pssm_dir,\n",
    "        \"D:/mulocdeep/mul_data/lv1_val_fold\" + str(foldnum) + \"_seq.npz\")\n",
    "\n",
    "    batch_size = 128\n",
    "    print(\"doing \" + str(foldnum) + \"th fold\")\n",
    "    model_big, model_small = singlemodel(train_x)  #模型为singlemodel\n",
    "\n",
    "    filepath_acc_big_lv1 = output_dir+\"fold\" + str(\n",
    "        foldnum) + \"_big_lv1_acc-weights.hdf5\"  # -improvement-{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "\n",
    "    filepath_acc_small_lv2 = output_dir+\"fold\" + str(\n",
    "        foldnum) + \"_small_lv2_acc-weights.hdf5\"  # -improvement-{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "\n",
    "    filepath_loss_big_lv1 = output_dir+\"fold\" + str(\n",
    "        foldnum) + \"_big_lv1_loss-weights.hdf5\"  # -improvement-{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "\n",
    "    filepath_loss_small_lv2 = output_dir+\"fold\" + str(\n",
    "        foldnum) + \"_small_lv2_loss-weights.hdf5\"  # -improvement-{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "\n",
    "\n",
    "    checkpoint_acc_big_lev1 = ModelCheckpoint(filepath_acc_big_lv1, monitor='val_accuracy', save_best_only=True,\n",
    "                                          mode='max',\n",
    "                                          save_weights_only=True, verbose=1)\n",
    "\n",
    "    checkpoint_acc_small_lev2 = ModelCheckpoint(filepath_acc_small_lv2, monitor='val_lev2_accuracy', save_best_only=True,\n",
    "                                          mode='max',\n",
    "                                          save_weights_only=True, verbose=1)\n",
    "    \n",
    "    checkpoint_loss_big_lev1 = ModelCheckpoint(filepath_loss_big_lv1, monitor='val_loss', save_best_only=True,\n",
    "                                          mode='min',\n",
    "                                          save_weights_only=True, verbose=1)\n",
    "    \n",
    "    checkpoint_loss_small_lev2 = ModelCheckpoint(filepath_loss_small_lv2, monitor='val_lev2_loss', save_best_only=True,\n",
    "                                          mode='min',\n",
    "                                          save_weights_only=True, verbose=1)\n",
    "    \n",
    "    \n",
    "    for i in range(80):\n",
    "        # train small model\n",
    "        print(\"epoch \"+str(i)+\"\\n\")\n",
    "        '''fitHistory_batch_small = model_small.fit([train_x, train_mask.reshape(-1, 1000, 1)],\n",
    "                                                 [train_y,getTrue4out1(train_y)],\n",
    "                                                 batch_size=batch_size, epochs=1,\n",
    "                                                 validation_data=(\n",
    "                                                 [val_x, val_mask.reshape(-1, 1000, 1)], [val_y,getTrue4out1(val_y)]),\n",
    "                                                 callbacks=[checkpoint_acc_small_lev2,checkpoint_loss_small_lev2],verbose=1)'''\n",
    "        \n",
    "        # train big model  \n",
    "        fitHistory_batch_big = model_big.fit([train_x_big, train_mask_big.reshape(-1, 1000, 1)],\n",
    "                                             [getTrue4out1(train_y_big)],  #为何大模型没有train_y_big\n",
    "                                             batch_size=batch_size, epochs=1,  #等于1？？\n",
    "                                             validation_data=(\n",
    "                                             [val_x_big, val_mask_big.reshape(-1, 1000, 1)], [getTrue4out1(val_y_big)]),  #也没有val_y_big\n",
    "                                             callbacks=[checkpoint_acc_big_lev1,checkpoint_loss_big_lev1], verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "alert-announcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_var(input_var,pssm_dir,output_dir,foldnum):\n",
    "    # get small data\n",
    "    [train_x,train_y,train_mask,train_ids]=endpad(input_var+\"deeploc_40nr_train_fold\"+str(foldnum)+\"_seq\",\n",
    "                                        input_var+\"deeploc_40nr_train_fold\"+str(foldnum)+\"_label\",\n",
    "                                        pssm_dir,\n",
    "                                        \"D:/deeploc/deeploc_40nr_8folds/train_fold\"+str(foldnum)+\"_seq.npz\")\n",
    "    [val_x,val_y,val_mask,val_ids]=endpad(input_var+\"deeploc_40nr_val_fold\"+str(foldnum)+\"_seq\",\n",
    "                                  input_var+\"deeploc_40nr_val_fold\"+str(foldnum)+\"_label\",\n",
    "                                  pssm_dir,\n",
    "                                  \"D:/deeploc/deeploc_40nr_8folds/val_fold\"+str(foldnum)+\"_seq.npz\")\n",
    "    batch_size = 128\n",
    "    print(\"doing \" + str(foldnum) + \"th fold\")\n",
    "    model = var_model(train_x)   #这里的模型是var_model\n",
    "\n",
    "    filepath_acc = output_dir+\"fold\" + str(foldnum) + \"acc-weights.hdf5\"  # -improvement-{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "    checkpoint_acc = ModelCheckpoint(filepath_acc, monitor='val_accuracy', save_best_only=True, mode='max',\n",
    "                                 save_weights_only=True, verbose=1)\n",
    "    fitHistory_batch = model.fit([train_x,train_mask.reshape(-1,1000,1)],getTrue4out1(train_y),\n",
    "                                 batch_size=batch_size, epochs=20,\n",
    "                                 validation_data=([val_x,val_mask.reshape(-1,1000,1)], getTrue4out1(val_y)),\n",
    "                                 callbacks=[checkpoint_acc],verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "spiritual-affiliate",
   "metadata": {},
   "outputs": [],
   "source": [
    " '''我们常常可以把argparse的使用简化成下面四个步骤\n",
    "       1：import argparse\n",
    "       2：parser = argparse.ArgumentParser()\n",
    "       3：parser.add_argument()\n",
    "       4：parser.parse_args()\n",
    "       上面四个步骤解释如下：首先导入该模块；然后创建一个解析对象；然后向该对象中添加你要关注的命令行参数和选项，\n",
    "       每一个add_argument方法对应一个你要关注的参数或选项；最后调用parse_args()方法进行解析；解析成功之后即可使用'''\n",
    "    \n",
    "def main():\n",
    "    #加default\n",
    "    # description= 这个参数简要描述这个程度做什么以及怎么做\n",
    "    parser=argparse.ArgumentParser(\n",
    "        description='MULocDeep: interpretable protein localization classifier at sub-cellular and sub-organellar levels')\n",
    "    #MULocDeep_model  UniLoc-train-20nr\n",
    "    #--lv1_input_dir/--lv2_input_dir 亚细胞训练数据，包含8折蛋白质序列和标记  需自己添加\n",
    "    parser.add_argument('--lv1_input_dir', dest='lv1_dir', type=str, \n",
    "                        default=\"D:/mulocdeep/mul_data\",\n",
    "                        help='sub-cellular training data, contains 8 folds protein sequences and labels', required=False)\n",
    "    parser.add_argument('--lv2_input_dir', dest='lv2_dir', type=str,\n",
    "                       default=\"D:/mulocdeep/mul_data\",\n",
    "                       help='sub-cellular training data, contains 8 folds protein sequences and labels', required=False)\n",
    "    #--MULocDeep_model 添加它来训练MULocDeep模型，否则训练一个var模型\n",
    "    parser.add_argument('--MULocDeep_model', dest='modeltype', action='store_false',  #触发，store_true会触发DeepLoc\n",
    "                        #如果是store_false,则默认值是True，如果是store_true,则默认值是False  \n",
    "                        help='Add this to train the MULocDeep model, otherwise train a variant model', required=False)\n",
    "    #--model_output 受过训练的模型存储的目录的名称  需自己添加\n",
    "    parser.add_argument('--model_output', dest='outputdir', type=str, \n",
    "                       default=\"D:/mulocdeep/lv1_result44\",\n",
    "                       help='the name of the directory where the trained model stores', required=False)  #由True改成False\n",
    "    \n",
    "    parser.add_argument('-existPSSM', dest='existPSSM', type=str,\n",
    "                        default=\"D:/mulocdeep/mulocdeep_pssm_empty\",\n",
    "                        help='the name of the existing PSSM directory if there is one.', required=False)\n",
    "    \n",
    "    #var_model  deeploc_40nr_8folds\n",
    "    #--input_dir 训练var模型的数据，包含8折蛋白质序列和标记  需自己添加\n",
    "    parser.add_argument('--input_var', dest='var_dir', type=str,\n",
    "                        default=\"D:/deeploc/deeploc_40nr_8folds\",\n",
    "                        help='data for traing the variant model, contains 8 folds protein sequences and labels', required=False)\n",
    "    #改true  并且还需要加一个model_ouput  一个是deeploc  一个是MULocDeep\n",
    "    parser.add_argument('--var_model_output', dest='var_outputdir', type=str, help='the name of the directory where the trained model stores', \n",
    "                        default=\"D:/deeploc/var_model_result1\",\n",
    "                        required=False)  #由True改成False\n",
    "    parser.add_argument('-var_existPSSM', dest='var_existPSSM', type=str,\n",
    "                        default=\"D:/deeploc/deeploc_pssm\",\n",
    "                        help='the name of the existing PSSM directory if there is one.', required=False)\n",
    "    parser.set_defaults(feature=True)\n",
    "    #args = parser.parse_args()   #改\n",
    "    args = parser.parse_known_args()[0]   #jupyter下运行解析需要此代码\n",
    "    model_type=args.modeltype\n",
    "    input_lv1=args.lv1_dir\n",
    "    input_lv2 = args.lv2_dir\n",
    "    outputdir=args.outputdir\n",
    "    existPSSM = args.existPSSM\n",
    "    input_var=args.var_dir\n",
    "    var_outputdir=args.var_outputdir\n",
    "    var_existPSSM = args.var_existPSSM\n",
    "\n",
    "    if model_type==True:\n",
    "        if not input_lv1[len(input_lv1) - 1] == \"/\":\n",
    "            input_lv1 = input_lv1 + \"/\"\n",
    "        if not input_lv2[len(input_lv2) - 1] == \"/\":\n",
    "            input_lv2 = input_lv2 + \"/\"\n",
    "        if not outputdir[len(outputdir) - 1] == \"/\":\n",
    "            outputdir = outputdir + \"/\"\n",
    "        if not os.path.exists(outputdir):\n",
    "            os.mkdir(outputdir)\n",
    "        if existPSSM != \"\":\n",
    "            if not existPSSM[len(existPSSM) - 1] == \"/\":\n",
    "                existPSSM = existPSSM + \"/\"\n",
    "        if ((existPSSM == \"\") or (not os.path.exists(existPSSM))):\n",
    "            ts = calendar.timegm(time.gmtime())\n",
    "            pssmdir = outputdir + str(ts) + \"_pssm/\"\n",
    "            if not os.path.exists(pssmdir):\n",
    "                os.makedirs(pssmdir)\n",
    "            process_input_train(input_lv1 + \"lv1_train.txt\", pssmdir)\n",
    "            process_input_train(input_lv2 + \"lv2_train.txt\", pssmdir)\n",
    "            for foldnum in range(8):\n",
    "                train_MULocDeep(input_lv1, input_lv2, pssmdir, outputdir, foldnum)\n",
    "        else:\n",
    "            for foldnum in range(8):\n",
    "                train_MULocDeep(input_lv1, input_lv2, existPSSM, outputdir, foldnum)\n",
    "    elif model_type==False:\n",
    "        if not input_var[len(input_var) - 1] == \"/\":\n",
    "            input_var = input_var + \"/\"\n",
    "        if not var_outputdir[len(var_outputdir) - 1] == \"/\":\n",
    "            var_outputdir = var_outputdir + \"/\"\n",
    "        if not os.path.exists(var_outputdir):\n",
    "            os.mkdir(var_outputdir)\n",
    "        if existPSSM != \"\":\n",
    "            if not var_existPSSM[len(var_existPSSM) - 1] == \"/\":\n",
    "                var_existPSSM = var_existPSSM + \"/\"\n",
    "        if ((var_existPSSM == \"\") or (not os.path.exists(var_existPSSM))):\n",
    "            ts = calendar.timegm(time.gmtime())\n",
    "            pssmdir = var_outputdir + str(ts) + \"_pssm/\"\n",
    "            if not os.path.exists(pssmdir):\n",
    "                os.makedirs(pssmdir)\n",
    "            process_input_train(input_var + \"processed_deeploc_train_S_seq\", pssmdir)\n",
    "            for foldnum in range(8):\n",
    "                train_var(input_var, pssmdir, var_outputdir, foldnum)\n",
    "        else:\n",
    "            for foldnum in range(8):\n",
    "                train_var(input_var, var_existPSSM, var_outputdir, foldnum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "associate-hearts",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing 0th fold\n",
      "WARNING:tensorflow:From C:\\Users\\vipuser\\.conda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:423: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input (InputLayer)              (None, 1000, 25)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1000, 25)     0           Input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 1000, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1000, 25)     0           dropout_1[0][0]                  \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 1000, 25)     650         lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 1000, 25)     100         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1000, 25)     0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 1000, 25)     0           dropout_2[0][0]                  \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1000, 25)     1900        lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 1000, 25)     100         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 1000, 25)     0           batch_normalization_2[0][0]      \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 1000, 25)     0           lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 1000, 25)     0           dropout_3[0][0]                  \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 1000, 25)     3150        lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1000, 25)     100         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 1000, 25)     0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 1000, 25)     0           dropout_4[0][0]                  \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 1000, 25)     5650        lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 1000, 25)     100         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 1000, 25)     0           batch_normalization_4[0][0]      \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 1000, 25)     0           lambda_6[0][0]                   \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1000, 25)     9400        lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 1000, 25)     100         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 1000, 25)     0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 1000, 25)     0           dropout_6[0][0]                  \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 1000, 25)     13150       lambda_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 1000, 25)     100         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 1000, 25)     0           batch_normalization_6[0][0]      \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 1000, 25)     0           lambda_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 1000, 25)     0           dropout_7[0][0]                  \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 1000, 25)     1900        lambda_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 1000, 25)     100         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 1000, 25)     0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, 1000, 25)     0           dropout_9[0][0]                  \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 1000, 180)    223560      lambda_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 1000, 180)    720         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 1000, 180)    0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)              (None, 1000, 180)    0           dropout_10[0][0]                 \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 1000, 180)    390960      lambda_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 1000, 180)    720         bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 1000, 180)    0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_14 (Lambda)              (None, 1000, 180)    0           dropout_11[0][0]                 \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 1000, 180)    390960      lambda_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 1000, 180)    720         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 1000, 180)    0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_15 (Lambda)              (None, 1000, 180)    0           dropout_12[0][0]                 \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1000, 181)    0           lambda_15[0][0]                  \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         [(None, 41, 180), (N 81549       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 41, 180)      720         attention_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 41, 180)      0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 7380)         0           dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 7380)         0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 80)           590480      dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 10, 8, 1)     0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 10, 8, 1)     4           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 10, 8, 1)     0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 10, 1, 1)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "1ev2 (Reshape)                  (None, 10)           0           max_pooling2d_1[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 1,716,893\n",
      "Trainable params: 1,715,101\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input (InputLayer)              (None, 1000, 25)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1000, 25)     0           Input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 1000, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1000, 25)     0           dropout_1[0][0]                  \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 1000, 25)     650         lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 1000, 25)     100         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1000, 25)     0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 1000, 25)     0           dropout_2[0][0]                  \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1000, 25)     1900        lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 1000, 25)     100         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 1000, 25)     0           batch_normalization_2[0][0]      \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 1000, 25)     0           lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 1000, 25)     0           dropout_3[0][0]                  \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 1000, 25)     3150        lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1000, 25)     100         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 1000, 25)     0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 1000, 25)     0           dropout_4[0][0]                  \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 1000, 25)     5650        lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 1000, 25)     100         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 1000, 25)     0           batch_normalization_4[0][0]      \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 1000, 25)     0           lambda_6[0][0]                   \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1000, 25)     9400        lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 1000, 25)     100         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 1000, 25)     0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 1000, 25)     0           dropout_6[0][0]                  \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 1000, 25)     13150       lambda_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 1000, 25)     100         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 1000, 25)     0           batch_normalization_6[0][0]      \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 1000, 25)     0           lambda_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 1000, 25)     0           dropout_7[0][0]                  \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 1000, 25)     1900        lambda_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 1000, 25)     100         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 1000, 25)     0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, 1000, 25)     0           dropout_9[0][0]                  \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 1000, 180)    223560      lambda_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 1000, 180)    720         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 1000, 180)    0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)              (None, 1000, 180)    0           dropout_10[0][0]                 \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 1000, 180)    390960      lambda_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 1000, 180)    720         bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 1000, 180)    0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_14 (Lambda)              (None, 1000, 180)    0           dropout_11[0][0]                 \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 1000, 180)    390960      lambda_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 1000, 180)    720         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 1000, 180)    0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_15 (Lambda)              (None, 1000, 180)    0           dropout_12[0][0]                 \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1000, 181)    0           lambda_15[0][0]                  \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         [(None, 41, 180), (N 81549       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 41, 180)      720         attention_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 41, 180)      0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 7380)         0           dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 7380)         0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 80)           590480      dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 10, 8, 1)     0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 10, 8, 1)     4           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 10, 8, 1)     0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 10, 1, 1)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lev1 (Reshape)                  (None, 10, 8)        0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "1ev2 (Reshape)                  (None, 10)           0           max_pooling2d_1[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 1,716,893\n",
      "Trainable params: 1,715,101\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n",
      "epoch 0\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\vipuser\\.conda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 175s 10ms/step - loss: 0.7213 - accuracy: 0.7157 - val_loss: 0.7065 - val_accuracy: 0.8130\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.81297, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.70653, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_loss-weights.hdf5\n",
      "epoch 1\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 159s 9ms/step - loss: 0.6259 - accuracy: 0.8270 - val_loss: 0.6110 - val_accuracy: 0.8409\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.81297 to 0.84090, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.70653 to 0.61102, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_loss-weights.hdf5\n",
      "epoch 2\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 182s 11ms/step - loss: 0.5826 - accuracy: 0.8597 - val_loss: 0.5572 - val_accuracy: 0.8645\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.84090 to 0.86454, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.61102 to 0.55720, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_loss-weights.hdf5\n",
      "epoch 3\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 162s 9ms/step - loss: 0.5516 - accuracy: 0.8789 - val_loss: 0.5288 - val_accuracy: 0.8885\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.86454 to 0.88855, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.55720 to 0.52878, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_loss-weights.hdf5\n",
      "epoch 4\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 160s 9ms/step - loss: 0.5265 - accuracy: 0.8888 - val_loss: 0.5085 - val_accuracy: 0.8911\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.88855 to 0.89112, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.52878 to 0.50853, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_loss-weights.hdf5\n",
      "epoch 5\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 155s 9ms/step - loss: 0.5029 - accuracy: 0.8967 - val_loss: 0.4836 - val_accuracy: 0.9024\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.89112 to 0.90237, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.50853 to 0.48360, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_loss-weights.hdf5\n",
      "epoch 6\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 159s 9ms/step - loss: 0.4825 - accuracy: 0.9015 - val_loss: 0.4728 - val_accuracy: 0.8996\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90237\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.48360 to 0.47282, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_loss-weights.hdf5\n",
      "epoch 7\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 161s 9ms/step - loss: 0.4625 - accuracy: 0.9058 - val_loss: 0.4526 - val_accuracy: 0.9044\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.90237 to 0.90438, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.47282 to 0.45258, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_loss-weights.hdf5\n",
      "epoch 8\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 171s 10ms/step - loss: 0.4432 - accuracy: 0.9113 - val_loss: 0.4322 - val_accuracy: 0.9114\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.90438 to 0.91137, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.45258 to 0.43223, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_loss-weights.hdf5\n",
      "epoch 9\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 156s 9ms/step - loss: 0.4248 - accuracy: 0.9140 - val_loss: 0.4155 - val_accuracy: 0.9168\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.91137 to 0.91677, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.43223 to 0.41551, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_loss-weights.hdf5\n",
      "epoch 10\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 157s 9ms/step - loss: 0.4076 - accuracy: 0.9181 - val_loss: 0.3997 - val_accuracy: 0.9185\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.91677 to 0.91853, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.41551 to 0.39975, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_loss-weights.hdf5\n",
      "epoch 11\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 155s 9ms/step - loss: 0.3911 - accuracy: 0.9211 - val_loss: 0.3888 - val_accuracy: 0.9209\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.91853 to 0.92094, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.39975 to 0.38876, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_loss-weights.hdf5\n",
      "epoch 12\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 156s 9ms/step - loss: 0.3754 - accuracy: 0.9248 - val_loss: 0.3724 - val_accuracy: 0.9207\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92094\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.38876 to 0.37238, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_loss-weights.hdf5\n",
      "epoch 13\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 156s 9ms/step - loss: 0.3616 - accuracy: 0.9262 - val_loss: 0.3638 - val_accuracy: 0.9222\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92094 to 0.92225, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.37238 to 0.36380, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_loss-weights.hdf5\n",
      "epoch 14\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 175s 10ms/step - loss: 0.3474 - accuracy: 0.9285 - val_loss: 0.3580 - val_accuracy: 0.9182\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92225\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.36380 to 0.35803, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_loss-weights.hdf5\n",
      "epoch 15\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 156s 9ms/step - loss: 0.3341 - accuracy: 0.9307 - val_loss: 0.3394 - val_accuracy: 0.9255\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92225 to 0.92548, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.35803 to 0.33939, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_loss-weights.hdf5\n",
      "epoch 16\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 154s 9ms/step - loss: 0.3208 - accuracy: 0.9326 - val_loss: 0.3304 - val_accuracy: 0.9272\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92548 to 0.92720, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.33939 to 0.33040, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_loss-weights.hdf5\n",
      "epoch 17\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 156s 9ms/step - loss: 0.3097 - accuracy: 0.9341 - val_loss: 0.3257 - val_accuracy: 0.9262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92720\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.33040 to 0.32574, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_loss-weights.hdf5\n",
      "epoch 18\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 155s 9ms/step - loss: 0.2979 - accuracy: 0.9363 - val_loss: 0.3153 - val_accuracy: 0.9261\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92720\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.32574 to 0.31531, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_loss-weights.hdf5\n",
      "epoch 19\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 156s 9ms/step - loss: 0.2880 - accuracy: 0.9382 - val_loss: 0.3080 - val_accuracy: 0.9238\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92720\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.31531 to 0.30795, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_loss-weights.hdf5\n",
      "epoch 20\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 154s 9ms/step - loss: 0.2774 - accuracy: 0.9397 - val_loss: 0.3066 - val_accuracy: 0.9267\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92720\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.30795 to 0.30657, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_loss-weights.hdf5\n",
      "epoch 21\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 172s 10ms/step - loss: 0.2679 - accuracy: 0.9413 - val_loss: 0.2946 - val_accuracy: 0.9275\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92720 to 0.92753, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.30657 to 0.29458, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_loss-weights.hdf5\n",
      "epoch 22\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 155s 9ms/step - loss: 0.2596 - accuracy: 0.9427 - val_loss: 0.2912 - val_accuracy: 0.9280\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92753 to 0.92802, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.29458 to 0.29116, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_loss-weights.hdf5\n",
      "epoch 23\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 157s 9ms/step - loss: 0.2502 - accuracy: 0.9440 - val_loss: 0.2803 - val_accuracy: 0.9282\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92802 to 0.92822, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.29116 to 0.28031, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_loss-weights.hdf5\n",
      "epoch 24\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 156s 9ms/step - loss: 0.2436 - accuracy: 0.9448 - val_loss: 0.2757 - val_accuracy: 0.9269\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92822\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.28031 to 0.27574, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_loss-weights.hdf5\n",
      "epoch 25\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 159s 9ms/step - loss: 0.2351 - accuracy: 0.9464 - val_loss: 0.2727 - val_accuracy: 0.9267\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92822\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.27574 to 0.27275, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_loss-weights.hdf5\n",
      "epoch 26\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 162s 9ms/step - loss: 0.2276 - accuracy: 0.9485 - val_loss: 0.2676 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92822 to 0.92863, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.27275 to 0.26760, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_loss-weights.hdf5\n",
      "epoch 27\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 174s 10ms/step - loss: 0.2215 - accuracy: 0.9487 - val_loss: 0.2859 - val_accuracy: 0.9209\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92863\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26760\n",
      "epoch 28\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 157s 9ms/step - loss: 0.2158 - accuracy: 0.9498 - val_loss: 0.2609 - val_accuracy: 0.9282\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92863\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.26760 to 0.26087, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_loss-weights.hdf5\n",
      "epoch 29\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 156s 9ms/step - loss: 0.2076 - accuracy: 0.9513 - val_loss: 0.2593 - val_accuracy: 0.9280\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92863\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.26087 to 0.25935, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_loss-weights.hdf5\n",
      "epoch 30\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 154s 9ms/step - loss: 0.2013 - accuracy: 0.9529 - val_loss: 0.2616 - val_accuracy: 0.9288\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92863 to 0.92875, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.25935\n",
      "epoch 31\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 163s 10ms/step - loss: 0.1946 - accuracy: 0.9546 - val_loss: 0.2534 - val_accuracy: 0.9288\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92875\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.25935 to 0.25340, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_loss-weights.hdf5\n",
      "epoch 32\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 165s 10ms/step - loss: 0.1898 - accuracy: 0.9553 - val_loss: 0.2530 - val_accuracy: 0.9293\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92875 to 0.92928, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.25340 to 0.25297, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_loss-weights.hdf5\n",
      "epoch 33\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 176s 10ms/step - loss: 0.1837 - accuracy: 0.9570 - val_loss: 0.2679 - val_accuracy: 0.9267\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92928\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.25297\n",
      "epoch 34\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 162s 9ms/step - loss: 0.1809 - accuracy: 0.9565 - val_loss: 0.2499 - val_accuracy: 0.9290\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92928\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.25297 to 0.24988, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_loss-weights.hdf5\n",
      "epoch 35\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 161s 9ms/step - loss: 0.1744 - accuracy: 0.9586 - val_loss: 0.2481 - val_accuracy: 0.9281\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92928\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.24988 to 0.24808, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_loss-weights.hdf5\n",
      "epoch 36\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 160s 9ms/step - loss: 0.1692 - accuracy: 0.9596 - val_loss: 0.2489 - val_accuracy: 0.9261\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92928\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24808\n",
      "epoch 37\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 164s 10ms/step - loss: 0.1655 - accuracy: 0.9598 - val_loss: 0.2504 - val_accuracy: 0.9308\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92928 to 0.93084, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24808\n",
      "epoch 38\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17138/17138 [==============================] - 171s 10ms/step - loss: 0.1613 - accuracy: 0.9605 - val_loss: 0.2403 - val_accuracy: 0.9299\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93084\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.24808 to 0.24030, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_loss-weights.hdf5\n",
      "epoch 39\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 168s 10ms/step - loss: 0.1567 - accuracy: 0.9616 - val_loss: 0.2419 - val_accuracy: 0.9298\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93084\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24030\n",
      "epoch 40\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 164s 10ms/step - loss: 0.1520 - accuracy: 0.9632 - val_loss: 0.2431 - val_accuracy: 0.9289\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93084\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24030\n",
      "epoch 41\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 172s 10ms/step - loss: 0.1488 - accuracy: 0.9638 - val_loss: 0.2425 - val_accuracy: 0.9281\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93084\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24030\n",
      "epoch 42\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 169s 10ms/step - loss: 0.1453 - accuracy: 0.9646 - val_loss: 0.2439 - val_accuracy: 0.9268\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93084\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24030\n",
      "epoch 43\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 170s 10ms/step - loss: 0.1425 - accuracy: 0.9641 - val_loss: 0.2424 - val_accuracy: 0.9277\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93084\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24030\n",
      "epoch 44\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 179s 10ms/step - loss: 0.1382 - accuracy: 0.9656 - val_loss: 0.2456 - val_accuracy: 0.9259\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93084\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24030\n",
      "epoch 45\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 170s 10ms/step - loss: 0.1364 - accuracy: 0.9654 - val_loss: 0.2422 - val_accuracy: 0.9276\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93084\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24030\n",
      "epoch 46\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 167s 10ms/step - loss: 0.1323 - accuracy: 0.9665 - val_loss: 0.2518 - val_accuracy: 0.9260\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93084\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24030\n",
      "epoch 47\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 169s 10ms/step - loss: 0.1288 - accuracy: 0.9679 - val_loss: 0.2472 - val_accuracy: 0.9283\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93084\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24030\n",
      "epoch 48\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 172s 10ms/step - loss: 0.1248 - accuracy: 0.9685 - val_loss: 0.2445 - val_accuracy: 0.9285\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93084\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24030\n",
      "epoch 49\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 171s 10ms/step - loss: 0.1233 - accuracy: 0.9687 - val_loss: 0.2457 - val_accuracy: 0.9297\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93084\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24030\n",
      "epoch 50\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 185s 11ms/step - loss: 0.1201 - accuracy: 0.9696 - val_loss: 0.2403 - val_accuracy: 0.9272\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93084\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.24030 to 0.24026, saving model to D:/mulocdeep/lv1_result44/fold0_big_lv1_loss-weights.hdf5\n",
      "epoch 51\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 171s 10ms/step - loss: 0.1211 - accuracy: 0.9682 - val_loss: 0.2585 - val_accuracy: 0.9238\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93084\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24026\n",
      "epoch 52\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 171s 10ms/step - loss: 0.1179 - accuracy: 0.9700 - val_loss: 0.2424 - val_accuracy: 0.9260\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93084\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24026\n",
      "epoch 53\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 170s 10ms/step - loss: 0.1140 - accuracy: 0.9705 - val_loss: 0.2544 - val_accuracy: 0.9244\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93084\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24026\n",
      "epoch 54\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 170s 10ms/step - loss: 0.1107 - accuracy: 0.9717 - val_loss: 0.2472 - val_accuracy: 0.9283\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93084\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24026\n",
      "epoch 55\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 185s 11ms/step - loss: 0.1079 - accuracy: 0.9726 - val_loss: 0.2533 - val_accuracy: 0.9255\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93084\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24026\n",
      "epoch 56\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 173s 10ms/step - loss: 0.1062 - accuracy: 0.9725 - val_loss: 0.2460 - val_accuracy: 0.9279\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93084\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24026\n",
      "epoch 57\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 172s 10ms/step - loss: 0.1024 - accuracy: 0.9741 - val_loss: 0.2486 - val_accuracy: 0.9272\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93084\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24026\n",
      "epoch 58\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 169s 10ms/step - loss: 0.0998 - accuracy: 0.9746 - val_loss: 0.2485 - val_accuracy: 0.9284\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93084\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24026\n",
      "epoch 59\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 166s 10ms/step - loss: 0.0984 - accuracy: 0.9750 - val_loss: 0.2558 - val_accuracy: 0.9270\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93084\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24026\n",
      "epoch 60\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 165s 10ms/step - loss: 0.0963 - accuracy: 0.9751 - val_loss: 0.2652 - val_accuracy: 0.9258\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93084\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24026\n",
      "epoch 61\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 163s 10ms/step - loss: 0.0952 - accuracy: 0.9753 - val_loss: 0.2687 - val_accuracy: 0.9258\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93084\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24026\n",
      "epoch 62\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 161s 9ms/step - loss: 0.0919 - accuracy: 0.9764 - val_loss: 0.2602 - val_accuracy: 0.9283\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93084\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24026\n",
      "epoch 63\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 162s 9ms/step - loss: 0.0926 - accuracy: 0.9757 - val_loss: 0.2652 - val_accuracy: 0.9254\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93084\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24026\n",
      "epoch 64\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17138/17138 [==============================] - 162s 9ms/step - loss: 0.0897 - accuracy: 0.9766 - val_loss: 0.2613 - val_accuracy: 0.9253\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93084\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24026\n",
      "epoch 65\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 158s 9ms/step - loss: 0.0876 - accuracy: 0.9775 - val_loss: 0.2537 - val_accuracy: 0.9287\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93084\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24026\n",
      "epoch 66\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 153s 9ms/step - loss: 0.0848 - accuracy: 0.9782 - val_loss: 0.2588 - val_accuracy: 0.9272\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93084\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24026\n",
      "epoch 67\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 164s 10ms/step - loss: 0.0826 - accuracy: 0.9787 - val_loss: 0.2644 - val_accuracy: 0.9262\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93084\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24026\n",
      "epoch 68\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 154s 9ms/step - loss: 0.0821 - accuracy: 0.9783 - val_loss: 0.2753 - val_accuracy: 0.9235\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93084\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24026\n",
      "epoch 69\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 154s 9ms/step - loss: 0.0804 - accuracy: 0.9792 - val_loss: 0.2683 - val_accuracy: 0.9291\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93084\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24026\n",
      "epoch 70\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 154s 9ms/step - loss: 0.0776 - accuracy: 0.9802 - val_loss: 0.2881 - val_accuracy: 0.9244\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93084\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24026\n",
      "epoch 71\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 154s 9ms/step - loss: 0.0761 - accuracy: 0.9804 - val_loss: 0.2748 - val_accuracy: 0.9288\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93084\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24026\n",
      "epoch 72\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 154s 9ms/step - loss: 0.0750 - accuracy: 0.9807 - val_loss: 0.2748 - val_accuracy: 0.9267\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93084\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24026\n",
      "epoch 73\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 169s 10ms/step - loss: 0.0750 - accuracy: 0.9809 - val_loss: 0.2786 - val_accuracy: 0.9258\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93084\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24026\n",
      "epoch 74\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 155s 9ms/step - loss: 0.0730 - accuracy: 0.9814 - val_loss: 0.2795 - val_accuracy: 0.9275\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93084\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24026\n",
      "epoch 75\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 154s 9ms/step - loss: 0.0716 - accuracy: 0.9817 - val_loss: 0.2836 - val_accuracy: 0.9283\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93084\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24026\n",
      "epoch 76\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 155s 9ms/step - loss: 0.0700 - accuracy: 0.9822 - val_loss: 0.2913 - val_accuracy: 0.9270\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93084\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24026\n",
      "epoch 77\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 155s 9ms/step - loss: 0.0680 - accuracy: 0.9829 - val_loss: 0.2906 - val_accuracy: 0.9271\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93084\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24026\n",
      "epoch 78\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 155s 9ms/step - loss: 0.0657 - accuracy: 0.9835 - val_loss: 0.2790 - val_accuracy: 0.9284\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93084\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24026\n",
      "epoch 79\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 170s 10ms/step - loss: 0.0660 - accuracy: 0.9831 - val_loss: 0.2868 - val_accuracy: 0.9268\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93084\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24026\n",
      "doing 1th fold\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input (InputLayer)              (None, 1000, 25)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 1000, 25)     0           Input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1000, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)              (None, 1000, 25)     0           dropout_15[0][0]                 \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 1000, 25)     650         lambda_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 1000, 25)     100         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 1000, 25)     0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_17 (Lambda)              (None, 1000, 25)     0           dropout_16[0][0]                 \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 1000, 25)     1900        lambda_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 1000, 25)     100         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_18 (Lambda)              (None, 1000, 25)     0           batch_normalization_15[0][0]     \n",
      "                                                                 dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 1000, 25)     0           lambda_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_19 (Lambda)              (None, 1000, 25)     0           dropout_17[0][0]                 \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 1000, 25)     3150        lambda_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 1000, 25)     100         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 1000, 25)     0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_20 (Lambda)              (None, 1000, 25)     0           dropout_18[0][0]                 \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 1000, 25)     5650        lambda_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 1000, 25)     100         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_21 (Lambda)              (None, 1000, 25)     0           batch_normalization_17[0][0]     \n",
      "                                                                 dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_22 (Lambda)              (None, 1000, 25)     0           lambda_21[0][0]                  \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 1000, 25)     9400        lambda_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 1000, 25)     100         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 1000, 25)     0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_23 (Lambda)              (None, 1000, 25)     0           dropout_20[0][0]                 \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1000, 25)     13150       lambda_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 1000, 25)     100         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_24 (Lambda)              (None, 1000, 25)     0           batch_normalization_19[0][0]     \n",
      "                                                                 dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 1000, 25)     0           lambda_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_26 (Lambda)              (None, 1000, 25)     0           dropout_21[0][0]                 \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 1000, 25)     1900        lambda_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 1000, 25)     100         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 1000, 25)     0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_27 (Lambda)              (None, 1000, 25)     0           dropout_23[0][0]                 \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 1000, 180)    223560      lambda_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 1000, 180)    720         bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 1000, 180)    0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_28 (Lambda)              (None, 1000, 180)    0           dropout_24[0][0]                 \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 1000, 180)    390960      lambda_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 1000, 180)    720         bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 1000, 180)    0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_29 (Lambda)              (None, 1000, 180)    0           dropout_25[0][0]                 \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 1000, 180)    390960      lambda_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 1000, 180)    720         bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 1000, 180)    0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_30 (Lambda)              (None, 1000, 180)    0           dropout_26[0][0]                 \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1000, 181)    0           lambda_30[0][0]                  \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         [(None, 41, 180), (N 81549       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 41, 180)      720         attention_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 41, 180)      0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 7380)         0           dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 7380)         0           flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 80)           590480      dropout_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 10, 8, 1)     0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 10, 8, 1)     4           reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 10, 8, 1)     0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 10, 1, 1)     0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "1ev2 (Reshape)                  (None, 10)           0           max_pooling2d_2[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 1,716,893\n",
      "Trainable params: 1,715,101\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input (InputLayer)              (None, 1000, 25)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 1000, 25)     0           Input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1000, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)              (None, 1000, 25)     0           dropout_15[0][0]                 \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 1000, 25)     650         lambda_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 1000, 25)     100         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 1000, 25)     0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_17 (Lambda)              (None, 1000, 25)     0           dropout_16[0][0]                 \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 1000, 25)     1900        lambda_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 1000, 25)     100         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_18 (Lambda)              (None, 1000, 25)     0           batch_normalization_15[0][0]     \n",
      "                                                                 dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 1000, 25)     0           lambda_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_19 (Lambda)              (None, 1000, 25)     0           dropout_17[0][0]                 \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 1000, 25)     3150        lambda_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 1000, 25)     100         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 1000, 25)     0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_20 (Lambda)              (None, 1000, 25)     0           dropout_18[0][0]                 \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 1000, 25)     5650        lambda_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 1000, 25)     100         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_21 (Lambda)              (None, 1000, 25)     0           batch_normalization_17[0][0]     \n",
      "                                                                 dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_22 (Lambda)              (None, 1000, 25)     0           lambda_21[0][0]                  \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 1000, 25)     9400        lambda_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 1000, 25)     100         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 1000, 25)     0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_23 (Lambda)              (None, 1000, 25)     0           dropout_20[0][0]                 \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1000, 25)     13150       lambda_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 1000, 25)     100         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_24 (Lambda)              (None, 1000, 25)     0           batch_normalization_19[0][0]     \n",
      "                                                                 dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 1000, 25)     0           lambda_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_26 (Lambda)              (None, 1000, 25)     0           dropout_21[0][0]                 \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 1000, 25)     1900        lambda_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 1000, 25)     100         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 1000, 25)     0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_27 (Lambda)              (None, 1000, 25)     0           dropout_23[0][0]                 \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 1000, 180)    223560      lambda_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 1000, 180)    720         bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 1000, 180)    0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_28 (Lambda)              (None, 1000, 180)    0           dropout_24[0][0]                 \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 1000, 180)    390960      lambda_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 1000, 180)    720         bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 1000, 180)    0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_29 (Lambda)              (None, 1000, 180)    0           dropout_25[0][0]                 \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 1000, 180)    390960      lambda_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 1000, 180)    720         bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 1000, 180)    0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_30 (Lambda)              (None, 1000, 180)    0           dropout_26[0][0]                 \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1000, 181)    0           lambda_30[0][0]                  \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         [(None, 41, 180), (N 81549       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 41, 180)      720         attention_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 41, 180)      0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 7380)         0           dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 7380)         0           flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 80)           590480      dropout_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 10, 8, 1)     0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 10, 8, 1)     4           reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 10, 8, 1)     0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 10, 1, 1)     0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lev1 (Reshape)                  (None, 10, 8)        0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "1ev2 (Reshape)                  (None, 10)           0           max_pooling2d_2[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 1,716,893\n",
      "Trainable params: 1,715,101\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n",
      "epoch 0\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 170s 10ms/step - loss: 0.7367 - accuracy: 0.6999 - val_loss: 0.6651 - val_accuracy: 0.7930\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.79297, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.66512, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_loss-weights.hdf5\n",
      "epoch 1\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 159s 9ms/step - loss: 0.6335 - accuracy: 0.8216 - val_loss: 0.6332 - val_accuracy: 0.8301\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.79297 to 0.83010, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.66512 to 0.63323, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_loss-weights.hdf5\n",
      "epoch 2\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 157s 9ms/step - loss: 0.5870 - accuracy: 0.8584 - val_loss: 0.5874 - val_accuracy: 0.8737\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.83010 to 0.87370, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.63323 to 0.58739, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_loss-weights.hdf5\n",
      "epoch 3\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 157s 9ms/step - loss: 0.5537 - accuracy: 0.8781 - val_loss: 0.5490 - val_accuracy: 0.8820\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.87370 to 0.88204, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.58739 to 0.54900, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_loss-weights.hdf5\n",
      "epoch 4\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 175s 10ms/step - loss: 0.5262 - accuracy: 0.8898 - val_loss: 0.5195 - val_accuracy: 0.8967\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.88204 to 0.89669, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.54900 to 0.51954, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_loss-weights.hdf5\n",
      "epoch 5\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 155s 9ms/step - loss: 0.5018 - accuracy: 0.8976 - val_loss: 0.4970 - val_accuracy: 0.8951\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89669\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.51954 to 0.49696, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_loss-weights.hdf5\n",
      "epoch 6\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 155s 9ms/step - loss: 0.4798 - accuracy: 0.9035 - val_loss: 0.4959 - val_accuracy: 0.9025\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.89669 to 0.90245, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.49696 to 0.49587, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_loss-weights.hdf5\n",
      "epoch 7\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 156s 9ms/step - loss: 0.4600 - accuracy: 0.9082 - val_loss: 0.4644 - val_accuracy: 0.9020\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90245\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.49587 to 0.46436, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_loss-weights.hdf5\n",
      "epoch 8\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 154s 9ms/step - loss: 0.4403 - accuracy: 0.9130 - val_loss: 0.4480 - val_accuracy: 0.9054\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.90245 to 0.90536, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.46436 to 0.44796, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_loss-weights.hdf5\n",
      "epoch 9\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 157s 9ms/step - loss: 0.4214 - accuracy: 0.9172 - val_loss: 0.4236 - val_accuracy: 0.9105\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.90536 to 0.91047, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.44796 to 0.42360, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_loss-weights.hdf5\n",
      "epoch 10\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 169s 10ms/step - loss: 0.4048 - accuracy: 0.9202 - val_loss: 0.4162 - val_accuracy: 0.9086\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.91047\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.42360 to 0.41620, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_loss-weights.hdf5\n",
      "epoch 11\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 159s 9ms/step - loss: 0.3880 - accuracy: 0.9244 - val_loss: 0.4031 - val_accuracy: 0.9122\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.91047 to 0.91219, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.41620 to 0.40311, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_loss-weights.hdf5\n",
      "epoch 12\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 157s 9ms/step - loss: 0.3721 - accuracy: 0.9268 - val_loss: 0.3833 - val_accuracy: 0.9146\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.91219 to 0.91460, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.40311 to 0.38329, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_loss-weights.hdf5\n",
      "epoch 13\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 157s 9ms/step - loss: 0.3567 - accuracy: 0.9298 - val_loss: 0.3785 - val_accuracy: 0.9135\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.91460\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.38329 to 0.37847, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_loss-weights.hdf5\n",
      "epoch 14\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 162s 9ms/step - loss: 0.3429 - accuracy: 0.9318 - val_loss: 0.3635 - val_accuracy: 0.9183\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.91460 to 0.91832, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.37847 to 0.36347, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_loss-weights.hdf5\n",
      "epoch 15\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 155s 9ms/step - loss: 0.3298 - accuracy: 0.9341 - val_loss: 0.3610 - val_accuracy: 0.9202\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.91832 to 0.92025, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.36347 to 0.36101, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_loss-weights.hdf5\n",
      "epoch 16\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 166s 10ms/step - loss: 0.3173 - accuracy: 0.9365 - val_loss: 0.3380 - val_accuracy: 0.9225\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92025 to 0.92249, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.36101 to 0.33802, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_loss-weights.hdf5\n",
      "epoch 17\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 154s 9ms/step - loss: 0.3056 - accuracy: 0.9378 - val_loss: 0.3413 - val_accuracy: 0.9221\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92249\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.33802\n",
      "epoch 18\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 153s 9ms/step - loss: 0.2936 - accuracy: 0.9399 - val_loss: 0.3277 - val_accuracy: 0.9194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92249\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.33802 to 0.32772, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_loss-weights.hdf5\n",
      "epoch 19\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 153s 9ms/step - loss: 0.2836 - accuracy: 0.9413 - val_loss: 0.3180 - val_accuracy: 0.9211\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92249\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.32772 to 0.31801, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_loss-weights.hdf5\n",
      "epoch 20\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 155s 9ms/step - loss: 0.2730 - accuracy: 0.9425 - val_loss: 0.3377 - val_accuracy: 0.9184\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92249\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.31801\n",
      "epoch 21\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 153s 9ms/step - loss: 0.2639 - accuracy: 0.9442 - val_loss: 0.3097 - val_accuracy: 0.9223\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92249\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.31801 to 0.30971, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_loss-weights.hdf5\n",
      "epoch 22\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 166s 10ms/step - loss: 0.2560 - accuracy: 0.9451 - val_loss: 0.2978 - val_accuracy: 0.9228\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92249 to 0.92278, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.30971 to 0.29784, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_loss-weights.hdf5\n",
      "epoch 23\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 156s 9ms/step - loss: 0.2458 - accuracy: 0.9470 - val_loss: 0.3020 - val_accuracy: 0.9234\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92278 to 0.92339, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.29784\n",
      "epoch 24\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 157s 9ms/step - loss: 0.2378 - accuracy: 0.9487 - val_loss: 0.3091 - val_accuracy: 0.9207\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92339\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.29784\n",
      "epoch 25\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 156s 9ms/step - loss: 0.2298 - accuracy: 0.9498 - val_loss: 0.3107 - val_accuracy: 0.9225\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92339\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.29784\n",
      "epoch 26\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 156s 9ms/step - loss: 0.2243 - accuracy: 0.9506 - val_loss: 0.2876 - val_accuracy: 0.9235\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92339 to 0.92348, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.29784 to 0.28763, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_loss-weights.hdf5\n",
      "epoch 27\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 160s 9ms/step - loss: 0.2154 - accuracy: 0.9523 - val_loss: 0.2893 - val_accuracy: 0.9173\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92348\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.28763\n",
      "epoch 28\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 170s 10ms/step - loss: 0.2101 - accuracy: 0.9528 - val_loss: 0.2801 - val_accuracy: 0.9242\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92348 to 0.92421, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.28763 to 0.28008, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_loss-weights.hdf5\n",
      "epoch 29\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 161s 9ms/step - loss: 0.2018 - accuracy: 0.9549 - val_loss: 0.2822 - val_accuracy: 0.9246\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92421 to 0.92458, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.28008\n",
      "epoch 30\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 158s 9ms/step - loss: 0.1974 - accuracy: 0.9548 - val_loss: 0.2725 - val_accuracy: 0.9266\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92458 to 0.92658, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.28008 to 0.27251, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_loss-weights.hdf5\n",
      "epoch 31\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 159s 9ms/step - loss: 0.1904 - accuracy: 0.9566 - val_loss: 0.2745 - val_accuracy: 0.9234\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92658\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.27251\n",
      "epoch 32\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 155s 9ms/step - loss: 0.1846 - accuracy: 0.9577 - val_loss: 0.2706 - val_accuracy: 0.9254\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92658\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.27251 to 0.27060, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_loss-weights.hdf5\n",
      "epoch 33\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 158s 9ms/step - loss: 0.1786 - accuracy: 0.9592 - val_loss: 0.2742 - val_accuracy: 0.9244\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92658\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.27060\n",
      "epoch 34\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 167s 10ms/step - loss: 0.1736 - accuracy: 0.9602 - val_loss: 0.2665 - val_accuracy: 0.9255\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92658\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.27060 to 0.26652, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_loss-weights.hdf5\n",
      "epoch 35\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 158s 9ms/step - loss: 0.1694 - accuracy: 0.9608 - val_loss: 0.2665 - val_accuracy: 0.9250\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92658\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.26652 to 0.26650, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_loss-weights.hdf5\n",
      "epoch 36\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 161s 9ms/step - loss: 0.1749 - accuracy: 0.9564 - val_loss: 0.2777 - val_accuracy: 0.9227\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92658\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26650\n",
      "epoch 37\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 161s 9ms/step - loss: 0.1657 - accuracy: 0.9598 - val_loss: 0.2625 - val_accuracy: 0.9247\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92658\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.26650 to 0.26249, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_loss-weights.hdf5\n",
      "epoch 38\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 163s 10ms/step - loss: 0.1581 - accuracy: 0.9622 - val_loss: 0.2663 - val_accuracy: 0.9227\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92658\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26249\n",
      "epoch 39\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 171s 10ms/step - loss: 0.1547 - accuracy: 0.9630 - val_loss: 0.2617 - val_accuracy: 0.9230\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92658\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.26249 to 0.26171, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_loss-weights.hdf5\n",
      "epoch 40\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17138/17138 [==============================] - 168s 10ms/step - loss: 0.1497 - accuracy: 0.9634 - val_loss: 0.2801 - val_accuracy: 0.9260\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92658\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26171\n",
      "epoch 41\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 163s 10ms/step - loss: 0.1449 - accuracy: 0.9652 - val_loss: 0.2694 - val_accuracy: 0.9247\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92658\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26171\n",
      "epoch 42\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 166s 10ms/step - loss: 0.1415 - accuracy: 0.9658 - val_loss: 0.2602 - val_accuracy: 0.9261\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92658\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.26171 to 0.26019, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_loss-weights.hdf5\n",
      "epoch 43\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 164s 10ms/step - loss: 0.1417 - accuracy: 0.9650 - val_loss: 0.2755 - val_accuracy: 0.9239\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92658\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26019\n",
      "epoch 44\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 164s 10ms/step - loss: 0.1346 - accuracy: 0.9671 - val_loss: 0.2607 - val_accuracy: 0.9245\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92658\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26019\n",
      "epoch 45\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 171s 10ms/step - loss: 0.1363 - accuracy: 0.9657 - val_loss: 0.2817 - val_accuracy: 0.9231\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92658\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26019\n",
      "epoch 46\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 162s 9ms/step - loss: 0.1302 - accuracy: 0.9679 - val_loss: 0.2703 - val_accuracy: 0.9260\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92658\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26019\n",
      "epoch 47\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 161s 9ms/step - loss: 0.1271 - accuracy: 0.9686 - val_loss: 0.2609 - val_accuracy: 0.9245\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92658\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26019\n",
      "epoch 48\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 163s 9ms/step - loss: 0.1233 - accuracy: 0.9693 - val_loss: 0.2736 - val_accuracy: 0.9223\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92658\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26019\n",
      "epoch 49\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 161s 9ms/step - loss: 0.1184 - accuracy: 0.9708 - val_loss: 0.2771 - val_accuracy: 0.9224\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92658\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26019\n",
      "epoch 50\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 161s 9ms/step - loss: 0.1179 - accuracy: 0.9704 - val_loss: 0.2618 - val_accuracy: 0.9217\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92658\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26019\n",
      "epoch 51\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 173s 10ms/step - loss: 0.1153 - accuracy: 0.9714 - val_loss: 0.2637 - val_accuracy: 0.9211\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92658\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26019\n",
      "epoch 52\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 165s 10ms/step - loss: 0.1142 - accuracy: 0.9714 - val_loss: 0.2991 - val_accuracy: 0.9175\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92658\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26019\n",
      "epoch 53\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 164s 10ms/step - loss: 0.1175 - accuracy: 0.9693 - val_loss: 0.2726 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92658\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26019\n",
      "epoch 54\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 161s 9ms/step - loss: 0.1105 - accuracy: 0.9715 - val_loss: 0.2712 - val_accuracy: 0.9251\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92658\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26019\n",
      "epoch 55\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 159s 9ms/step - loss: 0.1066 - accuracy: 0.9731 - val_loss: 0.2710 - val_accuracy: 0.9241\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92658\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26019\n",
      "epoch 56\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 159s 9ms/step - loss: 0.1034 - accuracy: 0.9741 - val_loss: 0.2707 - val_accuracy: 0.9243\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92658\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26019\n",
      "epoch 57\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 173s 10ms/step - loss: 0.1015 - accuracy: 0.9740 - val_loss: 0.2851 - val_accuracy: 0.9203\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92658\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26019\n",
      "epoch 58\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 166s 10ms/step - loss: 0.0978 - accuracy: 0.9750 - val_loss: 0.2733 - val_accuracy: 0.9251\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92658\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26019\n",
      "epoch 59\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 161s 9ms/step - loss: 0.0967 - accuracy: 0.9755 - val_loss: 0.2711 - val_accuracy: 0.9267\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92658 to 0.92675, saving model to D:/mulocdeep/lv1_result44/fold1_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26019\n",
      "epoch 60\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 164s 10ms/step - loss: 0.0949 - accuracy: 0.9758 - val_loss: 0.2701 - val_accuracy: 0.9245\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92675\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26019\n",
      "epoch 61\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 161s 9ms/step - loss: 0.0935 - accuracy: 0.9757 - val_loss: 0.2901 - val_accuracy: 0.9226\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92675\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26019\n",
      "epoch 62\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 172s 10ms/step - loss: 0.0905 - accuracy: 0.9769 - val_loss: 0.2955 - val_accuracy: 0.9207\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92675\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26019\n",
      "epoch 63\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 170s 10ms/step - loss: 0.0906 - accuracy: 0.9763 - val_loss: 0.2988 - val_accuracy: 0.9212\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92675\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26019\n",
      "epoch 64\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 174s 10ms/step - loss: 0.0873 - accuracy: 0.9776 - val_loss: 0.2900 - val_accuracy: 0.9215\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92675\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26019\n",
      "epoch 65\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 165s 10ms/step - loss: 0.0866 - accuracy: 0.9779 - val_loss: 0.2881 - val_accuracy: 0.9228\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92675\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26019\n",
      "epoch 66\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17138/17138 [==============================] - 163s 10ms/step - loss: 0.0831 - accuracy: 0.9792 - val_loss: 0.3080 - val_accuracy: 0.9209\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92675\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26019\n",
      "epoch 67\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 164s 10ms/step - loss: 0.0855 - accuracy: 0.9777 - val_loss: 0.2956 - val_accuracy: 0.9205\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92675\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26019\n",
      "epoch 68\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 176s 10ms/step - loss: 0.0816 - accuracy: 0.9791 - val_loss: 0.2929 - val_accuracy: 0.9209\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92675\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26019\n",
      "epoch 69\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 164s 10ms/step - loss: 0.0789 - accuracy: 0.9798 - val_loss: 0.2868 - val_accuracy: 0.9215\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92675\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26019\n",
      "epoch 70\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 164s 10ms/step - loss: 0.0782 - accuracy: 0.9795 - val_loss: 0.2870 - val_accuracy: 0.9212\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92675\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26019\n",
      "epoch 71\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 166s 10ms/step - loss: 0.0767 - accuracy: 0.9803 - val_loss: 0.3057 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92675\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26019\n",
      "epoch 72\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 164s 10ms/step - loss: 0.0747 - accuracy: 0.9808 - val_loss: 0.2936 - val_accuracy: 0.9244\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92675\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26019\n",
      "epoch 73\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 164s 10ms/step - loss: 0.0723 - accuracy: 0.9814 - val_loss: 0.3020 - val_accuracy: 0.9220\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92675\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26019\n",
      "epoch 74\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 176s 10ms/step - loss: 0.0706 - accuracy: 0.9824 - val_loss: 0.2973 - val_accuracy: 0.9217\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92675\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26019\n",
      "epoch 75\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 164s 10ms/step - loss: 0.0694 - accuracy: 0.9823 - val_loss: 0.3059 - val_accuracy: 0.9207\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92675\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26019\n",
      "epoch 76\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 161s 9ms/step - loss: 0.0682 - accuracy: 0.9826 - val_loss: 0.3154 - val_accuracy: 0.9220\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92675\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26019\n",
      "epoch 77\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 163s 9ms/step - loss: 0.0667 - accuracy: 0.9832 - val_loss: 0.3094 - val_accuracy: 0.9240\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92675\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26019\n",
      "epoch 78\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 159s 9ms/step - loss: 0.0656 - accuracy: 0.9835 - val_loss: 0.3045 - val_accuracy: 0.9245\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92675\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26019\n",
      "epoch 79\n",
      "\n",
      "Train on 17138 samples, validate on 2445 samples\n",
      "Epoch 1/1\n",
      "17138/17138 [==============================] - 168s 10ms/step - loss: 0.0637 - accuracy: 0.9840 - val_loss: 0.2992 - val_accuracy: 0.9240\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92675\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26019\n",
      "doing 2th fold\n",
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input (InputLayer)              (None, 1000, 25)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 1000, 25)     0           Input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1000, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_31 (Lambda)              (None, 1000, 25)     0           dropout_29[0][0]                 \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 1000, 25)     650         lambda_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 1000, 25)     100         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)            (None, 1000, 25)     0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_32 (Lambda)              (None, 1000, 25)     0           dropout_30[0][0]                 \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 1000, 25)     1900        lambda_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 1000, 25)     100         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_33 (Lambda)              (None, 1000, 25)     0           batch_normalization_28[0][0]     \n",
      "                                                                 dropout_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, 1000, 25)     0           lambda_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_34 (Lambda)              (None, 1000, 25)     0           dropout_31[0][0]                 \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 1000, 25)     3150        lambda_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 1000, 25)     100         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)            (None, 1000, 25)     0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_35 (Lambda)              (None, 1000, 25)     0           dropout_32[0][0]                 \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1000, 25)     5650        lambda_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 1000, 25)     100         conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_36 (Lambda)              (None, 1000, 25)     0           batch_normalization_30[0][0]     \n",
      "                                                                 dropout_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_37 (Lambda)              (None, 1000, 25)     0           lambda_36[0][0]                  \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 1000, 25)     9400        lambda_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 1000, 25)     100         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_34 (Dropout)            (None, 1000, 25)     0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_38 (Lambda)              (None, 1000, 25)     0           dropout_34[0][0]                 \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 1000, 25)     13150       lambda_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 1000, 25)     100         conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_39 (Lambda)              (None, 1000, 25)     0           batch_normalization_32[0][0]     \n",
      "                                                                 dropout_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_35 (Dropout)            (None, 1000, 25)     0           lambda_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_41 (Lambda)              (None, 1000, 25)     0           dropout_35[0][0]                 \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 1000, 25)     1900        lambda_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 1000, 25)     100         conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 1000, 25)     0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_42 (Lambda)              (None, 1000, 25)     0           dropout_37[0][0]                 \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 1000, 180)    223560      lambda_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 1000, 180)    720         bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 1000, 180)    0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_43 (Lambda)              (None, 1000, 180)    0           dropout_38[0][0]                 \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 1000, 180)    390960      lambda_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 1000, 180)    720         bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 1000, 180)    0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_44 (Lambda)              (None, 1000, 180)    0           dropout_39[0][0]                 \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 1000, 180)    390960      lambda_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 1000, 180)    720         bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 1000, 180)    0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_45 (Lambda)              (None, 1000, 180)    0           dropout_40[0][0]                 \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 1000, 181)    0           lambda_45[0][0]                  \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_3 (Attention)         [(None, 41, 180), (N 81549       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 41, 180)      720         attention_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_41 (Dropout)            (None, 41, 180)      0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 7380)         0           dropout_41[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_42 (Dropout)            (None, 7380)         0           flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 80)           590480      dropout_42[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 10, 8, 1)     0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 10, 8, 1)     4           reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 10, 8, 1)     0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 10, 1, 1)     0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "1ev2 (Reshape)                  (None, 10)           0           max_pooling2d_3[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 1,716,893\n",
      "Trainable params: 1,715,101\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input (InputLayer)              (None, 1000, 25)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 1000, 25)     0           Input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1000, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_31 (Lambda)              (None, 1000, 25)     0           dropout_29[0][0]                 \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 1000, 25)     650         lambda_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 1000, 25)     100         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)            (None, 1000, 25)     0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_32 (Lambda)              (None, 1000, 25)     0           dropout_30[0][0]                 \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 1000, 25)     1900        lambda_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 1000, 25)     100         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_33 (Lambda)              (None, 1000, 25)     0           batch_normalization_28[0][0]     \n",
      "                                                                 dropout_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, 1000, 25)     0           lambda_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_34 (Lambda)              (None, 1000, 25)     0           dropout_31[0][0]                 \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 1000, 25)     3150        lambda_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 1000, 25)     100         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)            (None, 1000, 25)     0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_35 (Lambda)              (None, 1000, 25)     0           dropout_32[0][0]                 \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1000, 25)     5650        lambda_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 1000, 25)     100         conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_36 (Lambda)              (None, 1000, 25)     0           batch_normalization_30[0][0]     \n",
      "                                                                 dropout_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_37 (Lambda)              (None, 1000, 25)     0           lambda_36[0][0]                  \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 1000, 25)     9400        lambda_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 1000, 25)     100         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_34 (Dropout)            (None, 1000, 25)     0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_38 (Lambda)              (None, 1000, 25)     0           dropout_34[0][0]                 \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 1000, 25)     13150       lambda_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 1000, 25)     100         conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_39 (Lambda)              (None, 1000, 25)     0           batch_normalization_32[0][0]     \n",
      "                                                                 dropout_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_35 (Dropout)            (None, 1000, 25)     0           lambda_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_41 (Lambda)              (None, 1000, 25)     0           dropout_35[0][0]                 \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 1000, 25)     1900        lambda_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 1000, 25)     100         conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 1000, 25)     0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_42 (Lambda)              (None, 1000, 25)     0           dropout_37[0][0]                 \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 1000, 180)    223560      lambda_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 1000, 180)    720         bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 1000, 180)    0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_43 (Lambda)              (None, 1000, 180)    0           dropout_38[0][0]                 \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 1000, 180)    390960      lambda_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 1000, 180)    720         bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 1000, 180)    0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_44 (Lambda)              (None, 1000, 180)    0           dropout_39[0][0]                 \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 1000, 180)    390960      lambda_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 1000, 180)    720         bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 1000, 180)    0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_45 (Lambda)              (None, 1000, 180)    0           dropout_40[0][0]                 \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 1000, 181)    0           lambda_45[0][0]                  \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_3 (Attention)         [(None, 41, 180), (N 81549       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 41, 180)      720         attention_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_41 (Dropout)            (None, 41, 180)      0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 7380)         0           dropout_41[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_42 (Dropout)            (None, 7380)         0           flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 80)           590480      dropout_42[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 10, 8, 1)     0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 10, 8, 1)     4           reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 10, 8, 1)     0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 10, 1, 1)     0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lev1 (Reshape)                  (None, 10, 8)        0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "1ev2 (Reshape)                  (None, 10)           0           max_pooling2d_3[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 1,716,893\n",
      "Trainable params: 1,715,101\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n",
      "epoch 0\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 170s 10ms/step - loss: 0.7236 - accuracy: 0.7049 - val_loss: 0.7077 - val_accuracy: 0.8092\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.80916, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.70765, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_loss-weights.hdf5\n",
      "epoch 1\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 160s 9ms/step - loss: 0.6335 - accuracy: 0.8234 - val_loss: 0.6208 - val_accuracy: 0.8559\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.80916 to 0.85593, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.70765 to 0.62081, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_loss-weights.hdf5\n",
      "epoch 2\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 158s 9ms/step - loss: 0.5884 - accuracy: 0.8542 - val_loss: 0.5737 - val_accuracy: 0.8731\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.85593 to 0.87311, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.62081 to 0.57367, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_loss-weights.hdf5\n",
      "epoch 3\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 160s 9ms/step - loss: 0.5542 - accuracy: 0.8767 - val_loss: 0.5350 - val_accuracy: 0.8874\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.87311 to 0.88742, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.57367 to 0.53498, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_loss-weights.hdf5\n",
      "epoch 4\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 177s 10ms/step - loss: 0.5266 - accuracy: 0.8888 - val_loss: 0.5177 - val_accuracy: 0.8781\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.88742\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.53498 to 0.51771, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_loss-weights.hdf5\n",
      "epoch 5\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 156s 9ms/step - loss: 0.5026 - accuracy: 0.8956 - val_loss: 0.4905 - val_accuracy: 0.8969\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.88742 to 0.89688, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.51771 to 0.49055, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_loss-weights.hdf5\n",
      "epoch 6\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 156s 9ms/step - loss: 0.4806 - accuracy: 0.9021 - val_loss: 0.4774 - val_accuracy: 0.8976\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.89688 to 0.89759, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.49055 to 0.47739, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_loss-weights.hdf5\n",
      "epoch 7\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 158s 9ms/step - loss: 0.4603 - accuracy: 0.9070 - val_loss: 0.4640 - val_accuracy: 0.9064\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.89759 to 0.90642, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.47739 to 0.46398, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_loss-weights.hdf5\n",
      "epoch 8\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 159s 9ms/step - loss: 0.4415 - accuracy: 0.9111 - val_loss: 0.4352 - val_accuracy: 0.9133\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.90642 to 0.91325, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.46398 to 0.43519, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_loss-weights.hdf5\n",
      "epoch 9\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 162s 9ms/step - loss: 0.4226 - accuracy: 0.9153 - val_loss: 0.4199 - val_accuracy: 0.9115\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.91325\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.43519 to 0.41987, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_loss-weights.hdf5\n",
      "epoch 10\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 162s 9ms/step - loss: 0.4053 - accuracy: 0.9187 - val_loss: 0.4081 - val_accuracy: 0.9157\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.91325 to 0.91566, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.41987 to 0.40808, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_loss-weights.hdf5\n",
      "epoch 11\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 155s 9ms/step - loss: 0.3890 - accuracy: 0.9219 - val_loss: 0.3888 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.91566 to 0.92064, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.40808 to 0.38882, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_loss-weights.hdf5\n",
      "epoch 12\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 157s 9ms/step - loss: 0.3722 - accuracy: 0.9250 - val_loss: 0.3747 - val_accuracy: 0.9213\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92064 to 0.92132, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.38882 to 0.37470, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_loss-weights.hdf5\n",
      "epoch 13\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 156s 9ms/step - loss: 0.3578 - accuracy: 0.9271 - val_loss: 0.3682 - val_accuracy: 0.9194\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92132\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.37470 to 0.36818, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_loss-weights.hdf5\n",
      "epoch 14\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 164s 10ms/step - loss: 0.3437 - accuracy: 0.9294 - val_loss: 0.3597 - val_accuracy: 0.9204\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92132\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.36818 to 0.35972, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_loss-weights.hdf5\n",
      "epoch 15\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 156s 9ms/step - loss: 0.3310 - accuracy: 0.9314 - val_loss: 0.3466 - val_accuracy: 0.9238\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92132 to 0.92385, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.35972 to 0.34662, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_loss-weights.hdf5\n",
      "epoch 16\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 161s 9ms/step - loss: 0.3184 - accuracy: 0.9334 - val_loss: 0.3346 - val_accuracy: 0.9229\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92385\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.34662 to 0.33463, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_loss-weights.hdf5\n",
      "epoch 17\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 154s 9ms/step - loss: 0.3066 - accuracy: 0.9356 - val_loss: 0.3331 - val_accuracy: 0.9186\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92385\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.33463 to 0.33314, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_loss-weights.hdf5\n",
      "epoch 18\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 157s 9ms/step - loss: 0.2946 - accuracy: 0.9376 - val_loss: 0.3213 - val_accuracy: 0.9229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92385\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.33314 to 0.32126, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_loss-weights.hdf5\n",
      "epoch 19\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 161s 9ms/step - loss: 0.2848 - accuracy: 0.9391 - val_loss: 0.3159 - val_accuracy: 0.9252\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92385 to 0.92524, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.32126 to 0.31586, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_loss-weights.hdf5\n",
      "epoch 20\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 160s 9ms/step - loss: 0.2746 - accuracy: 0.9409 - val_loss: 0.3006 - val_accuracy: 0.9261\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92524 to 0.92609, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.31586 to 0.30060, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_loss-weights.hdf5\n",
      "epoch 21\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 162s 9ms/step - loss: 0.2651 - accuracy: 0.9423 - val_loss: 0.3018 - val_accuracy: 0.9235\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92609\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.30060\n",
      "epoch 22\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 155s 9ms/step - loss: 0.2572 - accuracy: 0.9433 - val_loss: 0.2963 - val_accuracy: 0.9227\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92609\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.30060 to 0.29627, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_loss-weights.hdf5\n",
      "epoch 23\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 154s 9ms/step - loss: 0.2478 - accuracy: 0.9455 - val_loss: 0.2850 - val_accuracy: 0.9260\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92609\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.29627 to 0.28497, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_loss-weights.hdf5\n",
      "epoch 24\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 158s 9ms/step - loss: 0.2404 - accuracy: 0.9466 - val_loss: 0.2893 - val_accuracy: 0.9247\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92609\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.28497\n",
      "epoch 25\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 157s 9ms/step - loss: 0.2322 - accuracy: 0.9481 - val_loss: 0.2850 - val_accuracy: 0.9262\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92609 to 0.92617, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.28497 to 0.28496, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_loss-weights.hdf5\n",
      "epoch 26\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 154s 9ms/step - loss: 0.2237 - accuracy: 0.9498 - val_loss: 0.2780 - val_accuracy: 0.9276\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92617 to 0.92765, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.28496 to 0.27802, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_loss-weights.hdf5\n",
      "epoch 27\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 156s 9ms/step - loss: 0.2186 - accuracy: 0.9501 - val_loss: 0.2698 - val_accuracy: 0.9271\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92765\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.27802 to 0.26979, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_loss-weights.hdf5\n",
      "epoch 28\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 153s 9ms/step - loss: 0.2131 - accuracy: 0.9512 - val_loss: 0.2678 - val_accuracy: 0.9262\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92765\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.26979 to 0.26781, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_loss-weights.hdf5\n",
      "epoch 29\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 154s 9ms/step - loss: 0.2063 - accuracy: 0.9522 - val_loss: 0.2791 - val_accuracy: 0.9259\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92765\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26781\n",
      "epoch 30\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 153s 9ms/step - loss: 0.1997 - accuracy: 0.9543 - val_loss: 0.2664 - val_accuracy: 0.9281\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92765 to 0.92811, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.26781 to 0.26640, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_loss-weights.hdf5\n",
      "epoch 31\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 157s 9ms/step - loss: 0.1934 - accuracy: 0.9551 - val_loss: 0.2598 - val_accuracy: 0.9283\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92811 to 0.92832, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.26640 to 0.25979, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_loss-weights.hdf5\n",
      "epoch 32\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 156s 9ms/step - loss: 0.1881 - accuracy: 0.9560 - val_loss: 0.2605 - val_accuracy: 0.9243\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92832\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.25979\n",
      "epoch 33\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 158s 9ms/step - loss: 0.1851 - accuracy: 0.9560 - val_loss: 0.2558 - val_accuracy: 0.9285\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92832 to 0.92845, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.25979 to 0.25579, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_loss-weights.hdf5\n",
      "epoch 34\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 153s 9ms/step - loss: 0.1782 - accuracy: 0.9577 - val_loss: 0.2643 - val_accuracy: 0.9269\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92845\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.25579\n",
      "epoch 35\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 154s 9ms/step - loss: 0.1738 - accuracy: 0.9580 - val_loss: 0.2523 - val_accuracy: 0.9278\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92845\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.25579 to 0.25233, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_loss-weights.hdf5\n",
      "epoch 36\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 154s 9ms/step - loss: 0.1687 - accuracy: 0.9594 - val_loss: 0.2522 - val_accuracy: 0.9250\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92845\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.25233 to 0.25225, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_loss-weights.hdf5\n",
      "epoch 37\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 164s 10ms/step - loss: 0.1646 - accuracy: 0.9600 - val_loss: 0.2555 - val_accuracy: 0.9273\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92845\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.25225\n",
      "epoch 38\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 155s 9ms/step - loss: 0.1586 - accuracy: 0.9622 - val_loss: 0.2470 - val_accuracy: 0.9295\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92845 to 0.92951, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.25225 to 0.24702, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_loss-weights.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 39\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 163s 9ms/step - loss: 0.1552 - accuracy: 0.9624 - val_loss: 0.2493 - val_accuracy: 0.9271\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92951\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24702\n",
      "epoch 40\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 159s 9ms/step - loss: 0.1501 - accuracy: 0.9638 - val_loss: 0.2520 - val_accuracy: 0.9296\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92951 to 0.92959, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24702\n",
      "epoch 41\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 155s 9ms/step - loss: 0.1478 - accuracy: 0.9639 - val_loss: 0.2530 - val_accuracy: 0.9265\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92959\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24702\n",
      "epoch 42\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 156s 9ms/step - loss: 0.1440 - accuracy: 0.9646 - val_loss: 0.2525 - val_accuracy: 0.9278\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92959\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24702\n",
      "epoch 43\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 156s 9ms/step - loss: 0.1429 - accuracy: 0.9640 - val_loss: 0.2502 - val_accuracy: 0.9288\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92959\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24702\n",
      "epoch 44\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 156s 9ms/step - loss: 0.1368 - accuracy: 0.9663 - val_loss: 0.2456 - val_accuracy: 0.9287\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92959\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.24702 to 0.24563, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_loss-weights.hdf5\n",
      "epoch 45\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 155s 9ms/step - loss: 0.1338 - accuracy: 0.9668 - val_loss: 0.2487 - val_accuracy: 0.9241\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92959\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24563\n",
      "epoch 46\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 162s 9ms/step - loss: 0.1313 - accuracy: 0.9669 - val_loss: 0.2547 - val_accuracy: 0.9273\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92959\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24563\n",
      "epoch 47\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 155s 9ms/step - loss: 0.1278 - accuracy: 0.9681 - val_loss: 0.2490 - val_accuracy: 0.9280\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92959\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24563\n",
      "epoch 48\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 156s 9ms/step - loss: 0.1241 - accuracy: 0.9691 - val_loss: 0.2489 - val_accuracy: 0.9303\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92959 to 0.93027, saving model to D:/mulocdeep/lv1_result44/fold2_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24563\n",
      "epoch 49\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 155s 9ms/step - loss: 0.1221 - accuracy: 0.9694 - val_loss: 0.2498 - val_accuracy: 0.9266\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93027\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24563\n",
      "epoch 50\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 155s 9ms/step - loss: 0.1187 - accuracy: 0.9697 - val_loss: 0.2526 - val_accuracy: 0.9284\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93027\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24563\n",
      "epoch 51\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 153s 9ms/step - loss: 0.1167 - accuracy: 0.9702 - val_loss: 0.2588 - val_accuracy: 0.9250\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93027\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24563\n",
      "epoch 52\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 162s 9ms/step - loss: 0.1142 - accuracy: 0.9710 - val_loss: 0.2507 - val_accuracy: 0.9240\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93027\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24563\n",
      "epoch 53\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 155s 9ms/step - loss: 0.1117 - accuracy: 0.9714 - val_loss: 0.2579 - val_accuracy: 0.9258\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93027\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24563\n",
      "epoch 54\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 155s 9ms/step - loss: 0.1095 - accuracy: 0.9720 - val_loss: 0.2459 - val_accuracy: 0.9284\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93027\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24563\n",
      "epoch 55\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 156s 9ms/step - loss: 0.1064 - accuracy: 0.9729 - val_loss: 0.2533 - val_accuracy: 0.9284\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93027\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24563\n",
      "epoch 56\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 157s 9ms/step - loss: 0.1037 - accuracy: 0.9733 - val_loss: 0.2479 - val_accuracy: 0.9301\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93027\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24563\n",
      "epoch 57\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 157s 9ms/step - loss: 0.1016 - accuracy: 0.9739 - val_loss: 0.2490 - val_accuracy: 0.9294\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93027\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24563\n",
      "epoch 58\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 163s 9ms/step - loss: 0.0994 - accuracy: 0.9744 - val_loss: 0.2557 - val_accuracy: 0.9283\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93027\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24563\n",
      "epoch 59\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 156s 9ms/step - loss: 0.0984 - accuracy: 0.9745 - val_loss: 0.2536 - val_accuracy: 0.9280\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93027\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24563\n",
      "epoch 60\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 156s 9ms/step - loss: 0.0947 - accuracy: 0.9759 - val_loss: 0.2610 - val_accuracy: 0.9283\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93027\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24563\n",
      "epoch 61\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 158s 9ms/step - loss: 0.0942 - accuracy: 0.9755 - val_loss: 0.2612 - val_accuracy: 0.9275\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93027\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24563\n",
      "epoch 62\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 155s 9ms/step - loss: 0.0920 - accuracy: 0.9761 - val_loss: 0.2591 - val_accuracy: 0.9285\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93027\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24563\n",
      "epoch 63\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 154s 9ms/step - loss: 0.0898 - accuracy: 0.9767 - val_loss: 0.2712 - val_accuracy: 0.9263\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93027\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24563\n",
      "epoch 64\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17214/17214 [==============================] - 163s 9ms/step - loss: 0.0878 - accuracy: 0.9774 - val_loss: 0.2626 - val_accuracy: 0.9255\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93027\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24563\n",
      "epoch 65\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 153s 9ms/step - loss: 0.0856 - accuracy: 0.9784 - val_loss: 0.2648 - val_accuracy: 0.9245\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93027\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24563\n",
      "epoch 66\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 156s 9ms/step - loss: 0.0850 - accuracy: 0.9781 - val_loss: 0.2763 - val_accuracy: 0.9247\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93027\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24563\n",
      "epoch 67\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 156s 9ms/step - loss: 0.0830 - accuracy: 0.9785 - val_loss: 0.2621 - val_accuracy: 0.9272\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93027\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24563\n",
      "epoch 68\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 156s 9ms/step - loss: 0.0812 - accuracy: 0.9791 - val_loss: 0.2749 - val_accuracy: 0.9271\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93027\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24563\n",
      "epoch 69\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 157s 9ms/step - loss: 0.0802 - accuracy: 0.9793 - val_loss: 0.2757 - val_accuracy: 0.9256\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93027\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24563\n",
      "epoch 70\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 163s 9ms/step - loss: 0.0781 - accuracy: 0.9800 - val_loss: 0.2835 - val_accuracy: 0.9268\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93027\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24563\n",
      "epoch 71\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 153s 9ms/step - loss: 0.0771 - accuracy: 0.9802 - val_loss: 0.2692 - val_accuracy: 0.9269\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93027\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24563\n",
      "epoch 72\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 154s 9ms/step - loss: 0.0755 - accuracy: 0.9801 - val_loss: 0.2804 - val_accuracy: 0.9263\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93027\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24563\n",
      "epoch 73\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 155s 9ms/step - loss: 0.0755 - accuracy: 0.9800 - val_loss: 0.2676 - val_accuracy: 0.9282\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93027\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24563\n",
      "epoch 74\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 156s 9ms/step - loss: 0.0718 - accuracy: 0.9815 - val_loss: 0.2639 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93027\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24563\n",
      "epoch 75\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 156s 9ms/step - loss: 0.0710 - accuracy: 0.9817 - val_loss: 0.2861 - val_accuracy: 0.9268\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93027\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24563\n",
      "epoch 76\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 163s 9ms/step - loss: 0.0686 - accuracy: 0.9824 - val_loss: 0.2664 - val_accuracy: 0.9285\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93027\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24563\n",
      "epoch 77\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 156s 9ms/step - loss: 0.0674 - accuracy: 0.9829 - val_loss: 0.2761 - val_accuracy: 0.9263\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93027\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24563\n",
      "epoch 78\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 158s 9ms/step - loss: 0.0663 - accuracy: 0.9831 - val_loss: 0.2856 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93027\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24563\n",
      "epoch 79\n",
      "\n",
      "Train on 17214 samples, validate on 2369 samples\n",
      "Epoch 1/1\n",
      "17214/17214 [==============================] - 158s 9ms/step - loss: 0.0655 - accuracy: 0.9831 - val_loss: 0.2771 - val_accuracy: 0.9254\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93027\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24563\n",
      "doing 3th fold\n",
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input (InputLayer)              (None, 1000, 25)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_43 (Dropout)            (None, 1000, 25)     0           Input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 1000, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_46 (Lambda)              (None, 1000, 25)     0           dropout_43[0][0]                 \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 1000, 25)     650         lambda_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 1000, 25)     100         conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_44 (Dropout)            (None, 1000, 25)     0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_47 (Lambda)              (None, 1000, 25)     0           dropout_44[0][0]                 \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 1000, 25)     1900        lambda_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 1000, 25)     100         conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_48 (Lambda)              (None, 1000, 25)     0           batch_normalization_41[0][0]     \n",
      "                                                                 dropout_43[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_45 (Dropout)            (None, 1000, 25)     0           lambda_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_49 (Lambda)              (None, 1000, 25)     0           dropout_45[0][0]                 \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 1000, 25)     3150        lambda_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 1000, 25)     100         conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_46 (Dropout)            (None, 1000, 25)     0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_50 (Lambda)              (None, 1000, 25)     0           dropout_46[0][0]                 \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 1000, 25)     5650        lambda_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 1000, 25)     100         conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_51 (Lambda)              (None, 1000, 25)     0           batch_normalization_43[0][0]     \n",
      "                                                                 dropout_43[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_52 (Lambda)              (None, 1000, 25)     0           lambda_51[0][0]                  \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 1000, 25)     9400        lambda_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 1000, 25)     100         conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_48 (Dropout)            (None, 1000, 25)     0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_53 (Lambda)              (None, 1000, 25)     0           dropout_48[0][0]                 \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 1000, 25)     13150       lambda_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 1000, 25)     100         conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_54 (Lambda)              (None, 1000, 25)     0           batch_normalization_45[0][0]     \n",
      "                                                                 dropout_43[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_49 (Dropout)            (None, 1000, 25)     0           lambda_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_56 (Lambda)              (None, 1000, 25)     0           dropout_49[0][0]                 \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 1000, 25)     1900        lambda_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 1000, 25)     100         conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_51 (Dropout)            (None, 1000, 25)     0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_57 (Lambda)              (None, 1000, 25)     0           dropout_51[0][0]                 \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_10 (Bidirectional (None, 1000, 180)    223560      lambda_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 1000, 180)    720         bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_52 (Dropout)            (None, 1000, 180)    0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_58 (Lambda)              (None, 1000, 180)    0           dropout_52[0][0]                 \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_11 (Bidirectional (None, 1000, 180)    390960      lambda_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 1000, 180)    720         bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_53 (Dropout)            (None, 1000, 180)    0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_59 (Lambda)              (None, 1000, 180)    0           dropout_53[0][0]                 \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_12 (Bidirectional (None, 1000, 180)    390960      lambda_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 1000, 180)    720         bidirectional_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_54 (Dropout)            (None, 1000, 180)    0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_60 (Lambda)              (None, 1000, 180)    0           dropout_54[0][0]                 \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 1000, 181)    0           lambda_60[0][0]                  \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_4 (Attention)         [(None, 41, 180), (N 81549       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 41, 180)      720         attention_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_55 (Dropout)            (None, 41, 180)      0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 7380)         0           dropout_55[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_56 (Dropout)            (None, 7380)         0           flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 80)           590480      dropout_56[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 10, 8, 1)     0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 10, 8, 1)     4           reshape_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 10, 8, 1)     0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 10, 1, 1)     0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "1ev2 (Reshape)                  (None, 10)           0           max_pooling2d_4[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 1,716,893\n",
      "Trainable params: 1,715,101\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input (InputLayer)              (None, 1000, 25)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_43 (Dropout)            (None, 1000, 25)     0           Input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 1000, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_46 (Lambda)              (None, 1000, 25)     0           dropout_43[0][0]                 \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 1000, 25)     650         lambda_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 1000, 25)     100         conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_44 (Dropout)            (None, 1000, 25)     0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_47 (Lambda)              (None, 1000, 25)     0           dropout_44[0][0]                 \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 1000, 25)     1900        lambda_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 1000, 25)     100         conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_48 (Lambda)              (None, 1000, 25)     0           batch_normalization_41[0][0]     \n",
      "                                                                 dropout_43[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_45 (Dropout)            (None, 1000, 25)     0           lambda_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_49 (Lambda)              (None, 1000, 25)     0           dropout_45[0][0]                 \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 1000, 25)     3150        lambda_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 1000, 25)     100         conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_46 (Dropout)            (None, 1000, 25)     0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_50 (Lambda)              (None, 1000, 25)     0           dropout_46[0][0]                 \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 1000, 25)     5650        lambda_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 1000, 25)     100         conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_51 (Lambda)              (None, 1000, 25)     0           batch_normalization_43[0][0]     \n",
      "                                                                 dropout_43[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_52 (Lambda)              (None, 1000, 25)     0           lambda_51[0][0]                  \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 1000, 25)     9400        lambda_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 1000, 25)     100         conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_48 (Dropout)            (None, 1000, 25)     0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_53 (Lambda)              (None, 1000, 25)     0           dropout_48[0][0]                 \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 1000, 25)     13150       lambda_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 1000, 25)     100         conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_54 (Lambda)              (None, 1000, 25)     0           batch_normalization_45[0][0]     \n",
      "                                                                 dropout_43[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_49 (Dropout)            (None, 1000, 25)     0           lambda_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_56 (Lambda)              (None, 1000, 25)     0           dropout_49[0][0]                 \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 1000, 25)     1900        lambda_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 1000, 25)     100         conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_51 (Dropout)            (None, 1000, 25)     0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_57 (Lambda)              (None, 1000, 25)     0           dropout_51[0][0]                 \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_10 (Bidirectional (None, 1000, 180)    223560      lambda_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 1000, 180)    720         bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_52 (Dropout)            (None, 1000, 180)    0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_58 (Lambda)              (None, 1000, 180)    0           dropout_52[0][0]                 \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_11 (Bidirectional (None, 1000, 180)    390960      lambda_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 1000, 180)    720         bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_53 (Dropout)            (None, 1000, 180)    0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_59 (Lambda)              (None, 1000, 180)    0           dropout_53[0][0]                 \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_12 (Bidirectional (None, 1000, 180)    390960      lambda_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 1000, 180)    720         bidirectional_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_54 (Dropout)            (None, 1000, 180)    0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_60 (Lambda)              (None, 1000, 180)    0           dropout_54[0][0]                 \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 1000, 181)    0           lambda_60[0][0]                  \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_4 (Attention)         [(None, 41, 180), (N 81549       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 41, 180)      720         attention_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_55 (Dropout)            (None, 41, 180)      0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 7380)         0           dropout_55[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_56 (Dropout)            (None, 7380)         0           flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 80)           590480      dropout_56[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 10, 8, 1)     0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 10, 8, 1)     4           reshape_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 10, 8, 1)     0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 10, 1, 1)     0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lev1 (Reshape)                  (None, 10, 8)        0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "1ev2 (Reshape)                  (None, 10)           0           max_pooling2d_4[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 1,716,893\n",
      "Trainable params: 1,715,101\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n",
      "epoch 0\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 181s 11ms/step - loss: 0.7219 - accuracy: 0.7031 - val_loss: 0.7816 - val_accuracy: 0.8165\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.81647, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.78156, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_loss-weights.hdf5\n",
      "epoch 1\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 158s 9ms/step - loss: 0.6299 - accuracy: 0.8213 - val_loss: 0.6012 - val_accuracy: 0.8443\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.81647 to 0.84426, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.78156 to 0.60119, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_loss-weights.hdf5\n",
      "epoch 2\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 150s 9ms/step - loss: 0.5843 - accuracy: 0.8585 - val_loss: 0.5596 - val_accuracy: 0.8776\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.84426 to 0.87765, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.60119 to 0.55961, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_loss-weights.hdf5\n",
      "epoch 3\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 149s 9ms/step - loss: 0.5533 - accuracy: 0.8780 - val_loss: 0.5284 - val_accuracy: 0.8852\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.87765 to 0.88523, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.55961 to 0.52840, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_loss-weights.hdf5\n",
      "epoch 4\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 152s 9ms/step - loss: 0.5267 - accuracy: 0.8898 - val_loss: 0.5006 - val_accuracy: 0.8967\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.88523 to 0.89671, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.52840 to 0.50057, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_loss-weights.hdf5\n",
      "epoch 5\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 149s 9ms/step - loss: 0.5028 - accuracy: 0.8960 - val_loss: 0.4870 - val_accuracy: 0.8970\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.89671 to 0.89704, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.50057 to 0.48703, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_loss-weights.hdf5\n",
      "epoch 6\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 150s 9ms/step - loss: 0.4820 - accuracy: 0.9012 - val_loss: 0.4645 - val_accuracy: 0.9044\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.89704 to 0.90442, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.48703 to 0.46447, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_loss-weights.hdf5\n",
      "epoch 7\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 155s 9ms/step - loss: 0.4612 - accuracy: 0.9066 - val_loss: 0.4568 - val_accuracy: 0.9101\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.90442 to 0.91006, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.46447 to 0.45677, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_loss-weights.hdf5\n",
      "epoch 8\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 151s 9ms/step - loss: 0.4427 - accuracy: 0.9107 - val_loss: 0.4330 - val_accuracy: 0.9117\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.91006 to 0.91172, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.45677 to 0.43300, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_loss-weights.hdf5\n",
      "epoch 9\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 149s 9ms/step - loss: 0.4244 - accuracy: 0.9140 - val_loss: 0.4190 - val_accuracy: 0.9155\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.91172 to 0.91546, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.43300 to 0.41897, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_loss-weights.hdf5\n",
      "epoch 10\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 149s 9ms/step - loss: 0.4071 - accuracy: 0.9172 - val_loss: 0.4009 - val_accuracy: 0.9152\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.91546\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.41897 to 0.40094, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_loss-weights.hdf5\n",
      "epoch 11\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 150s 9ms/step - loss: 0.3911 - accuracy: 0.9204 - val_loss: 0.3899 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.91546 to 0.91671, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.40094 to 0.38990, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_loss-weights.hdf5\n",
      "epoch 12\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 150s 9ms/step - loss: 0.3757 - accuracy: 0.9229 - val_loss: 0.3775 - val_accuracy: 0.9177\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.91671 to 0.91769, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.38990 to 0.37754, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_loss-weights.hdf5\n",
      "epoch 13\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 156s 9ms/step - loss: 0.3608 - accuracy: 0.9258 - val_loss: 0.3590 - val_accuracy: 0.9215\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.91769 to 0.92146, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.37754 to 0.35899, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_loss-weights.hdf5\n",
      "epoch 14\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 153s 9ms/step - loss: 0.3464 - accuracy: 0.9289 - val_loss: 0.3568 - val_accuracy: 0.9211\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92146\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.35899 to 0.35681, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_loss-weights.hdf5\n",
      "epoch 15\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 156s 9ms/step - loss: 0.3337 - accuracy: 0.9302 - val_loss: 0.3466 - val_accuracy: 0.9227\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92146 to 0.92272, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.35681 to 0.34657, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_loss-weights.hdf5\n",
      "epoch 16\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 152s 9ms/step - loss: 0.3215 - accuracy: 0.9329 - val_loss: 0.3466 - val_accuracy: 0.9190\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92272\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.34657 to 0.34657, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_loss-weights.hdf5\n",
      "epoch 17\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 151s 9ms/step - loss: 0.3104 - accuracy: 0.9340 - val_loss: 0.3233 - val_accuracy: 0.9256\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92272 to 0.92560, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.34657 to 0.32328, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_loss-weights.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 151s 9ms/step - loss: 0.2988 - accuracy: 0.9365 - val_loss: 0.3146 - val_accuracy: 0.9267\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92560 to 0.92669, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.32328 to 0.31456, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_loss-weights.hdf5\n",
      "epoch 19\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 156s 9ms/step - loss: 0.2882 - accuracy: 0.9379 - val_loss: 0.3045 - val_accuracy: 0.9291\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92669 to 0.92913, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.31456 to 0.30448, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_loss-weights.hdf5\n",
      "epoch 20\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 152s 9ms/step - loss: 0.2778 - accuracy: 0.9399 - val_loss: 0.2998 - val_accuracy: 0.9267\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92913\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.30448 to 0.29979, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_loss-weights.hdf5\n",
      "epoch 21\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 152s 9ms/step - loss: 0.2688 - accuracy: 0.9411 - val_loss: 0.2916 - val_accuracy: 0.9300\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92913 to 0.92998, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.29979 to 0.29163, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_loss-weights.hdf5\n",
      "epoch 22\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 152s 9ms/step - loss: 0.2603 - accuracy: 0.9425 - val_loss: 0.2904 - val_accuracy: 0.9284\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92998\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.29163 to 0.29040, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_loss-weights.hdf5\n",
      "epoch 23\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 151s 9ms/step - loss: 0.2506 - accuracy: 0.9444 - val_loss: 0.2813 - val_accuracy: 0.9290\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92998\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.29040 to 0.28131, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_loss-weights.hdf5\n",
      "epoch 24\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 151s 9ms/step - loss: 0.2439 - accuracy: 0.9449 - val_loss: 0.2804 - val_accuracy: 0.9276\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92998\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.28131 to 0.28045, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_loss-weights.hdf5\n",
      "epoch 25\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 153s 9ms/step - loss: 0.2355 - accuracy: 0.9465 - val_loss: 0.2733 - val_accuracy: 0.9277\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92998\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.28045 to 0.27331, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_loss-weights.hdf5\n",
      "epoch 26\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 160s 9ms/step - loss: 0.2285 - accuracy: 0.9479 - val_loss: 0.2664 - val_accuracy: 0.9328\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92998 to 0.93282, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.27331 to 0.26643, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_loss-weights.hdf5\n",
      "epoch 27\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 154s 9ms/step - loss: 0.2204 - accuracy: 0.9499 - val_loss: 0.2682 - val_accuracy: 0.9265\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93282\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26643\n",
      "epoch 28\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 153s 9ms/step - loss: 0.2150 - accuracy: 0.9501 - val_loss: 0.2593 - val_accuracy: 0.9301\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93282\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.26643 to 0.25925, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_loss-weights.hdf5\n",
      "epoch 29\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 152s 9ms/step - loss: 0.2070 - accuracy: 0.9521 - val_loss: 0.2527 - val_accuracy: 0.9297\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93282\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.25925 to 0.25271, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_loss-weights.hdf5\n",
      "epoch 30\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 154s 9ms/step - loss: 0.2021 - accuracy: 0.9521 - val_loss: 0.2533 - val_accuracy: 0.9294\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93282\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.25271\n",
      "epoch 31\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 154s 9ms/step - loss: 0.1961 - accuracy: 0.9540 - val_loss: 0.2508 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93282\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.25271 to 0.25076, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_loss-weights.hdf5\n",
      "epoch 32\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 161s 9ms/step - loss: 0.1897 - accuracy: 0.9556 - val_loss: 0.2517 - val_accuracy: 0.9307\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93282\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.25076\n",
      "epoch 33\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 152s 9ms/step - loss: 0.1857 - accuracy: 0.9560 - val_loss: 0.2449 - val_accuracy: 0.9297\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93282\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.25076 to 0.24495, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_loss-weights.hdf5\n",
      "epoch 34\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 153s 9ms/step - loss: 0.1805 - accuracy: 0.9564 - val_loss: 0.2553 - val_accuracy: 0.9305\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93282\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24495\n",
      "epoch 35\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 152s 9ms/step - loss: 0.1749 - accuracy: 0.9577 - val_loss: 0.2650 - val_accuracy: 0.9263\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93282\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24495\n",
      "epoch 36\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 154s 9ms/step - loss: 0.1698 - accuracy: 0.9593 - val_loss: 0.2392 - val_accuracy: 0.9310\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93282\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.24495 to 0.23920, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_loss-weights.hdf5\n",
      "epoch 37\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 150s 9ms/step - loss: 0.1655 - accuracy: 0.9594 - val_loss: 0.2431 - val_accuracy: 0.9317\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93282\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23920\n",
      "epoch 38\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 162s 9ms/step - loss: 0.1619 - accuracy: 0.9605 - val_loss: 0.2348 - val_accuracy: 0.9316\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93282\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.23920 to 0.23485, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_loss-weights.hdf5\n",
      "epoch 39\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17118/17118 [==============================] - 153s 9ms/step - loss: 0.1563 - accuracy: 0.9621 - val_loss: 0.2529 - val_accuracy: 0.9282\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93282\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23485\n",
      "epoch 40\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 150s 9ms/step - loss: 0.1531 - accuracy: 0.9627 - val_loss: 0.2531 - val_accuracy: 0.9295\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93282\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23485\n",
      "epoch 41\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 150s 9ms/step - loss: 0.1521 - accuracy: 0.9623 - val_loss: 0.2354 - val_accuracy: 0.9300\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93282\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23485\n",
      "epoch 42\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 151s 9ms/step - loss: 0.1467 - accuracy: 0.9635 - val_loss: 0.2337 - val_accuracy: 0.9318\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93282\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.23485 to 0.23365, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_loss-weights.hdf5\n",
      "epoch 43\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 151s 9ms/step - loss: 0.1413 - accuracy: 0.9649 - val_loss: 0.2569 - val_accuracy: 0.9267\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93282\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23365\n",
      "epoch 44\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 162s 9ms/step - loss: 0.1397 - accuracy: 0.9649 - val_loss: 0.2331 - val_accuracy: 0.9327\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93282\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.23365 to 0.23314, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_loss-weights.hdf5\n",
      "epoch 45\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 152s 9ms/step - loss: 0.1348 - accuracy: 0.9664 - val_loss: 0.2357 - val_accuracy: 0.9327\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93282\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23314\n",
      "epoch 46\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 150s 9ms/step - loss: 0.1316 - accuracy: 0.9668 - val_loss: 0.2366 - val_accuracy: 0.9308\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93282\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23314\n",
      "epoch 47\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 150s 9ms/step - loss: 0.1298 - accuracy: 0.9676 - val_loss: 0.2372 - val_accuracy: 0.9334\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.93282 to 0.93339, saving model to D:/mulocdeep/lv1_result44/fold3_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23314\n",
      "epoch 48\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 151s 9ms/step - loss: 0.1253 - accuracy: 0.9686 - val_loss: 0.2525 - val_accuracy: 0.9289\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93339\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23314\n",
      "epoch 49\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 151s 9ms/step - loss: 0.1241 - accuracy: 0.9682 - val_loss: 0.2486 - val_accuracy: 0.9288\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93339\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23314\n",
      "epoch 50\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 162s 9ms/step - loss: 0.1208 - accuracy: 0.9696 - val_loss: 0.2406 - val_accuracy: 0.9304\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93339\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23314\n",
      "epoch 51\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 154s 9ms/step - loss: 0.1189 - accuracy: 0.9698 - val_loss: 0.2414 - val_accuracy: 0.9310\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93339\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23314\n",
      "epoch 52\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 150s 9ms/step - loss: 0.1134 - accuracy: 0.9711 - val_loss: 0.2381 - val_accuracy: 0.9314\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93339\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23314\n",
      "epoch 53\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 151s 9ms/step - loss: 0.1125 - accuracy: 0.9710 - val_loss: 0.2450 - val_accuracy: 0.9317\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93339\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23314\n",
      "epoch 54\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 151s 9ms/step - loss: 0.1099 - accuracy: 0.9721 - val_loss: 0.2445 - val_accuracy: 0.9303\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93339\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23314\n",
      "epoch 55\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 151s 9ms/step - loss: 0.1085 - accuracy: 0.9722 - val_loss: 0.2423 - val_accuracy: 0.9320\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93339\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23314\n",
      "epoch 56\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 160s 9ms/step - loss: 0.1049 - accuracy: 0.9727 - val_loss: 0.2404 - val_accuracy: 0.9318\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93339\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23314\n",
      "epoch 57\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 149s 9ms/step - loss: 0.1032 - accuracy: 0.9735 - val_loss: 0.2532 - val_accuracy: 0.9308\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93339\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23314\n",
      "epoch 58\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 152s 9ms/step - loss: 0.1017 - accuracy: 0.9736 - val_loss: 0.2434 - val_accuracy: 0.9314\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93339\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23314\n",
      "epoch 59\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 151s 9ms/step - loss: 0.0984 - accuracy: 0.9745 - val_loss: 0.2505 - val_accuracy: 0.9308\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93339\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23314\n",
      "epoch 60\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 151s 9ms/step - loss: 0.0972 - accuracy: 0.9748 - val_loss: 0.2403 - val_accuracy: 0.9302\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93339\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23314\n",
      "epoch 61\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 152s 9ms/step - loss: 0.0942 - accuracy: 0.9754 - val_loss: 0.2516 - val_accuracy: 0.9305\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93339\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23314\n",
      "epoch 62\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 158s 9ms/step - loss: 0.0921 - accuracy: 0.9761 - val_loss: 0.2546 - val_accuracy: 0.9302\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93339\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23314\n",
      "epoch 63\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 154s 9ms/step - loss: 0.0900 - accuracy: 0.9770 - val_loss: 0.2473 - val_accuracy: 0.9290\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93339\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23314\n",
      "epoch 64\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 151s 9ms/step - loss: 0.0884 - accuracy: 0.9773 - val_loss: 0.2543 - val_accuracy: 0.9293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93339\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23314\n",
      "epoch 65\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 151s 9ms/step - loss: 0.0871 - accuracy: 0.9774 - val_loss: 0.2740 - val_accuracy: 0.9272\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93339\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23314\n",
      "epoch 66\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 151s 9ms/step - loss: 0.0854 - accuracy: 0.9780 - val_loss: 0.2584 - val_accuracy: 0.9258\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93339\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23314\n",
      "epoch 67\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 152s 9ms/step - loss: 0.0865 - accuracy: 0.9774 - val_loss: 0.2880 - val_accuracy: 0.9262\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93339\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23314\n",
      "epoch 68\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 152s 9ms/step - loss: 0.0816 - accuracy: 0.9792 - val_loss: 0.2504 - val_accuracy: 0.9302\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93339\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23314\n",
      "epoch 69\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 158s 9ms/step - loss: 0.0801 - accuracy: 0.9796 - val_loss: 0.2728 - val_accuracy: 0.9278\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93339\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23314\n",
      "epoch 70\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 151s 9ms/step - loss: 0.0789 - accuracy: 0.9797 - val_loss: 0.2693 - val_accuracy: 0.9274\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93339\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23314\n",
      "epoch 71\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 152s 9ms/step - loss: 0.0769 - accuracy: 0.9802 - val_loss: 0.2755 - val_accuracy: 0.9311\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93339\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23314\n",
      "epoch 72\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 151s 9ms/step - loss: 0.0758 - accuracy: 0.9804 - val_loss: 0.2585 - val_accuracy: 0.9312\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93339\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23314\n",
      "epoch 73\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 152s 9ms/step - loss: 0.0745 - accuracy: 0.9807 - val_loss: 0.2655 - val_accuracy: 0.9315\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93339\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23314\n",
      "epoch 74\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 153s 9ms/step - loss: 0.0737 - accuracy: 0.9811 - val_loss: 0.2682 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93339\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23314\n",
      "epoch 75\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 160s 9ms/step - loss: 0.0705 - accuracy: 0.9823 - val_loss: 0.2676 - val_accuracy: 0.9305\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93339\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23314\n",
      "epoch 76\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 152s 9ms/step - loss: 0.0693 - accuracy: 0.9825 - val_loss: 0.2869 - val_accuracy: 0.9277\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93339\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23314\n",
      "epoch 77\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 152s 9ms/step - loss: 0.0677 - accuracy: 0.9827 - val_loss: 0.2568 - val_accuracy: 0.9302\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93339\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23314\n",
      "epoch 78\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 152s 9ms/step - loss: 0.0675 - accuracy: 0.9828 - val_loss: 0.2725 - val_accuracy: 0.9291\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93339\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23314\n",
      "epoch 79\n",
      "\n",
      "Train on 17118 samples, validate on 2465 samples\n",
      "Epoch 1/1\n",
      "17118/17118 [==============================] - 154s 9ms/step - loss: 0.0659 - accuracy: 0.9830 - val_loss: 0.2679 - val_accuracy: 0.9290\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93339\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23314\n",
      "doing 4th fold\n",
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input (InputLayer)              (None, 1000, 25)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_57 (Dropout)            (None, 1000, 25)     0           Input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 1000, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_61 (Lambda)              (None, 1000, 25)     0           dropout_57[0][0]                 \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 1000, 25)     650         lambda_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 1000, 25)     100         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_58 (Dropout)            (None, 1000, 25)     0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_62 (Lambda)              (None, 1000, 25)     0           dropout_58[0][0]                 \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 1000, 25)     1900        lambda_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 1000, 25)     100         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_63 (Lambda)              (None, 1000, 25)     0           batch_normalization_54[0][0]     \n",
      "                                                                 dropout_57[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_59 (Dropout)            (None, 1000, 25)     0           lambda_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_64 (Lambda)              (None, 1000, 25)     0           dropout_59[0][0]                 \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1000, 25)     3150        lambda_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 1000, 25)     100         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_60 (Dropout)            (None, 1000, 25)     0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_65 (Lambda)              (None, 1000, 25)     0           dropout_60[0][0]                 \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 1000, 25)     5650        lambda_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 1000, 25)     100         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_66 (Lambda)              (None, 1000, 25)     0           batch_normalization_56[0][0]     \n",
      "                                                                 dropout_57[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_67 (Lambda)              (None, 1000, 25)     0           lambda_66[0][0]                  \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 1000, 25)     9400        lambda_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 1000, 25)     100         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_62 (Dropout)            (None, 1000, 25)     0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_68 (Lambda)              (None, 1000, 25)     0           dropout_62[0][0]                 \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1000, 25)     13150       lambda_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 1000, 25)     100         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_69 (Lambda)              (None, 1000, 25)     0           batch_normalization_58[0][0]     \n",
      "                                                                 dropout_57[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_63 (Dropout)            (None, 1000, 25)     0           lambda_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_71 (Lambda)              (None, 1000, 25)     0           dropout_63[0][0]                 \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 1000, 25)     1900        lambda_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 1000, 25)     100         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_65 (Dropout)            (None, 1000, 25)     0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_72 (Lambda)              (None, 1000, 25)     0           dropout_65[0][0]                 \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_13 (Bidirectional (None, 1000, 180)    223560      lambda_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 1000, 180)    720         bidirectional_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_66 (Dropout)            (None, 1000, 180)    0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_73 (Lambda)              (None, 1000, 180)    0           dropout_66[0][0]                 \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_14 (Bidirectional (None, 1000, 180)    390960      lambda_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 1000, 180)    720         bidirectional_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_67 (Dropout)            (None, 1000, 180)    0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_74 (Lambda)              (None, 1000, 180)    0           dropout_67[0][0]                 \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_15 (Bidirectional (None, 1000, 180)    390960      lambda_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 1000, 180)    720         bidirectional_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_68 (Dropout)            (None, 1000, 180)    0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_75 (Lambda)              (None, 1000, 180)    0           dropout_68[0][0]                 \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 1000, 181)    0           lambda_75[0][0]                  \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_5 (Attention)         [(None, 41, 180), (N 81549       concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 41, 180)      720         attention_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_69 (Dropout)            (None, 41, 180)      0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 7380)         0           dropout_69[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_70 (Dropout)            (None, 7380)         0           flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 80)           590480      dropout_70[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 10, 8, 1)     0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 10, 8, 1)     4           reshape_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 10, 8, 1)     0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 10, 1, 1)     0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "1ev2 (Reshape)                  (None, 10)           0           max_pooling2d_5[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 1,716,893\n",
      "Trainable params: 1,715,101\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input (InputLayer)              (None, 1000, 25)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_57 (Dropout)            (None, 1000, 25)     0           Input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 1000, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_61 (Lambda)              (None, 1000, 25)     0           dropout_57[0][0]                 \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 1000, 25)     650         lambda_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 1000, 25)     100         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_58 (Dropout)            (None, 1000, 25)     0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_62 (Lambda)              (None, 1000, 25)     0           dropout_58[0][0]                 \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 1000, 25)     1900        lambda_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 1000, 25)     100         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_63 (Lambda)              (None, 1000, 25)     0           batch_normalization_54[0][0]     \n",
      "                                                                 dropout_57[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_59 (Dropout)            (None, 1000, 25)     0           lambda_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_64 (Lambda)              (None, 1000, 25)     0           dropout_59[0][0]                 \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1000, 25)     3150        lambda_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 1000, 25)     100         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_60 (Dropout)            (None, 1000, 25)     0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_65 (Lambda)              (None, 1000, 25)     0           dropout_60[0][0]                 \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 1000, 25)     5650        lambda_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 1000, 25)     100         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_66 (Lambda)              (None, 1000, 25)     0           batch_normalization_56[0][0]     \n",
      "                                                                 dropout_57[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_67 (Lambda)              (None, 1000, 25)     0           lambda_66[0][0]                  \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 1000, 25)     9400        lambda_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 1000, 25)     100         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_62 (Dropout)            (None, 1000, 25)     0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_68 (Lambda)              (None, 1000, 25)     0           dropout_62[0][0]                 \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1000, 25)     13150       lambda_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 1000, 25)     100         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_69 (Lambda)              (None, 1000, 25)     0           batch_normalization_58[0][0]     \n",
      "                                                                 dropout_57[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_63 (Dropout)            (None, 1000, 25)     0           lambda_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_71 (Lambda)              (None, 1000, 25)     0           dropout_63[0][0]                 \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 1000, 25)     1900        lambda_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 1000, 25)     100         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_65 (Dropout)            (None, 1000, 25)     0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_72 (Lambda)              (None, 1000, 25)     0           dropout_65[0][0]                 \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_13 (Bidirectional (None, 1000, 180)    223560      lambda_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 1000, 180)    720         bidirectional_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_66 (Dropout)            (None, 1000, 180)    0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_73 (Lambda)              (None, 1000, 180)    0           dropout_66[0][0]                 \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_14 (Bidirectional (None, 1000, 180)    390960      lambda_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 1000, 180)    720         bidirectional_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_67 (Dropout)            (None, 1000, 180)    0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_74 (Lambda)              (None, 1000, 180)    0           dropout_67[0][0]                 \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_15 (Bidirectional (None, 1000, 180)    390960      lambda_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 1000, 180)    720         bidirectional_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_68 (Dropout)            (None, 1000, 180)    0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_75 (Lambda)              (None, 1000, 180)    0           dropout_68[0][0]                 \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 1000, 181)    0           lambda_75[0][0]                  \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_5 (Attention)         [(None, 41, 180), (N 81549       concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 41, 180)      720         attention_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_69 (Dropout)            (None, 41, 180)      0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 7380)         0           dropout_69[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_70 (Dropout)            (None, 7380)         0           flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 80)           590480      dropout_70[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 10, 8, 1)     0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 10, 8, 1)     4           reshape_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation_5 (Activation)       (None, 10, 8, 1)     0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 10, 1, 1)     0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lev1 (Reshape)                  (None, 10, 8)        0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "1ev2 (Reshape)                  (None, 10)           0           max_pooling2d_5[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 1,716,893\n",
      "Trainable params: 1,715,101\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n",
      "epoch 0\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 164s 10ms/step - loss: 0.7272 - accuracy: 0.7041 - val_loss: 0.6642 - val_accuracy: 0.7963\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.79625, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.66422, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_loss-weights.hdf5\n",
      "epoch 1\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 150s 9ms/step - loss: 0.6323 - accuracy: 0.8215 - val_loss: 0.6176 - val_accuracy: 0.8468\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.79625 to 0.84680, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.66422 to 0.61757, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_loss-weights.hdf5\n",
      "epoch 2\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 148s 9ms/step - loss: 0.5875 - accuracy: 0.8567 - val_loss: 0.5589 - val_accuracy: 0.8722\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.84680 to 0.87223, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.61757 to 0.55889, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_loss-weights.hdf5\n",
      "epoch 3\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 147s 9ms/step - loss: 0.5537 - accuracy: 0.8781 - val_loss: 0.5320 - val_accuracy: 0.8872\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.87223 to 0.88722, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.55889 to 0.53197, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_loss-weights.hdf5\n",
      "epoch 4\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 149s 9ms/step - loss: 0.5267 - accuracy: 0.8891 - val_loss: 0.5102 - val_accuracy: 0.8868\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.88722\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.53197 to 0.51023, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_loss-weights.hdf5\n",
      "epoch 5\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 153s 9ms/step - loss: 0.5035 - accuracy: 0.8959 - val_loss: 0.4920 - val_accuracy: 0.8927\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.88722 to 0.89266, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.51023 to 0.49199, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_loss-weights.hdf5\n",
      "epoch 6\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 148s 9ms/step - loss: 0.4810 - accuracy: 0.9031 - val_loss: 0.4710 - val_accuracy: 0.8994\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.89266 to 0.89944, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.49199 to 0.47099, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_loss-weights.hdf5\n",
      "epoch 7\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 149s 9ms/step - loss: 0.4612 - accuracy: 0.9064 - val_loss: 0.4638 - val_accuracy: 0.9024\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.89944 to 0.90238, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.47099 to 0.46377, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_loss-weights.hdf5\n",
      "epoch 8\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 150s 9ms/step - loss: 0.4413 - accuracy: 0.9117 - val_loss: 0.4410 - val_accuracy: 0.9042\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.90238 to 0.90423, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.46377 to 0.44100, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_loss-weights.hdf5\n",
      "epoch 9\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 149s 9ms/step - loss: 0.4233 - accuracy: 0.9148 - val_loss: 0.4178 - val_accuracy: 0.9122\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.90423 to 0.91221, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.44100 to 0.41783, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_loss-weights.hdf5\n",
      "epoch 10\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 150s 9ms/step - loss: 0.4057 - accuracy: 0.9175 - val_loss: 0.4064 - val_accuracy: 0.9142\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.91221 to 0.91419, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.41783 to 0.40643, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_loss-weights.hdf5\n",
      "epoch 11\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 151s 9ms/step - loss: 0.3878 - accuracy: 0.9216 - val_loss: 0.3919 - val_accuracy: 0.9141\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.91419\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.40643 to 0.39194, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_loss-weights.hdf5\n",
      "epoch 12\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 157s 9ms/step - loss: 0.3733 - accuracy: 0.9237 - val_loss: 0.3744 - val_accuracy: 0.9188\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.91419 to 0.91878, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.39194 to 0.37436, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_loss-weights.hdf5\n",
      "epoch 13\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 155s 9ms/step - loss: 0.3580 - accuracy: 0.9267 - val_loss: 0.3692 - val_accuracy: 0.9195\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.91878 to 0.91955, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.37436 to 0.36917, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_loss-weights.hdf5\n",
      "epoch 14\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 157s 9ms/step - loss: 0.3440 - accuracy: 0.9290 - val_loss: 0.3503 - val_accuracy: 0.9189\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.91955\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.36917 to 0.35031, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_loss-weights.hdf5\n",
      "epoch 15\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 155s 9ms/step - loss: 0.3311 - accuracy: 0.9313 - val_loss: 0.3396 - val_accuracy: 0.9227\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.91955 to 0.92273, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.35031 to 0.33958, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_loss-weights.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 148s 9ms/step - loss: 0.3178 - accuracy: 0.9340 - val_loss: 0.3394 - val_accuracy: 0.9224\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92273\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.33958 to 0.33945, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_loss-weights.hdf5\n",
      "epoch 17\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 152s 9ms/step - loss: 0.3071 - accuracy: 0.9351 - val_loss: 0.3194 - val_accuracy: 0.9240\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92273 to 0.92398, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.33945 to 0.31936, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_loss-weights.hdf5\n",
      "epoch 18\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 155s 9ms/step - loss: 0.2952 - accuracy: 0.9372 - val_loss: 0.3187 - val_accuracy: 0.9231\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92398\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.31936 to 0.31866, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_loss-weights.hdf5\n",
      "epoch 19\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 148s 9ms/step - loss: 0.2845 - accuracy: 0.9389 - val_loss: 0.3102 - val_accuracy: 0.9262\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92398 to 0.92620, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.31866 to 0.31016, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_loss-weights.hdf5\n",
      "epoch 20\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 147s 9ms/step - loss: 0.2743 - accuracy: 0.9411 - val_loss: 0.3042 - val_accuracy: 0.9239\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92620\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.31016 to 0.30422, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_loss-weights.hdf5\n",
      "epoch 21\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 147s 9ms/step - loss: 0.2665 - accuracy: 0.9418 - val_loss: 0.2932 - val_accuracy: 0.9251\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92620\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.30422 to 0.29318, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_loss-weights.hdf5\n",
      "epoch 22\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 149s 9ms/step - loss: 0.2558 - accuracy: 0.9439 - val_loss: 0.2931 - val_accuracy: 0.9281\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92620 to 0.92813, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.29318 to 0.29311, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_loss-weights.hdf5\n",
      "epoch 23\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 147s 9ms/step - loss: 0.2476 - accuracy: 0.9455 - val_loss: 0.2860 - val_accuracy: 0.9279\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92813\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.29311 to 0.28604, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_loss-weights.hdf5\n",
      "epoch 24\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 160s 9ms/step - loss: 0.2403 - accuracy: 0.9464 - val_loss: 0.2784 - val_accuracy: 0.9273\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92813\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.28604 to 0.27836, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_loss-weights.hdf5\n",
      "epoch 25\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 152s 9ms/step - loss: 0.2313 - accuracy: 0.9483 - val_loss: 0.2778 - val_accuracy: 0.9264\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92813\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.27836 to 0.27777, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_loss-weights.hdf5\n",
      "epoch 26\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 150s 9ms/step - loss: 0.2257 - accuracy: 0.9487 - val_loss: 0.2668 - val_accuracy: 0.9264\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92813\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.27777 to 0.26683, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_loss-weights.hdf5\n",
      "epoch 27\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 149s 9ms/step - loss: 0.2176 - accuracy: 0.9503 - val_loss: 0.2696 - val_accuracy: 0.9270\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92813\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26683\n",
      "epoch 28\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 149s 9ms/step - loss: 0.2117 - accuracy: 0.9511 - val_loss: 0.2625 - val_accuracy: 0.9283\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92813 to 0.92825, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.26683 to 0.26247, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_loss-weights.hdf5\n",
      "epoch 29\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 151s 9ms/step - loss: 0.2060 - accuracy: 0.9524 - val_loss: 0.2618 - val_accuracy: 0.9285\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92825 to 0.92850, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.26247 to 0.26180, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_loss-weights.hdf5\n",
      "epoch 30\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 157s 9ms/step - loss: 0.1982 - accuracy: 0.9540 - val_loss: 0.2647 - val_accuracy: 0.9262\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92850\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26180\n",
      "epoch 31\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 149s 9ms/step - loss: 0.1934 - accuracy: 0.9547 - val_loss: 0.2540 - val_accuracy: 0.9283\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92850\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.26180 to 0.25400, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_loss-weights.hdf5\n",
      "epoch 32\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 150s 9ms/step - loss: 0.1866 - accuracy: 0.9561 - val_loss: 0.2511 - val_accuracy: 0.9322\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92850 to 0.93216, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.25400 to 0.25114, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_loss-weights.hdf5\n",
      "epoch 33\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 148s 9ms/step - loss: 0.1808 - accuracy: 0.9577 - val_loss: 0.2519 - val_accuracy: 0.9269\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.25114\n",
      "epoch 34\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 148s 9ms/step - loss: 0.1762 - accuracy: 0.9584 - val_loss: 0.2564 - val_accuracy: 0.9253\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.25114\n",
      "epoch 35\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 147s 9ms/step - loss: 0.1724 - accuracy: 0.9590 - val_loss: 0.2538 - val_accuracy: 0.9285\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.25114\n",
      "epoch 36\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 156s 9ms/step - loss: 0.1668 - accuracy: 0.9598 - val_loss: 0.2550 - val_accuracy: 0.9264\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.25114\n",
      "epoch 37\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17102/17102 [==============================] - 150s 9ms/step - loss: 0.1626 - accuracy: 0.9606 - val_loss: 0.2431 - val_accuracy: 0.9304\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.25114 to 0.24310, saving model to D:/mulocdeep/lv1_result44/fold4_big_lv1_loss-weights.hdf5\n",
      "epoch 38\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 148s 9ms/step - loss: 0.1590 - accuracy: 0.9614 - val_loss: 0.2460 - val_accuracy: 0.9291\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24310\n",
      "epoch 39\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 146s 9ms/step - loss: 0.1546 - accuracy: 0.9621 - val_loss: 0.2440 - val_accuracy: 0.9291\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24310\n",
      "epoch 40\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 148s 9ms/step - loss: 0.1502 - accuracy: 0.9635 - val_loss: 0.2796 - val_accuracy: 0.9229\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24310\n",
      "epoch 41\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 147s 9ms/step - loss: 0.1469 - accuracy: 0.9639 - val_loss: 0.2499 - val_accuracy: 0.9274\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24310\n",
      "epoch 42\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 156s 9ms/step - loss: 0.1420 - accuracy: 0.9653 - val_loss: 0.2540 - val_accuracy: 0.9247\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24310\n",
      "epoch 43\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 149s 9ms/step - loss: 0.1405 - accuracy: 0.9650 - val_loss: 0.2514 - val_accuracy: 0.9249\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24310\n",
      "epoch 44\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 148s 9ms/step - loss: 0.1385 - accuracy: 0.9647 - val_loss: 0.2446 - val_accuracy: 0.9267\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24310\n",
      "epoch 45\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 147s 9ms/step - loss: 0.1380 - accuracy: 0.9643 - val_loss: 0.2464 - val_accuracy: 0.9285\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24310\n",
      "epoch 46\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 148s 9ms/step - loss: 0.1311 - accuracy: 0.9673 - val_loss: 0.2676 - val_accuracy: 0.9250\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24310\n",
      "epoch 47\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 148s 9ms/step - loss: 0.1268 - accuracy: 0.9683 - val_loss: 0.2494 - val_accuracy: 0.9256\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24310\n",
      "epoch 48\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 148s 9ms/step - loss: 0.1231 - accuracy: 0.9694 - val_loss: 0.2468 - val_accuracy: 0.9260\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24310\n",
      "epoch 49\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 157s 9ms/step - loss: 0.1220 - accuracy: 0.9688 - val_loss: 0.2573 - val_accuracy: 0.9254\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24310\n",
      "epoch 50\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 149s 9ms/step - loss: 0.1196 - accuracy: 0.9697 - val_loss: 0.2530 - val_accuracy: 0.9244\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24310\n",
      "epoch 51\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 148s 9ms/step - loss: 0.1164 - accuracy: 0.9705 - val_loss: 0.2502 - val_accuracy: 0.9257\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24310\n",
      "epoch 52\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 147s 9ms/step - loss: 0.1127 - accuracy: 0.9712 - val_loss: 0.2515 - val_accuracy: 0.9266\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24310\n",
      "epoch 53\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 149s 9ms/step - loss: 0.1107 - accuracy: 0.9716 - val_loss: 0.2590 - val_accuracy: 0.9262\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24310\n",
      "epoch 54\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 149s 9ms/step - loss: 0.1085 - accuracy: 0.9720 - val_loss: 0.2670 - val_accuracy: 0.9267\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24310\n",
      "epoch 55\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 159s 9ms/step - loss: 0.1048 - accuracy: 0.9735 - val_loss: 0.2591 - val_accuracy: 0.9262\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24310\n",
      "epoch 56\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 150s 9ms/step - loss: 0.1026 - accuracy: 0.9737 - val_loss: 0.2525 - val_accuracy: 0.9250\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24310\n",
      "epoch 57\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 149s 9ms/step - loss: 0.1015 - accuracy: 0.9739 - val_loss: 0.2560 - val_accuracy: 0.9281\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24310\n",
      "epoch 58\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 148s 9ms/step - loss: 0.0996 - accuracy: 0.9741 - val_loss: 0.2751 - val_accuracy: 0.9204\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24310\n",
      "epoch 59\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 148s 9ms/step - loss: 0.0970 - accuracy: 0.9746 - val_loss: 0.2586 - val_accuracy: 0.9246\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24310\n",
      "epoch 60\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 149s 9ms/step - loss: 0.0963 - accuracy: 0.9748 - val_loss: 0.2654 - val_accuracy: 0.9262\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24310\n",
      "epoch 61\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 158s 9ms/step - loss: 0.0920 - accuracy: 0.9767 - val_loss: 0.2550 - val_accuracy: 0.9245\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24310\n",
      "epoch 62\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 150s 9ms/step - loss: 0.0904 - accuracy: 0.9765 - val_loss: 0.2669 - val_accuracy: 0.9246\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24310\n",
      "epoch 63\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17102/17102 [==============================] - 151s 9ms/step - loss: 0.0876 - accuracy: 0.9778 - val_loss: 0.2733 - val_accuracy: 0.9252\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24310\n",
      "epoch 64\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 151s 9ms/step - loss: 0.0865 - accuracy: 0.9776 - val_loss: 0.2667 - val_accuracy: 0.9248\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24310\n",
      "epoch 65\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 148s 9ms/step - loss: 0.0850 - accuracy: 0.9778 - val_loss: 0.2594 - val_accuracy: 0.9260\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24310\n",
      "epoch 66\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 151s 9ms/step - loss: 0.0831 - accuracy: 0.9784 - val_loss: 0.2759 - val_accuracy: 0.9248\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24310\n",
      "epoch 67\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 154s 9ms/step - loss: 0.0815 - accuracy: 0.9791 - val_loss: 0.2585 - val_accuracy: 0.9244\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24310\n",
      "epoch 68\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 152s 9ms/step - loss: 0.0796 - accuracy: 0.9794 - val_loss: 0.2738 - val_accuracy: 0.9258\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24310\n",
      "epoch 69\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 150s 9ms/step - loss: 0.0793 - accuracy: 0.9794 - val_loss: 0.2742 - val_accuracy: 0.9268\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24310\n",
      "epoch 70\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 151s 9ms/step - loss: 0.0767 - accuracy: 0.9801 - val_loss: 0.2776 - val_accuracy: 0.9258\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24310\n",
      "epoch 71\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 152s 9ms/step - loss: 0.0754 - accuracy: 0.9802 - val_loss: 0.2952 - val_accuracy: 0.9213\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24310\n",
      "epoch 72\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 151s 9ms/step - loss: 0.0729 - accuracy: 0.9815 - val_loss: 0.2776 - val_accuracy: 0.9255\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24310\n",
      "epoch 73\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 152s 9ms/step - loss: 0.1003 - accuracy: 0.9716 - val_loss: 0.2743 - val_accuracy: 0.9164\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24310\n",
      "epoch 74\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 158s 9ms/step - loss: 0.1220 - accuracy: 0.9606 - val_loss: 0.2594 - val_accuracy: 0.9248\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24310\n",
      "epoch 75\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 150s 9ms/step - loss: 0.1028 - accuracy: 0.9689 - val_loss: 0.2557 - val_accuracy: 0.9255\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24310\n",
      "epoch 76\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 152s 9ms/step - loss: 0.0936 - accuracy: 0.9717 - val_loss: 0.2509 - val_accuracy: 0.9278\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24310\n",
      "epoch 77\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 148s 9ms/step - loss: 0.0875 - accuracy: 0.9744 - val_loss: 0.2491 - val_accuracy: 0.9292\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24310\n",
      "epoch 78\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 149s 9ms/step - loss: 0.0844 - accuracy: 0.9757 - val_loss: 0.2536 - val_accuracy: 0.9270\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24310\n",
      "epoch 79\n",
      "\n",
      "Train on 17102 samples, validate on 2481 samples\n",
      "Epoch 1/1\n",
      "17102/17102 [==============================] - 147s 9ms/step - loss: 0.0804 - accuracy: 0.9768 - val_loss: 0.2708 - val_accuracy: 0.9238\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93216\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24310\n",
      "doing 5th fold\n",
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input (InputLayer)              (None, 1000, 25)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_71 (Dropout)            (None, 1000, 25)     0           Input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 1000, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_76 (Lambda)              (None, 1000, 25)     0           dropout_71[0][0]                 \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 1000, 25)     650         lambda_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 1000, 25)     100         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_72 (Dropout)            (None, 1000, 25)     0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_77 (Lambda)              (None, 1000, 25)     0           dropout_72[0][0]                 \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1000, 25)     1900        lambda_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 1000, 25)     100         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_78 (Lambda)              (None, 1000, 25)     0           batch_normalization_67[0][0]     \n",
      "                                                                 dropout_71[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_73 (Dropout)            (None, 1000, 25)     0           lambda_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_79 (Lambda)              (None, 1000, 25)     0           dropout_73[0][0]                 \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 1000, 25)     3150        lambda_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 1000, 25)     100         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_74 (Dropout)            (None, 1000, 25)     0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_80 (Lambda)              (None, 1000, 25)     0           dropout_74[0][0]                 \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 1000, 25)     5650        lambda_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 1000, 25)     100         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_81 (Lambda)              (None, 1000, 25)     0           batch_normalization_69[0][0]     \n",
      "                                                                 dropout_71[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_82 (Lambda)              (None, 1000, 25)     0           lambda_81[0][0]                  \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 1000, 25)     9400        lambda_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 1000, 25)     100         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_76 (Dropout)            (None, 1000, 25)     0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_83 (Lambda)              (None, 1000, 25)     0           dropout_76[0][0]                 \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 1000, 25)     13150       lambda_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 1000, 25)     100         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_84 (Lambda)              (None, 1000, 25)     0           batch_normalization_71[0][0]     \n",
      "                                                                 dropout_71[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_77 (Dropout)            (None, 1000, 25)     0           lambda_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_86 (Lambda)              (None, 1000, 25)     0           dropout_77[0][0]                 \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 1000, 25)     1900        lambda_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 1000, 25)     100         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_79 (Dropout)            (None, 1000, 25)     0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_87 (Lambda)              (None, 1000, 25)     0           dropout_79[0][0]                 \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_16 (Bidirectional (None, 1000, 180)    223560      lambda_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 1000, 180)    720         bidirectional_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_80 (Dropout)            (None, 1000, 180)    0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_88 (Lambda)              (None, 1000, 180)    0           dropout_80[0][0]                 \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_17 (Bidirectional (None, 1000, 180)    390960      lambda_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 1000, 180)    720         bidirectional_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_81 (Dropout)            (None, 1000, 180)    0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_89 (Lambda)              (None, 1000, 180)    0           dropout_81[0][0]                 \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_18 (Bidirectional (None, 1000, 180)    390960      lambda_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 1000, 180)    720         bidirectional_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_82 (Dropout)            (None, 1000, 180)    0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_90 (Lambda)              (None, 1000, 180)    0           dropout_82[0][0]                 \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 1000, 181)    0           lambda_90[0][0]                  \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_6 (Attention)         [(None, 41, 180), (N 81549       concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 41, 180)      720         attention_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_83 (Dropout)            (None, 41, 180)      0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 7380)         0           dropout_83[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_84 (Dropout)            (None, 7380)         0           flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 80)           590480      dropout_84[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, 10, 8, 1)     0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 10, 8, 1)     4           reshape_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 10, 8, 1)     0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 10, 1, 1)     0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "1ev2 (Reshape)                  (None, 10)           0           max_pooling2d_6[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 1,716,893\n",
      "Trainable params: 1,715,101\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input (InputLayer)              (None, 1000, 25)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_71 (Dropout)            (None, 1000, 25)     0           Input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 1000, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_76 (Lambda)              (None, 1000, 25)     0           dropout_71[0][0]                 \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 1000, 25)     650         lambda_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 1000, 25)     100         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_72 (Dropout)            (None, 1000, 25)     0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_77 (Lambda)              (None, 1000, 25)     0           dropout_72[0][0]                 \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1000, 25)     1900        lambda_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 1000, 25)     100         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_78 (Lambda)              (None, 1000, 25)     0           batch_normalization_67[0][0]     \n",
      "                                                                 dropout_71[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_73 (Dropout)            (None, 1000, 25)     0           lambda_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_79 (Lambda)              (None, 1000, 25)     0           dropout_73[0][0]                 \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 1000, 25)     3150        lambda_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 1000, 25)     100         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_74 (Dropout)            (None, 1000, 25)     0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_80 (Lambda)              (None, 1000, 25)     0           dropout_74[0][0]                 \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 1000, 25)     5650        lambda_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 1000, 25)     100         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_81 (Lambda)              (None, 1000, 25)     0           batch_normalization_69[0][0]     \n",
      "                                                                 dropout_71[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_82 (Lambda)              (None, 1000, 25)     0           lambda_81[0][0]                  \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 1000, 25)     9400        lambda_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 1000, 25)     100         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_76 (Dropout)            (None, 1000, 25)     0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_83 (Lambda)              (None, 1000, 25)     0           dropout_76[0][0]                 \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 1000, 25)     13150       lambda_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 1000, 25)     100         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_84 (Lambda)              (None, 1000, 25)     0           batch_normalization_71[0][0]     \n",
      "                                                                 dropout_71[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_77 (Dropout)            (None, 1000, 25)     0           lambda_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_86 (Lambda)              (None, 1000, 25)     0           dropout_77[0][0]                 \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 1000, 25)     1900        lambda_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 1000, 25)     100         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_79 (Dropout)            (None, 1000, 25)     0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_87 (Lambda)              (None, 1000, 25)     0           dropout_79[0][0]                 \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_16 (Bidirectional (None, 1000, 180)    223560      lambda_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 1000, 180)    720         bidirectional_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_80 (Dropout)            (None, 1000, 180)    0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_88 (Lambda)              (None, 1000, 180)    0           dropout_80[0][0]                 \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_17 (Bidirectional (None, 1000, 180)    390960      lambda_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 1000, 180)    720         bidirectional_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_81 (Dropout)            (None, 1000, 180)    0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_89 (Lambda)              (None, 1000, 180)    0           dropout_81[0][0]                 \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_18 (Bidirectional (None, 1000, 180)    390960      lambda_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 1000, 180)    720         bidirectional_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_82 (Dropout)            (None, 1000, 180)    0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_90 (Lambda)              (None, 1000, 180)    0           dropout_82[0][0]                 \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 1000, 181)    0           lambda_90[0][0]                  \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_6 (Attention)         [(None, 41, 180), (N 81549       concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 41, 180)      720         attention_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_83 (Dropout)            (None, 41, 180)      0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 7380)         0           dropout_83[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_84 (Dropout)            (None, 7380)         0           flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 80)           590480      dropout_84[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, 10, 8, 1)     0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 10, 8, 1)     4           reshape_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 10, 8, 1)     0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 10, 1, 1)     0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lev1 (Reshape)                  (None, 10, 8)        0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "1ev2 (Reshape)                  (None, 10)           0           max_pooling2d_6[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 1,716,893\n",
      "Trainable params: 1,715,101\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n",
      "epoch 0\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 167s 10ms/step - loss: 0.7309 - accuracy: 0.6757 - val_loss: 0.6882 - val_accuracy: 0.8044\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.80440, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.68821, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_loss-weights.hdf5\n",
      "epoch 1\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 153s 9ms/step - loss: 0.6308 - accuracy: 0.8367 - val_loss: 0.6131 - val_accuracy: 0.8462\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.80440 to 0.84620, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.68821 to 0.61311, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_loss-weights.hdf5\n",
      "epoch 2\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 151s 9ms/step - loss: 0.5848 - accuracy: 0.8606 - val_loss: 0.5586 - val_accuracy: 0.8690\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.84620 to 0.86904, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.61311 to 0.55862, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_loss-weights.hdf5\n",
      "epoch 3\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 151s 9ms/step - loss: 0.5525 - accuracy: 0.8805 - val_loss: 0.5371 - val_accuracy: 0.8839\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.86904 to 0.88386, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.55862 to 0.53710, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_loss-weights.hdf5\n",
      "epoch 4\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 163s 9ms/step - loss: 0.5262 - accuracy: 0.8909 - val_loss: 0.5093 - val_accuracy: 0.8906\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.88386 to 0.89061, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.53710 to 0.50931, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_loss-weights.hdf5\n",
      "epoch 5\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 149s 9ms/step - loss: 0.5032 - accuracy: 0.8971 - val_loss: 0.4953 - val_accuracy: 0.8927\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.89061 to 0.89266, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.50931 to 0.49530, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_loss-weights.hdf5\n",
      "epoch 6\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 147s 9ms/step - loss: 0.4811 - accuracy: 0.9035 - val_loss: 0.4747 - val_accuracy: 0.9005\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.89266 to 0.90047, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.49530 to 0.47466, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_loss-weights.hdf5\n",
      "epoch 7\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 147s 9ms/step - loss: 0.4611 - accuracy: 0.9071 - val_loss: 0.4584 - val_accuracy: 0.9035\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.90047 to 0.90350, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.47466 to 0.45840, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_loss-weights.hdf5\n",
      "epoch 8\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 148s 9ms/step - loss: 0.4409 - accuracy: 0.9122 - val_loss: 0.4429 - val_accuracy: 0.9043\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.90350 to 0.90431, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.45840 to 0.44289, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_loss-weights.hdf5\n",
      "epoch 9\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 147s 9ms/step - loss: 0.4229 - accuracy: 0.9153 - val_loss: 0.4208 - val_accuracy: 0.9084\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.90431 to 0.90837, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.44289 to 0.42077, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_loss-weights.hdf5\n",
      "epoch 10\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 153s 9ms/step - loss: 0.4050 - accuracy: 0.9193 - val_loss: 0.4082 - val_accuracy: 0.9096\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.90837 to 0.90956, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.42077 to 0.40816, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_loss-weights.hdf5\n",
      "epoch 11\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 150s 9ms/step - loss: 0.3897 - accuracy: 0.9214 - val_loss: 0.3907 - val_accuracy: 0.9149\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.90956 to 0.91486, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.40816 to 0.39073, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_loss-weights.hdf5\n",
      "epoch 12\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 150s 9ms/step - loss: 0.3744 - accuracy: 0.9237 - val_loss: 0.3880 - val_accuracy: 0.9146\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.91486\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.39073 to 0.38799, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_loss-weights.hdf5\n",
      "epoch 13\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 150s 9ms/step - loss: 0.3581 - accuracy: 0.9275 - val_loss: 0.3681 - val_accuracy: 0.9175\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.91486 to 0.91746, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.38799 to 0.36812, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_loss-weights.hdf5\n",
      "epoch 14\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 152s 9ms/step - loss: 0.3448 - accuracy: 0.9296 - val_loss: 0.3532 - val_accuracy: 0.9229\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.91746 to 0.92289, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.36812 to 0.35322, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_loss-weights.hdf5\n",
      "epoch 15\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 154s 9ms/step - loss: 0.3316 - accuracy: 0.9316 - val_loss: 0.3494 - val_accuracy: 0.9201\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92289\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.35322 to 0.34942, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_loss-weights.hdf5\n",
      "epoch 16\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 159s 9ms/step - loss: 0.3189 - accuracy: 0.9337 - val_loss: 0.3394 - val_accuracy: 0.9193\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92289\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.34942 to 0.33936, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_loss-weights.hdf5\n",
      "epoch 17\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 149s 9ms/step - loss: 0.3059 - accuracy: 0.9363 - val_loss: 0.3357 - val_accuracy: 0.9156\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92289\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.33936 to 0.33570, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_loss-weights.hdf5\n",
      "epoch 18\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17241/17241 [==============================] - 148s 9ms/step - loss: 0.2949 - accuracy: 0.9376 - val_loss: 0.3143 - val_accuracy: 0.9211\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92289\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.33570 to 0.31432, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_loss-weights.hdf5\n",
      "epoch 19\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 147s 9ms/step - loss: 0.2852 - accuracy: 0.9390 - val_loss: 0.3103 - val_accuracy: 0.9228\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92289\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.31432 to 0.31034, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_loss-weights.hdf5\n",
      "epoch 20\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 149s 9ms/step - loss: 0.2741 - accuracy: 0.9411 - val_loss: 0.3079 - val_accuracy: 0.9205\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92289\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.31034 to 0.30786, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_loss-weights.hdf5\n",
      "epoch 21\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 147s 9ms/step - loss: 0.2648 - accuracy: 0.9428 - val_loss: 0.2995 - val_accuracy: 0.9235\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92289 to 0.92353, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.30786 to 0.29949, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_loss-weights.hdf5\n",
      "epoch 22\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 145s 8ms/step - loss: 0.2558 - accuracy: 0.9438 - val_loss: 0.2939 - val_accuracy: 0.9222\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92353\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.29949 to 0.29385, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_loss-weights.hdf5\n",
      "epoch 23\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 155s 9ms/step - loss: 0.2480 - accuracy: 0.9449 - val_loss: 0.2958 - val_accuracy: 0.9226\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92353\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.29385\n",
      "epoch 24\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 149s 9ms/step - loss: 0.2395 - accuracy: 0.9464 - val_loss: 0.2859 - val_accuracy: 0.9243\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92353 to 0.92430, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.29385 to 0.28591, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_loss-weights.hdf5\n",
      "epoch 25\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 149s 9ms/step - loss: 0.2305 - accuracy: 0.9479 - val_loss: 0.2789 - val_accuracy: 0.9233\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92430\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.28591 to 0.27886, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_loss-weights.hdf5\n",
      "epoch 26\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 147s 9ms/step - loss: 0.2243 - accuracy: 0.9490 - val_loss: 0.2754 - val_accuracy: 0.9239\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92430\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.27886 to 0.27537, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_loss-weights.hdf5\n",
      "epoch 27\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 148s 9ms/step - loss: 0.2165 - accuracy: 0.9511 - val_loss: 0.2757 - val_accuracy: 0.9243\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92430 to 0.92434, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.27537\n",
      "epoch 28\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 148s 9ms/step - loss: 0.2107 - accuracy: 0.9517 - val_loss: 0.2770 - val_accuracy: 0.9231\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92434\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.27537\n",
      "epoch 29\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 157s 9ms/step - loss: 0.2030 - accuracy: 0.9534 - val_loss: 0.2681 - val_accuracy: 0.9244\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92434 to 0.92442, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.27537 to 0.26813, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_loss-weights.hdf5\n",
      "epoch 30\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 149s 9ms/step - loss: 0.1970 - accuracy: 0.9545 - val_loss: 0.2660 - val_accuracy: 0.9240\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92442\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.26813 to 0.26600, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_loss-weights.hdf5\n",
      "epoch 31\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 146s 8ms/step - loss: 0.1912 - accuracy: 0.9555 - val_loss: 0.2622 - val_accuracy: 0.9254\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92442 to 0.92541, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.26600 to 0.26217, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_loss-weights.hdf5\n",
      "epoch 32\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 147s 9ms/step - loss: 0.1849 - accuracy: 0.9566 - val_loss: 0.2567 - val_accuracy: 0.9261\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92541 to 0.92613, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.26217 to 0.25673, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_loss-weights.hdf5\n",
      "epoch 33\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 149s 9ms/step - loss: 0.1797 - accuracy: 0.9578 - val_loss: 0.2654 - val_accuracy: 0.9243\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92613\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.25673\n",
      "epoch 34\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 148s 9ms/step - loss: 0.1763 - accuracy: 0.9579 - val_loss: 0.2634 - val_accuracy: 0.9223\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92613\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.25673\n",
      "epoch 35\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 155s 9ms/step - loss: 0.2349 - accuracy: 0.9297 - val_loss: 0.2547 - val_accuracy: 0.9193\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92613\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.25673 to 0.25466, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_loss-weights.hdf5\n",
      "epoch 36\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 148s 9ms/step - loss: 0.2058 - accuracy: 0.9418 - val_loss: 0.2442 - val_accuracy: 0.9239\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92613\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.25466 to 0.24424, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_loss-weights.hdf5\n",
      "epoch 37\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 150s 9ms/step - loss: 0.1921 - accuracy: 0.9468 - val_loss: 0.2460 - val_accuracy: 0.9241\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92613\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24424\n",
      "epoch 38\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 150s 9ms/step - loss: 0.1830 - accuracy: 0.9501 - val_loss: 0.2440 - val_accuracy: 0.9231\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92613\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.24424 to 0.24402, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_loss-weights.hdf5\n",
      "epoch 39\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17241/17241 [==============================] - 146s 8ms/step - loss: 0.1757 - accuracy: 0.9521 - val_loss: 0.2442 - val_accuracy: 0.9246\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92613\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24402\n",
      "epoch 40\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 149s 9ms/step - loss: 0.1706 - accuracy: 0.9539 - val_loss: 0.2438 - val_accuracy: 0.9266\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92613 to 0.92656, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.24402 to 0.24380, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_loss-weights.hdf5\n",
      "epoch 41\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 147s 9ms/step - loss: 0.1643 - accuracy: 0.9558 - val_loss: 0.2475 - val_accuracy: 0.9246\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92656\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24380\n",
      "epoch 42\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 159s 9ms/step - loss: 0.1602 - accuracy: 0.9564 - val_loss: 0.2365 - val_accuracy: 0.9243\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92656\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.24380 to 0.23647, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_loss-weights.hdf5\n",
      "epoch 43\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 147s 9ms/step - loss: 0.1553 - accuracy: 0.9585 - val_loss: 0.2450 - val_accuracy: 0.9259\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92656\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23647\n",
      "epoch 44\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 146s 8ms/step - loss: 0.1505 - accuracy: 0.9596 - val_loss: 0.2437 - val_accuracy: 0.9253\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92656\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23647\n",
      "epoch 45\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 149s 9ms/step - loss: 0.1472 - accuracy: 0.9602 - val_loss: 0.2377 - val_accuracy: 0.9240\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92656\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23647\n",
      "epoch 46\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 148s 9ms/step - loss: 0.1436 - accuracy: 0.9617 - val_loss: 0.2354 - val_accuracy: 0.9262\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92656\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.23647 to 0.23541, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_loss-weights.hdf5\n",
      "epoch 47\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 151s 9ms/step - loss: 0.1408 - accuracy: 0.9620 - val_loss: 0.2443 - val_accuracy: 0.9257\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92656\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23541\n",
      "epoch 48\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 158s 9ms/step - loss: 0.1372 - accuracy: 0.9628 - val_loss: 0.2366 - val_accuracy: 0.9253\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92656\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23541\n",
      "epoch 49\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 151s 9ms/step - loss: 0.1339 - accuracy: 0.9638 - val_loss: 0.2381 - val_accuracy: 0.9263\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92656\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23541\n",
      "epoch 50\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 152s 9ms/step - loss: 0.1314 - accuracy: 0.9641 - val_loss: 0.2394 - val_accuracy: 0.9250\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92656\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23541\n",
      "epoch 51\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 149s 9ms/step - loss: 0.1284 - accuracy: 0.9655 - val_loss: 0.2398 - val_accuracy: 0.9280\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92656 to 0.92797, saving model to D:/mulocdeep/lv1_result44/fold5_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23541\n",
      "epoch 52\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 149s 9ms/step - loss: 0.1253 - accuracy: 0.9658 - val_loss: 0.2378 - val_accuracy: 0.9272\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92797\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23541\n",
      "epoch 53\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 149s 9ms/step - loss: 0.1222 - accuracy: 0.9666 - val_loss: 0.2412 - val_accuracy: 0.9269\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92797\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23541\n",
      "epoch 54\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 160s 9ms/step - loss: 0.1206 - accuracy: 0.9671 - val_loss: 0.2488 - val_accuracy: 0.9270\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92797\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23541\n",
      "epoch 55\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 150s 9ms/step - loss: 0.1166 - accuracy: 0.9683 - val_loss: 0.2445 - val_accuracy: 0.9258\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92797\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23541\n",
      "epoch 56\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 150s 9ms/step - loss: 0.1151 - accuracy: 0.9687 - val_loss: 0.2511 - val_accuracy: 0.9246\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92797\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23541\n",
      "epoch 57\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 148s 9ms/step - loss: 0.1116 - accuracy: 0.9695 - val_loss: 0.2550 - val_accuracy: 0.9228\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92797\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23541\n",
      "epoch 58\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 149s 9ms/step - loss: 0.1101 - accuracy: 0.9698 - val_loss: 0.2457 - val_accuracy: 0.9257\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92797\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23541\n",
      "epoch 59\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 149s 9ms/step - loss: 0.1075 - accuracy: 0.9706 - val_loss: 0.2494 - val_accuracy: 0.9257\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92797\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23541\n",
      "epoch 60\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 156s 9ms/step - loss: 0.1049 - accuracy: 0.9714 - val_loss: 0.2488 - val_accuracy: 0.9266\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92797\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23541\n",
      "epoch 61\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 151s 9ms/step - loss: 0.1038 - accuracy: 0.9719 - val_loss: 0.2656 - val_accuracy: 0.9226\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92797\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23541\n",
      "epoch 62\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 149s 9ms/step - loss: 0.1011 - accuracy: 0.9725 - val_loss: 0.2565 - val_accuracy: 0.9251\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92797\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23541\n",
      "epoch 63\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 149s 9ms/step - loss: 0.1000 - accuracy: 0.9723 - val_loss: 0.2551 - val_accuracy: 0.9232\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92797\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23541\n",
      "epoch 64\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17241/17241 [==============================] - 150s 9ms/step - loss: 0.0983 - accuracy: 0.9728 - val_loss: 0.2572 - val_accuracy: 0.9243\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92797\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23541\n",
      "epoch 65\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 150s 9ms/step - loss: 0.0960 - accuracy: 0.9737 - val_loss: 0.2588 - val_accuracy: 0.9241\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92797\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23541\n",
      "epoch 66\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 150s 9ms/step - loss: 0.0943 - accuracy: 0.9738 - val_loss: 0.2660 - val_accuracy: 0.9241\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92797\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23541\n",
      "epoch 67\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 161s 9ms/step - loss: 0.0923 - accuracy: 0.9745 - val_loss: 0.2734 - val_accuracy: 0.9230\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92797\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23541\n",
      "epoch 68\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 150s 9ms/step - loss: 0.0901 - accuracy: 0.9752 - val_loss: 0.2661 - val_accuracy: 0.9240\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92797\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23541\n",
      "epoch 69\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 151s 9ms/step - loss: 0.0899 - accuracy: 0.9750 - val_loss: 0.2606 - val_accuracy: 0.9271\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92797\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23541\n",
      "epoch 70\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 148s 9ms/step - loss: 0.0869 - accuracy: 0.9758 - val_loss: 0.2740 - val_accuracy: 0.9237\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92797\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23541\n",
      "epoch 71\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 151s 9ms/step - loss: 0.0854 - accuracy: 0.9763 - val_loss: 0.2667 - val_accuracy: 0.9228\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92797\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23541\n",
      "epoch 72\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 151s 9ms/step - loss: 0.0839 - accuracy: 0.9773 - val_loss: 0.2827 - val_accuracy: 0.9228\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92797\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23541\n",
      "epoch 73\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 161s 9ms/step - loss: 0.0829 - accuracy: 0.9773 - val_loss: 0.2749 - val_accuracy: 0.9245\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92797\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23541\n",
      "epoch 74\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 153s 9ms/step - loss: 0.0805 - accuracy: 0.9778 - val_loss: 0.2919 - val_accuracy: 0.9212\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92797\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23541\n",
      "epoch 75\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 150s 9ms/step - loss: 0.0804 - accuracy: 0.9778 - val_loss: 0.2854 - val_accuracy: 0.9220\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92797\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23541\n",
      "epoch 76\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 146s 8ms/step - loss: 0.0796 - accuracy: 0.9780 - val_loss: 0.2650 - val_accuracy: 0.9269\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92797\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23541\n",
      "epoch 77\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 149s 9ms/step - loss: 0.0777 - accuracy: 0.9785 - val_loss: 0.2684 - val_accuracy: 0.9236\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92797\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23541\n",
      "epoch 78\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 151s 9ms/step - loss: 0.0750 - accuracy: 0.9797 - val_loss: 0.2922 - val_accuracy: 0.9211\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92797\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23541\n",
      "epoch 79\n",
      "\n",
      "Train on 17241 samples, validate on 2342 samples\n",
      "Epoch 1/1\n",
      "17241/17241 [==============================] - 158s 9ms/step - loss: 0.0752 - accuracy: 0.9791 - val_loss: 0.2778 - val_accuracy: 0.9262\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92797\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23541\n",
      "doing 6th fold\n",
      "Model: \"model_14\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input (InputLayer)              (None, 1000, 25)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_85 (Dropout)            (None, 1000, 25)     0           Input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, 1000, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_91 (Lambda)              (None, 1000, 25)     0           dropout_85[0][0]                 \n",
      "                                                                 input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 1000, 25)     650         lambda_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 1000, 25)     100         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_86 (Dropout)            (None, 1000, 25)     0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_92 (Lambda)              (None, 1000, 25)     0           dropout_86[0][0]                 \n",
      "                                                                 input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 1000, 25)     1900        lambda_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 1000, 25)     100         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_93 (Lambda)              (None, 1000, 25)     0           batch_normalization_80[0][0]     \n",
      "                                                                 dropout_85[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_87 (Dropout)            (None, 1000, 25)     0           lambda_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_94 (Lambda)              (None, 1000, 25)     0           dropout_87[0][0]                 \n",
      "                                                                 input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 1000, 25)     3150        lambda_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 1000, 25)     100         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_88 (Dropout)            (None, 1000, 25)     0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_95 (Lambda)              (None, 1000, 25)     0           dropout_88[0][0]                 \n",
      "                                                                 input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 1000, 25)     5650        lambda_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 1000, 25)     100         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_96 (Lambda)              (None, 1000, 25)     0           batch_normalization_82[0][0]     \n",
      "                                                                 dropout_85[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_97 (Lambda)              (None, 1000, 25)     0           lambda_96[0][0]                  \n",
      "                                                                 input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1000, 25)     9400        lambda_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 1000, 25)     100         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_90 (Dropout)            (None, 1000, 25)     0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_98 (Lambda)              (None, 1000, 25)     0           dropout_90[0][0]                 \n",
      "                                                                 input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 1000, 25)     13150       lambda_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 1000, 25)     100         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_99 (Lambda)              (None, 1000, 25)     0           batch_normalization_84[0][0]     \n",
      "                                                                 dropout_85[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_91 (Dropout)            (None, 1000, 25)     0           lambda_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_101 (Lambda)             (None, 1000, 25)     0           dropout_91[0][0]                 \n",
      "                                                                 input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 1000, 25)     1900        lambda_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 1000, 25)     100         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_93 (Dropout)            (None, 1000, 25)     0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_102 (Lambda)             (None, 1000, 25)     0           dropout_93[0][0]                 \n",
      "                                                                 input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_19 (Bidirectional (None, 1000, 180)    223560      lambda_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 1000, 180)    720         bidirectional_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_94 (Dropout)            (None, 1000, 180)    0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_103 (Lambda)             (None, 1000, 180)    0           dropout_94[0][0]                 \n",
      "                                                                 input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_20 (Bidirectional (None, 1000, 180)    390960      lambda_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 1000, 180)    720         bidirectional_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_95 (Dropout)            (None, 1000, 180)    0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_104 (Lambda)             (None, 1000, 180)    0           dropout_95[0][0]                 \n",
      "                                                                 input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_21 (Bidirectional (None, 1000, 180)    390960      lambda_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 1000, 180)    720         bidirectional_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_96 (Dropout)            (None, 1000, 180)    0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_105 (Lambda)             (None, 1000, 180)    0           dropout_96[0][0]                 \n",
      "                                                                 input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 1000, 181)    0           lambda_105[0][0]                 \n",
      "                                                                 input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_7 (Attention)         [(None, 41, 180), (N 81549       concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 41, 180)      720         attention_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_97 (Dropout)            (None, 41, 180)      0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 7380)         0           dropout_97[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_98 (Dropout)            (None, 7380)         0           flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 80)           590480      dropout_98[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, 10, 8, 1)     0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 10, 8, 1)     4           reshape_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 10, 8, 1)     0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 10, 1, 1)     0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "1ev2 (Reshape)                  (None, 10)           0           max_pooling2d_7[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 1,716,893\n",
      "Trainable params: 1,715,101\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input (InputLayer)              (None, 1000, 25)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_85 (Dropout)            (None, 1000, 25)     0           Input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, 1000, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_91 (Lambda)              (None, 1000, 25)     0           dropout_85[0][0]                 \n",
      "                                                                 input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 1000, 25)     650         lambda_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 1000, 25)     100         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_86 (Dropout)            (None, 1000, 25)     0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_92 (Lambda)              (None, 1000, 25)     0           dropout_86[0][0]                 \n",
      "                                                                 input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 1000, 25)     1900        lambda_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 1000, 25)     100         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_93 (Lambda)              (None, 1000, 25)     0           batch_normalization_80[0][0]     \n",
      "                                                                 dropout_85[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_87 (Dropout)            (None, 1000, 25)     0           lambda_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_94 (Lambda)              (None, 1000, 25)     0           dropout_87[0][0]                 \n",
      "                                                                 input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 1000, 25)     3150        lambda_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 1000, 25)     100         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_88 (Dropout)            (None, 1000, 25)     0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_95 (Lambda)              (None, 1000, 25)     0           dropout_88[0][0]                 \n",
      "                                                                 input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 1000, 25)     5650        lambda_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 1000, 25)     100         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_96 (Lambda)              (None, 1000, 25)     0           batch_normalization_82[0][0]     \n",
      "                                                                 dropout_85[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_97 (Lambda)              (None, 1000, 25)     0           lambda_96[0][0]                  \n",
      "                                                                 input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1000, 25)     9400        lambda_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 1000, 25)     100         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_90 (Dropout)            (None, 1000, 25)     0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_98 (Lambda)              (None, 1000, 25)     0           dropout_90[0][0]                 \n",
      "                                                                 input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 1000, 25)     13150       lambda_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 1000, 25)     100         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_99 (Lambda)              (None, 1000, 25)     0           batch_normalization_84[0][0]     \n",
      "                                                                 dropout_85[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_91 (Dropout)            (None, 1000, 25)     0           lambda_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_101 (Lambda)             (None, 1000, 25)     0           dropout_91[0][0]                 \n",
      "                                                                 input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 1000, 25)     1900        lambda_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 1000, 25)     100         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_93 (Dropout)            (None, 1000, 25)     0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_102 (Lambda)             (None, 1000, 25)     0           dropout_93[0][0]                 \n",
      "                                                                 input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_19 (Bidirectional (None, 1000, 180)    223560      lambda_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 1000, 180)    720         bidirectional_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_94 (Dropout)            (None, 1000, 180)    0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_103 (Lambda)             (None, 1000, 180)    0           dropout_94[0][0]                 \n",
      "                                                                 input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_20 (Bidirectional (None, 1000, 180)    390960      lambda_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 1000, 180)    720         bidirectional_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_95 (Dropout)            (None, 1000, 180)    0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_104 (Lambda)             (None, 1000, 180)    0           dropout_95[0][0]                 \n",
      "                                                                 input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_21 (Bidirectional (None, 1000, 180)    390960      lambda_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 1000, 180)    720         bidirectional_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_96 (Dropout)            (None, 1000, 180)    0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_105 (Lambda)             (None, 1000, 180)    0           dropout_96[0][0]                 \n",
      "                                                                 input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 1000, 181)    0           lambda_105[0][0]                 \n",
      "                                                                 input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_7 (Attention)         [(None, 41, 180), (N 81549       concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 41, 180)      720         attention_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_97 (Dropout)            (None, 41, 180)      0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 7380)         0           dropout_97[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_98 (Dropout)            (None, 7380)         0           flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 80)           590480      dropout_98[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, 10, 8, 1)     0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 10, 8, 1)     4           reshape_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 10, 8, 1)     0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 10, 1, 1)     0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lev1 (Reshape)                  (None, 10, 8)        0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "1ev2 (Reshape)                  (None, 10)           0           max_pooling2d_7[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 1,716,893\n",
      "Trainable params: 1,715,101\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n",
      "epoch 0\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 166s 10ms/step - loss: 0.7185 - accuracy: 0.6979 - val_loss: 0.6624 - val_accuracy: 0.7933\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.79330, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.66243, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_loss-weights.hdf5\n",
      "epoch 1\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 152s 9ms/step - loss: 0.6255 - accuracy: 0.8253 - val_loss: 0.6056 - val_accuracy: 0.8270\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.79330 to 0.82702, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.66243 to 0.60560, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_loss-weights.hdf5\n",
      "epoch 2\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 152s 9ms/step - loss: 0.5851 - accuracy: 0.8563 - val_loss: 0.5608 - val_accuracy: 0.8639\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.82702 to 0.86389, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.60560 to 0.56083, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_loss-weights.hdf5\n",
      "epoch 3\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 166s 10ms/step - loss: 0.5548 - accuracy: 0.8762 - val_loss: 0.5312 - val_accuracy: 0.8814\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.86389 to 0.88140, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.56083 to 0.53117, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_loss-weights.hdf5\n",
      "epoch 4\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 165s 10ms/step - loss: 0.5283 - accuracy: 0.8874 - val_loss: 0.5066 - val_accuracy: 0.8970\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.88140 to 0.89701, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.53117 to 0.50658, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_loss-weights.hdf5\n",
      "epoch 5\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 160s 9ms/step - loss: 0.5044 - accuracy: 0.8948 - val_loss: 0.4855 - val_accuracy: 0.8977\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.89701 to 0.89769, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.50658 to 0.48548, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_loss-weights.hdf5\n",
      "epoch 6\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 157s 9ms/step - loss: 0.4823 - accuracy: 0.9015 - val_loss: 0.4623 - val_accuracy: 0.9084\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.89769 to 0.90838, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.48548 to 0.46227, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_loss-weights.hdf5\n",
      "epoch 7\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 157s 9ms/step - loss: 0.4620 - accuracy: 0.9062 - val_loss: 0.4461 - val_accuracy: 0.9093\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.90838 to 0.90926, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.46227 to 0.44606, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_loss-weights.hdf5\n",
      "epoch 8\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 158s 9ms/step - loss: 0.4428 - accuracy: 0.9105 - val_loss: 0.4278 - val_accuracy: 0.9121\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.90926 to 0.91213, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.44606 to 0.42776, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_loss-weights.hdf5\n",
      "epoch 9\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 166s 10ms/step - loss: 0.4232 - accuracy: 0.9146 - val_loss: 0.4220 - val_accuracy: 0.9097\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.91213\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.42776 to 0.42203, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_loss-weights.hdf5\n",
      "epoch 10\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 157s 9ms/step - loss: 0.4063 - accuracy: 0.9174 - val_loss: 0.3973 - val_accuracy: 0.9172\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.91213 to 0.91724, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.42203 to 0.39727, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_loss-weights.hdf5\n",
      "epoch 11\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 154s 9ms/step - loss: 0.3891 - accuracy: 0.9212 - val_loss: 0.3850 - val_accuracy: 0.9169\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.91724\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.39727 to 0.38498, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_loss-weights.hdf5\n",
      "epoch 12\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 155s 9ms/step - loss: 0.3731 - accuracy: 0.9241 - val_loss: 0.3739 - val_accuracy: 0.9191\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.91724 to 0.91911, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.38498 to 0.37386, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_loss-weights.hdf5\n",
      "epoch 13\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 155s 9ms/step - loss: 0.3595 - accuracy: 0.9260 - val_loss: 0.3586 - val_accuracy: 0.9184\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.91911\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.37386 to 0.35855, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_loss-weights.hdf5\n",
      "epoch 14\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 163s 10ms/step - loss: 0.3447 - accuracy: 0.9288 - val_loss: 0.3446 - val_accuracy: 0.9270\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.91911 to 0.92702, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.35855 to 0.34464, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_loss-weights.hdf5\n",
      "epoch 15\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 167s 10ms/step - loss: 0.3308 - accuracy: 0.9314 - val_loss: 0.3522 - val_accuracy: 0.9236\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92702\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.34464\n",
      "epoch 16\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 156s 9ms/step - loss: 0.3205 - accuracy: 0.9320 - val_loss: 0.3278 - val_accuracy: 0.9266\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92702\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.34464 to 0.32780, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_loss-weights.hdf5\n",
      "epoch 17\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 156s 9ms/step - loss: 0.3068 - accuracy: 0.9353 - val_loss: 0.3189 - val_accuracy: 0.9277\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92702 to 0.92765, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.32780 to 0.31888, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_loss-weights.hdf5\n",
      "epoch 18\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 155s 9ms/step - loss: 0.2961 - accuracy: 0.9363 - val_loss: 0.3223 - val_accuracy: 0.9253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92765\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.31888\n",
      "epoch 19\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 156s 9ms/step - loss: 0.2861 - accuracy: 0.9387 - val_loss: 0.3065 - val_accuracy: 0.9249\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92765\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.31888 to 0.30647, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_loss-weights.hdf5\n",
      "epoch 20\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 155s 9ms/step - loss: 0.2758 - accuracy: 0.9397 - val_loss: 0.3106 - val_accuracy: 0.9289\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92765 to 0.92893, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.30647\n",
      "epoch 21\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 167s 10ms/step - loss: 0.2666 - accuracy: 0.9417 - val_loss: 0.2964 - val_accuracy: 0.9279\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92893\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.30647 to 0.29643, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_loss-weights.hdf5\n",
      "epoch 22\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 154s 9ms/step - loss: 0.2570 - accuracy: 0.9438 - val_loss: 0.2946 - val_accuracy: 0.9266\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92893\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.29643 to 0.29461, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_loss-weights.hdf5\n",
      "epoch 23\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 156s 9ms/step - loss: 0.2487 - accuracy: 0.9454 - val_loss: 0.2856 - val_accuracy: 0.9275\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92893\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.29461 to 0.28560, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_loss-weights.hdf5\n",
      "epoch 24\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 155s 9ms/step - loss: 0.2395 - accuracy: 0.9470 - val_loss: 0.2763 - val_accuracy: 0.9298\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92893 to 0.92977, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.28560 to 0.27634, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_loss-weights.hdf5\n",
      "epoch 25\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 159s 9ms/step - loss: 0.2329 - accuracy: 0.9479 - val_loss: 0.2796 - val_accuracy: 0.9265\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92977\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.27634\n",
      "epoch 26\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 158s 9ms/step - loss: 0.2266 - accuracy: 0.9490 - val_loss: 0.2710 - val_accuracy: 0.9289\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92977\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.27634 to 0.27098, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_loss-weights.hdf5\n",
      "epoch 27\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 170s 10ms/step - loss: 0.2188 - accuracy: 0.9505 - val_loss: 0.2680 - val_accuracy: 0.9303\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92977 to 0.93029, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.27098 to 0.26795, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_loss-weights.hdf5\n",
      "epoch 28\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 158s 9ms/step - loss: 0.2140 - accuracy: 0.9509 - val_loss: 0.2609 - val_accuracy: 0.9324\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.93029 to 0.93236, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.26795 to 0.26090, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_loss-weights.hdf5\n",
      "epoch 29\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 157s 9ms/step - loss: 0.2068 - accuracy: 0.9524 - val_loss: 0.2577 - val_accuracy: 0.9315\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.26090 to 0.25769, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_loss-weights.hdf5\n",
      "epoch 30\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 155s 9ms/step - loss: 0.1997 - accuracy: 0.9538 - val_loss: 0.2571 - val_accuracy: 0.9294\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.25769 to 0.25712, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_loss-weights.hdf5\n",
      "epoch 31\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 157s 9ms/step - loss: 0.1945 - accuracy: 0.9546 - val_loss: 0.2586 - val_accuracy: 0.9293\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.25712\n",
      "epoch 32\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 157s 9ms/step - loss: 0.1902 - accuracy: 0.9550 - val_loss: 0.2527 - val_accuracy: 0.9293\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.25712 to 0.25269, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_loss-weights.hdf5\n",
      "epoch 33\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 170s 10ms/step - loss: 0.1844 - accuracy: 0.9564 - val_loss: 0.2500 - val_accuracy: 0.9289\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.25269 to 0.24996, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_loss-weights.hdf5\n",
      "epoch 34\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 158s 9ms/step - loss: 0.1792 - accuracy: 0.9574 - val_loss: 0.2470 - val_accuracy: 0.9289\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.24996 to 0.24699, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_loss-weights.hdf5\n",
      "epoch 35\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 159s 9ms/step - loss: 0.1729 - accuracy: 0.9593 - val_loss: 0.2556 - val_accuracy: 0.9304\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24699\n",
      "epoch 36\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 159s 9ms/step - loss: 0.1686 - accuracy: 0.9597 - val_loss: 0.2435 - val_accuracy: 0.9313\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.24699 to 0.24346, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_loss-weights.hdf5\n",
      "epoch 37\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 158s 9ms/step - loss: 0.1634 - accuracy: 0.9613 - val_loss: 0.2506 - val_accuracy: 0.9281\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24346\n",
      "epoch 38\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 158s 9ms/step - loss: 0.1609 - accuracy: 0.9609 - val_loss: 0.2498 - val_accuracy: 0.9275\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24346\n",
      "epoch 39\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 169s 10ms/step - loss: 0.1564 - accuracy: 0.9617 - val_loss: 0.2439 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24346\n",
      "epoch 40\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17077/17077 [==============================] - 158s 9ms/step - loss: 0.1532 - accuracy: 0.9623 - val_loss: 0.2380 - val_accuracy: 0.9289\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.24346 to 0.23796, saving model to D:/mulocdeep/lv1_result44/fold6_big_lv1_loss-weights.hdf5\n",
      "epoch 41\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 161s 9ms/step - loss: 0.1477 - accuracy: 0.9640 - val_loss: 0.2431 - val_accuracy: 0.9272\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23796\n",
      "epoch 42\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 162s 9ms/step - loss: 0.1439 - accuracy: 0.9650 - val_loss: 0.2450 - val_accuracy: 0.9292\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23796\n",
      "epoch 43\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 161s 9ms/step - loss: 0.1408 - accuracy: 0.9650 - val_loss: 0.2421 - val_accuracy: 0.9279\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23796\n",
      "epoch 44\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 160s 9ms/step - loss: 0.1369 - accuracy: 0.9663 - val_loss: 0.2530 - val_accuracy: 0.9265\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23796\n",
      "epoch 45\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 172s 10ms/step - loss: 0.1352 - accuracy: 0.9661 - val_loss: 0.2394 - val_accuracy: 0.9276\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23796\n",
      "epoch 46\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 159s 9ms/step - loss: 0.1319 - accuracy: 0.9668 - val_loss: 0.2597 - val_accuracy: 0.9278\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23796\n",
      "epoch 47\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 160s 9ms/step - loss: 0.1280 - accuracy: 0.9678 - val_loss: 0.2614 - val_accuracy: 0.9281\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23796\n",
      "epoch 48\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 161s 9ms/step - loss: 0.1237 - accuracy: 0.9691 - val_loss: 0.2428 - val_accuracy: 0.9301\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23796\n",
      "epoch 49\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 159s 9ms/step - loss: 0.1216 - accuracy: 0.9694 - val_loss: 0.2448 - val_accuracy: 0.9293\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23796\n",
      "epoch 50\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 157s 9ms/step - loss: 0.1193 - accuracy: 0.9696 - val_loss: 0.2414 - val_accuracy: 0.9296\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23796\n",
      "epoch 51\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 162s 9ms/step - loss: 0.1157 - accuracy: 0.9708 - val_loss: 0.2436 - val_accuracy: 0.9291\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23796\n",
      "epoch 52\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 165s 10ms/step - loss: 0.1123 - accuracy: 0.9720 - val_loss: 0.2539 - val_accuracy: 0.9289\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23796\n",
      "epoch 53\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 161s 9ms/step - loss: 0.1121 - accuracy: 0.9711 - val_loss: 0.2467 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23796\n",
      "epoch 54\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 169s 10ms/step - loss: 0.1094 - accuracy: 0.9718 - val_loss: 0.2432 - val_accuracy: 0.9305\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23796\n",
      "epoch 55\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 169s 10ms/step - loss: 0.1064 - accuracy: 0.9732 - val_loss: 0.2424 - val_accuracy: 0.9299\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23796\n",
      "epoch 56\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 170s 10ms/step - loss: 0.1046 - accuracy: 0.9729 - val_loss: 0.2581 - val_accuracy: 0.9270\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23796\n",
      "epoch 57\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 159s 9ms/step - loss: 0.1033 - accuracy: 0.9734 - val_loss: 0.2462 - val_accuracy: 0.9302\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23796\n",
      "epoch 58\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 156s 9ms/step - loss: 0.1004 - accuracy: 0.9746 - val_loss: 0.2459 - val_accuracy: 0.9299\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23796\n",
      "epoch 59\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 156s 9ms/step - loss: 0.0966 - accuracy: 0.9754 - val_loss: 0.2534 - val_accuracy: 0.9290\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23796\n",
      "epoch 60\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 156s 9ms/step - loss: 0.0955 - accuracy: 0.9759 - val_loss: 0.2560 - val_accuracy: 0.9270\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23796\n",
      "epoch 61\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 162s 9ms/step - loss: 0.0948 - accuracy: 0.9755 - val_loss: 0.2552 - val_accuracy: 0.9296\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23796\n",
      "epoch 62\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 169s 10ms/step - loss: 0.0916 - accuracy: 0.9764 - val_loss: 0.2537 - val_accuracy: 0.9277\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23796\n",
      "epoch 63\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 162s 9ms/step - loss: 0.0899 - accuracy: 0.9768 - val_loss: 0.2563 - val_accuracy: 0.9270\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23796\n",
      "epoch 64\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 160s 9ms/step - loss: 0.0880 - accuracy: 0.9774 - val_loss: 0.2470 - val_accuracy: 0.9310\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23796\n",
      "epoch 65\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 160s 9ms/step - loss: 0.0855 - accuracy: 0.9783 - val_loss: 0.2749 - val_accuracy: 0.9264\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23796\n",
      "epoch 66\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17077/17077 [==============================] - 160s 9ms/step - loss: 0.0850 - accuracy: 0.9782 - val_loss: 0.2551 - val_accuracy: 0.9281\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23796\n",
      "epoch 67\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 165s 10ms/step - loss: 0.0837 - accuracy: 0.9779 - val_loss: 0.2582 - val_accuracy: 0.9273\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23796\n",
      "epoch 68\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 172s 10ms/step - loss: 0.0809 - accuracy: 0.9789 - val_loss: 0.2588 - val_accuracy: 0.9285\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23796\n",
      "epoch 69\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 159s 9ms/step - loss: 0.0791 - accuracy: 0.9796 - val_loss: 0.2677 - val_accuracy: 0.9285\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23796\n",
      "epoch 70\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 159s 9ms/step - loss: 0.0783 - accuracy: 0.9797 - val_loss: 0.2661 - val_accuracy: 0.9268\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23796\n",
      "epoch 71\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 158s 9ms/step - loss: 0.0772 - accuracy: 0.9798 - val_loss: 0.2549 - val_accuracy: 0.9292\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23796\n",
      "epoch 72\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 156s 9ms/step - loss: 0.0740 - accuracy: 0.9810 - val_loss: 0.2677 - val_accuracy: 0.9288\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23796\n",
      "epoch 73\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 157s 9ms/step - loss: 0.0728 - accuracy: 0.9814 - val_loss: 0.2687 - val_accuracy: 0.9262\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23796\n",
      "epoch 74\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 168s 10ms/step - loss: 0.0730 - accuracy: 0.9808 - val_loss: 0.2905 - val_accuracy: 0.9243\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23796\n",
      "epoch 75\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 155s 9ms/step - loss: 0.0712 - accuracy: 0.9817 - val_loss: 0.2641 - val_accuracy: 0.9294\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23796\n",
      "epoch 76\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 156s 9ms/step - loss: 0.0698 - accuracy: 0.9818 - val_loss: 0.2711 - val_accuracy: 0.9255\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23796\n",
      "epoch 77\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 158s 9ms/step - loss: 0.0680 - accuracy: 0.9827 - val_loss: 0.2672 - val_accuracy: 0.9283\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23796\n",
      "epoch 78\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 157s 9ms/step - loss: 0.0655 - accuracy: 0.9837 - val_loss: 0.2723 - val_accuracy: 0.9274\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23796\n",
      "epoch 79\n",
      "\n",
      "Train on 17077 samples, validate on 2506 samples\n",
      "Epoch 1/1\n",
      "17077/17077 [==============================] - 158s 9ms/step - loss: 0.0649 - accuracy: 0.9836 - val_loss: 0.2748 - val_accuracy: 0.9283\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23796\n",
      "doing 7th fold\n",
      "Model: \"model_16\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input (InputLayer)              (None, 1000, 25)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_99 (Dropout)            (None, 1000, 25)     0           Input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 1000, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_106 (Lambda)             (None, 1000, 25)     0           dropout_99[0][0]                 \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 1000, 25)     650         lambda_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 1000, 25)     100         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_100 (Dropout)           (None, 1000, 25)     0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_107 (Lambda)             (None, 1000, 25)     0           dropout_100[0][0]                \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 1000, 25)     1900        lambda_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 1000, 25)     100         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_108 (Lambda)             (None, 1000, 25)     0           batch_normalization_93[0][0]     \n",
      "                                                                 dropout_99[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_101 (Dropout)           (None, 1000, 25)     0           lambda_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_109 (Lambda)             (None, 1000, 25)     0           dropout_101[0][0]                \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 1000, 25)     3150        lambda_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 1000, 25)     100         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_102 (Dropout)           (None, 1000, 25)     0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_110 (Lambda)             (None, 1000, 25)     0           dropout_102[0][0]                \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1000, 25)     5650        lambda_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 1000, 25)     100         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_111 (Lambda)             (None, 1000, 25)     0           batch_normalization_95[0][0]     \n",
      "                                                                 dropout_99[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_112 (Lambda)             (None, 1000, 25)     0           lambda_111[0][0]                 \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 1000, 25)     9400        lambda_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 1000, 25)     100         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_104 (Dropout)           (None, 1000, 25)     0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_113 (Lambda)             (None, 1000, 25)     0           dropout_104[0][0]                \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 1000, 25)     13150       lambda_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 1000, 25)     100         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_114 (Lambda)             (None, 1000, 25)     0           batch_normalization_97[0][0]     \n",
      "                                                                 dropout_99[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_105 (Dropout)           (None, 1000, 25)     0           lambda_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_116 (Lambda)             (None, 1000, 25)     0           dropout_105[0][0]                \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 1000, 25)     1900        lambda_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 1000, 25)     100         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_107 (Dropout)           (None, 1000, 25)     0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_117 (Lambda)             (None, 1000, 25)     0           dropout_107[0][0]                \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_22 (Bidirectional (None, 1000, 180)    223560      lambda_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 1000, 180)    720         bidirectional_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_108 (Dropout)           (None, 1000, 180)    0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_118 (Lambda)             (None, 1000, 180)    0           dropout_108[0][0]                \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_23 (Bidirectional (None, 1000, 180)    390960      lambda_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 1000, 180)    720         bidirectional_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_109 (Dropout)           (None, 1000, 180)    0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_119 (Lambda)             (None, 1000, 180)    0           dropout_109[0][0]                \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_24 (Bidirectional (None, 1000, 180)    390960      lambda_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 1000, 180)    720         bidirectional_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_110 (Dropout)           (None, 1000, 180)    0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_120 (Lambda)             (None, 1000, 180)    0           dropout_110[0][0]                \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 1000, 181)    0           lambda_120[0][0]                 \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_8 (Attention)         [(None, 41, 180), (N 81549       concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, 41, 180)      720         attention_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_111 (Dropout)           (None, 41, 180)      0           batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 7380)         0           dropout_111[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_112 (Dropout)           (None, 7380)         0           flatten_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 80)           590480      dropout_112[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_8 (Reshape)             (None, 10, 8, 1)     0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, 10, 8, 1)     4           reshape_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 10, 8, 1)     0           batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 10, 1, 1)     0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "1ev2 (Reshape)                  (None, 10)           0           max_pooling2d_8[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 1,716,893\n",
      "Trainable params: 1,715,101\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_15\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input (InputLayer)              (None, 1000, 25)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_99 (Dropout)            (None, 1000, 25)     0           Input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 1000, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_106 (Lambda)             (None, 1000, 25)     0           dropout_99[0][0]                 \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 1000, 25)     650         lambda_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 1000, 25)     100         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_100 (Dropout)           (None, 1000, 25)     0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_107 (Lambda)             (None, 1000, 25)     0           dropout_100[0][0]                \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 1000, 25)     1900        lambda_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 1000, 25)     100         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_108 (Lambda)             (None, 1000, 25)     0           batch_normalization_93[0][0]     \n",
      "                                                                 dropout_99[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_101 (Dropout)           (None, 1000, 25)     0           lambda_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_109 (Lambda)             (None, 1000, 25)     0           dropout_101[0][0]                \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 1000, 25)     3150        lambda_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 1000, 25)     100         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_102 (Dropout)           (None, 1000, 25)     0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_110 (Lambda)             (None, 1000, 25)     0           dropout_102[0][0]                \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1000, 25)     5650        lambda_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 1000, 25)     100         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_111 (Lambda)             (None, 1000, 25)     0           batch_normalization_95[0][0]     \n",
      "                                                                 dropout_99[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_112 (Lambda)             (None, 1000, 25)     0           lambda_111[0][0]                 \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 1000, 25)     9400        lambda_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 1000, 25)     100         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_104 (Dropout)           (None, 1000, 25)     0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_113 (Lambda)             (None, 1000, 25)     0           dropout_104[0][0]                \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 1000, 25)     13150       lambda_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 1000, 25)     100         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_114 (Lambda)             (None, 1000, 25)     0           batch_normalization_97[0][0]     \n",
      "                                                                 dropout_99[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_105 (Dropout)           (None, 1000, 25)     0           lambda_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_116 (Lambda)             (None, 1000, 25)     0           dropout_105[0][0]                \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 1000, 25)     1900        lambda_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 1000, 25)     100         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_107 (Dropout)           (None, 1000, 25)     0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_117 (Lambda)             (None, 1000, 25)     0           dropout_107[0][0]                \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_22 (Bidirectional (None, 1000, 180)    223560      lambda_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 1000, 180)    720         bidirectional_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_108 (Dropout)           (None, 1000, 180)    0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_118 (Lambda)             (None, 1000, 180)    0           dropout_108[0][0]                \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_23 (Bidirectional (None, 1000, 180)    390960      lambda_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 1000, 180)    720         bidirectional_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_109 (Dropout)           (None, 1000, 180)    0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_119 (Lambda)             (None, 1000, 180)    0           dropout_109[0][0]                \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_24 (Bidirectional (None, 1000, 180)    390960      lambda_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 1000, 180)    720         bidirectional_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_110 (Dropout)           (None, 1000, 180)    0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_120 (Lambda)             (None, 1000, 180)    0           dropout_110[0][0]                \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 1000, 181)    0           lambda_120[0][0]                 \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_8 (Attention)         [(None, 41, 180), (N 81549       concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, 41, 180)      720         attention_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_111 (Dropout)           (None, 41, 180)      0           batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 7380)         0           dropout_111[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_112 (Dropout)           (None, 7380)         0           flatten_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 80)           590480      dropout_112[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_8 (Reshape)             (None, 10, 8, 1)     0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, 10, 8, 1)     4           reshape_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 10, 8, 1)     0           batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 10, 1, 1)     0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lev1 (Reshape)                  (None, 10, 8)        0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "1ev2 (Reshape)                  (None, 10)           0           max_pooling2d_8[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 1,716,893\n",
      "Trainable params: 1,715,101\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n",
      "epoch 0\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 177s 10ms/step - loss: 0.7386 - accuracy: 0.7129 - val_loss: 0.6757 - val_accuracy: 0.8161\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.81613, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.67571, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_loss-weights.hdf5\n",
      "epoch 1\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 158s 9ms/step - loss: 0.6263 - accuracy: 0.8304 - val_loss: 0.6314 - val_accuracy: 0.8328\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.81613 to 0.83281, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.67571 to 0.63143, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_loss-weights.hdf5\n",
      "epoch 2\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 161s 9ms/step - loss: 0.5851 - accuracy: 0.8577 - val_loss: 0.5607 - val_accuracy: 0.8737\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.83281 to 0.87368, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.63143 to 0.56069, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_loss-weights.hdf5\n",
      "epoch 3\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 162s 9ms/step - loss: 0.5542 - accuracy: 0.8778 - val_loss: 0.5234 - val_accuracy: 0.8932\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.87368 to 0.89316, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.56069 to 0.52343, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_loss-weights.hdf5\n",
      "epoch 4\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 172s 10ms/step - loss: 0.5277 - accuracy: 0.8874 - val_loss: 0.5035 - val_accuracy: 0.8979\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.89316 to 0.89791, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.52343 to 0.50354, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_loss-weights.hdf5\n",
      "epoch 5\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 158s 9ms/step - loss: 0.5043 - accuracy: 0.8961 - val_loss: 0.4839 - val_accuracy: 0.9055\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.89791 to 0.90545, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.50354 to 0.48388, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_loss-weights.hdf5\n",
      "epoch 6\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 158s 9ms/step - loss: 0.4820 - accuracy: 0.9021 - val_loss: 0.4682 - val_accuracy: 0.9063\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.90545 to 0.90628, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.48388 to 0.46822, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_loss-weights.hdf5\n",
      "epoch 7\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 161s 9ms/step - loss: 0.4613 - accuracy: 0.9063 - val_loss: 0.4490 - val_accuracy: 0.9076\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.90628 to 0.90763, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.46822 to 0.44897, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_loss-weights.hdf5\n",
      "epoch 8\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 163s 10ms/step - loss: 0.4422 - accuracy: 0.9096 - val_loss: 0.4372 - val_accuracy: 0.9087\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.90763 to 0.90870, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.44897 to 0.43718, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_loss-weights.hdf5\n",
      "epoch 9\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 159s 9ms/step - loss: 0.4238 - accuracy: 0.9138 - val_loss: 0.4123 - val_accuracy: 0.9185\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.90870 to 0.91850, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.43718 to 0.41232, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_loss-weights.hdf5\n",
      "epoch 10\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 170s 10ms/step - loss: 0.4057 - accuracy: 0.9165 - val_loss: 0.4044 - val_accuracy: 0.9165\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.91850\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.41232 to 0.40443, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_loss-weights.hdf5\n",
      "epoch 11\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 163s 10ms/step - loss: 0.3896 - accuracy: 0.9204 - val_loss: 0.3899 - val_accuracy: 0.9174\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.91850\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.40443 to 0.38992, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_loss-weights.hdf5\n",
      "epoch 12\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 156s 9ms/step - loss: 0.3734 - accuracy: 0.9242 - val_loss: 0.3809 - val_accuracy: 0.9182\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.91850\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.38992 to 0.38093, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_loss-weights.hdf5\n",
      "epoch 13\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 158s 9ms/step - loss: 0.3594 - accuracy: 0.9252 - val_loss: 0.3635 - val_accuracy: 0.9236\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.91850 to 0.92360, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.38093 to 0.36347, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_loss-weights.hdf5\n",
      "epoch 14\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 159s 9ms/step - loss: 0.3449 - accuracy: 0.9282 - val_loss: 0.3477 - val_accuracy: 0.9244\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92360 to 0.92443, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.36347 to 0.34766, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_loss-weights.hdf5\n",
      "epoch 15\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 157s 9ms/step - loss: 0.3317 - accuracy: 0.9302 - val_loss: 0.3474 - val_accuracy: 0.9240\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92443\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.34766 to 0.34737, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_loss-weights.hdf5\n",
      "epoch 16\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 167s 10ms/step - loss: 0.3195 - accuracy: 0.9326 - val_loss: 0.3313 - val_accuracy: 0.9238\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92443\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.34737 to 0.33133, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_loss-weights.hdf5\n",
      "epoch 17\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 164s 10ms/step - loss: 0.3079 - accuracy: 0.9340 - val_loss: 0.3244 - val_accuracy: 0.9272\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92443 to 0.92719, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.33133 to 0.32443, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_loss-weights.hdf5\n",
      "epoch 18\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17053/17053 [==============================] - 159s 9ms/step - loss: 0.2951 - accuracy: 0.9366 - val_loss: 0.3177 - val_accuracy: 0.9249\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92719\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.32443 to 0.31771, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_loss-weights.hdf5\n",
      "epoch 19\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 160s 9ms/step - loss: 0.2870 - accuracy: 0.9381 - val_loss: 0.3062 - val_accuracy: 0.9255\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92719\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.31771 to 0.30622, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_loss-weights.hdf5\n",
      "epoch 20\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 158s 9ms/step - loss: 0.2763 - accuracy: 0.9397 - val_loss: 0.3126 - val_accuracy: 0.9231\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92719\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.30622\n",
      "epoch 21\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 159s 9ms/step - loss: 0.2665 - accuracy: 0.9412 - val_loss: 0.3075 - val_accuracy: 0.9235\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92719\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.30622\n",
      "epoch 22\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 172s 10ms/step - loss: 0.2581 - accuracy: 0.9433 - val_loss: 0.2961 - val_accuracy: 0.9230\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92719\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.30622 to 0.29613, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_loss-weights.hdf5\n",
      "epoch 23\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 163s 10ms/step - loss: 0.2496 - accuracy: 0.9442 - val_loss: 0.2899 - val_accuracy: 0.9257\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92719\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.29613 to 0.28992, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_loss-weights.hdf5\n",
      "epoch 24\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 158s 9ms/step - loss: 0.2418 - accuracy: 0.9456 - val_loss: 0.2807 - val_accuracy: 0.9274\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92719 to 0.92739, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.28992 to 0.28073, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_loss-weights.hdf5\n",
      "epoch 25\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 158s 9ms/step - loss: 0.2343 - accuracy: 0.9471 - val_loss: 0.2778 - val_accuracy: 0.9270\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92739\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.28073 to 0.27777, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_loss-weights.hdf5\n",
      "epoch 26\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 159s 9ms/step - loss: 0.2280 - accuracy: 0.9467 - val_loss: 0.2672 - val_accuracy: 0.9283\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92739 to 0.92830, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.27777 to 0.26716, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_loss-weights.hdf5\n",
      "epoch 27\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 161s 9ms/step - loss: 0.2205 - accuracy: 0.9491 - val_loss: 0.2758 - val_accuracy: 0.9255\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.92830\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26716\n",
      "epoch 28\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 176s 10ms/step - loss: 0.2142 - accuracy: 0.9502 - val_loss: 0.2641 - val_accuracy: 0.9289\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92830 to 0.92889, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.26716 to 0.26411, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_loss-weights.hdf5\n",
      "epoch 29\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 163s 10ms/step - loss: 0.2076 - accuracy: 0.9514 - val_loss: 0.2594 - val_accuracy: 0.9293\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92889 to 0.92933, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.26411 to 0.25936, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_loss-weights.hdf5\n",
      "epoch 30\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 160s 9ms/step - loss: 0.2010 - accuracy: 0.9530 - val_loss: 0.2548 - val_accuracy: 0.9304\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92933 to 0.93040, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.25936 to 0.25484, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_loss-weights.hdf5\n",
      "epoch 31\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 158s 9ms/step - loss: 0.1948 - accuracy: 0.9540 - val_loss: 0.2628 - val_accuracy: 0.9267\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93040\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.25484\n",
      "epoch 32\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 159s 9ms/step - loss: 0.1905 - accuracy: 0.9544 - val_loss: 0.2550 - val_accuracy: 0.9290\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93040\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.25484\n",
      "epoch 33\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 158s 9ms/step - loss: 0.1857 - accuracy: 0.9554 - val_loss: 0.2573 - val_accuracy: 0.9270\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93040\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.25484\n",
      "epoch 34\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 175s 10ms/step - loss: 0.1796 - accuracy: 0.9567 - val_loss: 0.2468 - val_accuracy: 0.9288\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93040\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.25484 to 0.24677, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_loss-weights.hdf5\n",
      "epoch 35\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 159s 9ms/step - loss: 0.1747 - accuracy: 0.9572 - val_loss: 0.2525 - val_accuracy: 0.9274\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93040\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24677\n",
      "epoch 36\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 159s 9ms/step - loss: 0.1702 - accuracy: 0.9588 - val_loss: 0.2429 - val_accuracy: 0.9290\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93040\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.24677 to 0.24290, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_loss-weights.hdf5\n",
      "epoch 37\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 159s 9ms/step - loss: 0.1648 - accuracy: 0.9597 - val_loss: 0.2462 - val_accuracy: 0.9285\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93040\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24290\n",
      "epoch 38\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 158s 9ms/step - loss: 0.1611 - accuracy: 0.9606 - val_loss: 0.2389 - val_accuracy: 0.9312\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.93040 to 0.93119, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_acc-weights.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.24290 to 0.23893, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_loss-weights.hdf5\n",
      "epoch 39\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 164s 10ms/step - loss: 0.1588 - accuracy: 0.9602 - val_loss: 0.2407 - val_accuracy: 0.9311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93119\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23893\n",
      "epoch 40\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 167s 10ms/step - loss: 0.1534 - accuracy: 0.9616 - val_loss: 0.2469 - val_accuracy: 0.9277\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93119\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23893\n",
      "epoch 41\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 162s 9ms/step - loss: 0.1493 - accuracy: 0.9629 - val_loss: 0.2390 - val_accuracy: 0.9264\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93119\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23893\n",
      "epoch 42\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 158s 9ms/step - loss: 0.1465 - accuracy: 0.9631 - val_loss: 0.2463 - val_accuracy: 0.9270\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93119\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23893\n",
      "epoch 43\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 160s 9ms/step - loss: 0.1427 - accuracy: 0.9644 - val_loss: 0.2362 - val_accuracy: 0.9287\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93119\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.23893 to 0.23620, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_loss-weights.hdf5\n",
      "epoch 44\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 158s 9ms/step - loss: 0.1392 - accuracy: 0.9653 - val_loss: 0.2535 - val_accuracy: 0.9283\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93119\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23620\n",
      "epoch 45\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 168s 10ms/step - loss: 0.1365 - accuracy: 0.9653 - val_loss: 0.2387 - val_accuracy: 0.9292\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93119\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23620\n",
      "epoch 46\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 165s 10ms/step - loss: 0.1324 - accuracy: 0.9669 - val_loss: 0.2521 - val_accuracy: 0.9266\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93119\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23620\n",
      "epoch 47\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 159s 9ms/step - loss: 0.1308 - accuracy: 0.9666 - val_loss: 0.2773 - val_accuracy: 0.9224\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93119\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23620\n",
      "epoch 48\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 158s 9ms/step - loss: 0.1269 - accuracy: 0.9674 - val_loss: 0.2378 - val_accuracy: 0.9283\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93119\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23620\n",
      "epoch 49\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 160s 9ms/step - loss: 0.1237 - accuracy: 0.9686 - val_loss: 0.2595 - val_accuracy: 0.9246\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93119\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23620\n",
      "epoch 50\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 158s 9ms/step - loss: 0.1222 - accuracy: 0.9684 - val_loss: 0.2439 - val_accuracy: 0.9287\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93119\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23620\n",
      "epoch 51\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 168s 10ms/step - loss: 0.1178 - accuracy: 0.9695 - val_loss: 0.2357 - val_accuracy: 0.9292\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93119\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.23620 to 0.23570, saving model to D:/mulocdeep/lv1_result44/fold7_big_lv1_loss-weights.hdf5\n",
      "epoch 52\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 163s 10ms/step - loss: 0.1160 - accuracy: 0.9700 - val_loss: 0.2508 - val_accuracy: 0.9288\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93119\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23570\n",
      "epoch 53\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 158s 9ms/step - loss: 0.1131 - accuracy: 0.9709 - val_loss: 0.2391 - val_accuracy: 0.9289\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93119\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23570\n",
      "epoch 54\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 157s 9ms/step - loss: 0.1115 - accuracy: 0.9711 - val_loss: 0.2526 - val_accuracy: 0.9267\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93119\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23570\n",
      "epoch 55\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 157s 9ms/step - loss: 0.1093 - accuracy: 0.9713 - val_loss: 0.2539 - val_accuracy: 0.9249\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93119\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23570\n",
      "epoch 56\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 154s 9ms/step - loss: 0.1060 - accuracy: 0.9727 - val_loss: 0.2401 - val_accuracy: 0.9310\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93119\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23570\n",
      "epoch 57\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 168s 10ms/step - loss: 0.1030 - accuracy: 0.9733 - val_loss: 0.2557 - val_accuracy: 0.9266\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93119\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23570\n",
      "epoch 58\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 157s 9ms/step - loss: 0.1013 - accuracy: 0.9739 - val_loss: 0.2498 - val_accuracy: 0.9296\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93119\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23570\n",
      "epoch 59\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 157s 9ms/step - loss: 0.0988 - accuracy: 0.9744 - val_loss: 0.2643 - val_accuracy: 0.9253\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93119\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23570\n",
      "epoch 60\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 155s 9ms/step - loss: 0.0975 - accuracy: 0.9746 - val_loss: 0.2456 - val_accuracy: 0.9275\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93119\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23570\n",
      "epoch 61\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 155s 9ms/step - loss: 0.0943 - accuracy: 0.9755 - val_loss: 0.2506 - val_accuracy: 0.9298\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93119\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23570\n",
      "epoch 62\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 157s 9ms/step - loss: 0.0930 - accuracy: 0.9758 - val_loss: 0.2548 - val_accuracy: 0.9249\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93119\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23570\n",
      "epoch 63\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 168s 10ms/step - loss: 0.0909 - accuracy: 0.9765 - val_loss: 0.2616 - val_accuracy: 0.9255\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93119\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23570\n",
      "epoch 64\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 155s 9ms/step - loss: 0.0888 - accuracy: 0.9772 - val_loss: 0.2611 - val_accuracy: 0.9259\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93119\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23570\n",
      "epoch 65\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17053/17053 [==============================] - 156s 9ms/step - loss: 0.0896 - accuracy: 0.9764 - val_loss: 0.2631 - val_accuracy: 0.9284\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93119\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23570\n",
      "epoch 66\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 156s 9ms/step - loss: 0.0861 - accuracy: 0.9774 - val_loss: 0.2618 - val_accuracy: 0.9270\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93119\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23570\n",
      "epoch 67\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 156s 9ms/step - loss: 0.0850 - accuracy: 0.9779 - val_loss: 0.2765 - val_accuracy: 0.9251\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93119\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23570\n",
      "epoch 68\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 155s 9ms/step - loss: 0.0821 - accuracy: 0.9789 - val_loss: 0.2631 - val_accuracy: 0.9253\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93119\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23570\n",
      "epoch 69\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 167s 10ms/step - loss: 0.0815 - accuracy: 0.9787 - val_loss: 0.2700 - val_accuracy: 0.9276\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93119\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23570\n",
      "epoch 70\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 157s 9ms/step - loss: 0.0800 - accuracy: 0.9791 - val_loss: 0.2594 - val_accuracy: 0.9298\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93119\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23570\n",
      "epoch 71\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 157s 9ms/step - loss: 0.0767 - accuracy: 0.9806 - val_loss: 0.2543 - val_accuracy: 0.9279\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93119\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23570\n",
      "epoch 72\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 155s 9ms/step - loss: 0.0764 - accuracy: 0.9799 - val_loss: 0.2776 - val_accuracy: 0.9272\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93119\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23570\n",
      "epoch 73\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 156s 9ms/step - loss: 0.0748 - accuracy: 0.9804 - val_loss: 0.2663 - val_accuracy: 0.9274\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93119\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23570\n",
      "epoch 74\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 158s 9ms/step - loss: 0.0748 - accuracy: 0.9804 - val_loss: 0.2716 - val_accuracy: 0.9272\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93119\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23570\n",
      "epoch 75\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 165s 10ms/step - loss: 0.0734 - accuracy: 0.9809 - val_loss: 0.2675 - val_accuracy: 0.9270\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93119\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23570\n",
      "epoch 76\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 158s 9ms/step - loss: 0.0703 - accuracy: 0.9817 - val_loss: 0.2789 - val_accuracy: 0.9264\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93119\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23570\n",
      "epoch 77\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 155s 9ms/step - loss: 0.0700 - accuracy: 0.9816 - val_loss: 0.2840 - val_accuracy: 0.9257\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93119\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23570\n",
      "epoch 78\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 155s 9ms/step - loss: 0.0682 - accuracy: 0.9825 - val_loss: 0.2714 - val_accuracy: 0.9280\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93119\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23570\n",
      "epoch 79\n",
      "\n",
      "Train on 17053 samples, validate on 2530 samples\n",
      "Epoch 1/1\n",
      "17053/17053 [==============================] - 157s 9ms/step - loss: 0.0666 - accuracy: 0.9827 - val_loss: 0.2785 - val_accuracy: 0.9258\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.93119\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23570\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-accused",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
